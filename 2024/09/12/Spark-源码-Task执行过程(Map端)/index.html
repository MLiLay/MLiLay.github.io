<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Spark-源码-Task执行过程（Map） | Mr.Li's blog</title><meta name="author" content="CHLi"><meta name="copyright" content="CHLi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="以下 spark 版本 3.3.2以下 spark 版本 3.3.2# Spark 源码 Spark 源码Task 提交之前在 Spark-源码-任务提交流程（Yarn）中我们知道 Task 在提交时会使用 LaunchTask 方法进行执行。 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoi">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark-源码-Task执行过程（Map）">
<meta property="og:url" content="http://example.com/2024/09/12/Spark-%E6%BA%90%E7%A0%81-Task%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B(Map%E7%AB%AF)/index.html">
<meta property="og:site_name" content="Mr.Li&#39;s blog">
<meta property="og:description" content="以下 spark 版本 3.3.2以下 spark 版本 3.3.2# Spark 源码 Spark 源码Task 提交之前在 Spark-源码-任务提交流程（Yarn）中我们知道 Task 在提交时会使用 LaunchTask 方法进行执行。 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoi">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/avatar.JPG">
<meta property="article:published_time" content="2024-09-12T12:22:00.000Z">
<meta property="article:modified_time" content="2024-09-12T12:21:35.371Z">
<meta property="article:author" content="CHLi">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/avatar.JPG"><link rel="shortcut icon" href="/img/spaceshuttle.png"><link rel="canonical" href="http://example.com/2024/09/12/Spark-%E6%BA%90%E7%A0%81-Task%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B(Map%E7%AB%AF)/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark-源码-Task执行过程（Map）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-12 20:21:35'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">72</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/topGraph.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Mr.Li's blog"><span class="site-name">Mr.Li's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark-源码-Task执行过程（Map）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-12T12:22:00.000Z" title="发表于 2024-09-12 20:22:00">2024-09-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-12T12:21:35.371Z" title="更新于 2024-09-12 20:21:35">2024-09-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark-源码-Task执行过程（Map）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>以下 spark 版本 3.3.2<br>以下 spark 版本 3.3.2# Spark 源码</p>
<h1 id="Spark-源码"><a href="#Spark-源码" class="headerlink" title="Spark 源码"></a>Spark 源码</h1><h2 id="Task-提交"><a href="#Task-提交" class="headerlink" title="Task 提交"></a>Task 提交</h2><p>之前在 <a href="/2024/07/01/Spark-%E6%BA%90%E7%A0%81-%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/" title="Spark-源码-任务提交流程（Yarn）">Spark-源码-任务提交流程（Yarn）</a>中我们知道 Task 在提交时会使用 LaunchTask 方法进行执行。</p>
<p>org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#launchTasks</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Launch tasks returned by a set of resource offers  </span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchTasks</span></span>(tasks: <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">TaskDescription</span>]]): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="keyword">for</span> (task &lt;- tasks.flatten) &#123;  </span><br><span class="line">    <span class="keyword">val</span> serializedTask = <span class="type">TaskDescription</span>.encode(task)  </span><br><span class="line">    <span class="keyword">if</span> (serializedTask.limit() &gt;= maxRpcMessageSize) &#123;  </span><br><span class="line">      <span class="type">Option</span>(scheduler.taskIdToTaskSetManager.get(task.taskId)).foreach &#123; taskSetMgr =&gt;  </span><br><span class="line">        <span class="keyword">try</span> &#123;  </span><br><span class="line">          <span class="keyword">var</span> msg = <span class="string">&quot;Serialized task %s:%d was %d bytes, which exceeds max allowed: &quot;</span> +  </span><br><span class="line">            <span class="string">s&quot;<span class="subst">$&#123;RPC_MESSAGE_MAX_SIZE.key&#125;</span> (%d bytes). Consider increasing &quot;</span> +  </span><br><span class="line">            <span class="string">s&quot;<span class="subst">$&#123;RPC_MESSAGE_MAX_SIZE.key&#125;</span> or using broadcast variables for large values.&quot;</span>          msg = msg.format(task.taskId, task.index, serializedTask.limit(), maxRpcMessageSize)  </span><br><span class="line">          taskSetMgr.abort(msg)  </span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logError(<span class="string">&quot;Exception in error callback&quot;</span>, e)  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">else</span> &#123;  </span><br><span class="line">      <span class="keyword">val</span> executorData = executorDataMap(task.executorId)  </span><br><span class="line">      <span class="comment">// Do resources allocation here. The allocated resources will get released after the task  </span></span><br><span class="line">      <span class="comment">// finishes.      val rpId = executorData.resourceProfileId  </span></span><br><span class="line">      <span class="keyword">val</span> prof = scheduler.sc.resourceProfileManager.resourceProfileFromId(rpId)  </span><br><span class="line">      <span class="keyword">val</span> taskCpus = <span class="type">ResourceProfile</span>.getTaskCpusOrDefaultForProfile(prof, conf)  </span><br><span class="line">      executorData.freeCores -= taskCpus  </span><br><span class="line">      task.resources.foreach &#123; <span class="keyword">case</span> (rName, rInfo) =&gt;  </span><br><span class="line">        assert(executorData.resourcesInfo.contains(rName))  </span><br><span class="line">        executorData.resourcesInfo(rName).acquire(rInfo.addresses)  </span><br><span class="line">      &#125;  </span><br><span class="line">  </span><br><span class="line">      logDebug(<span class="string">s&quot;Launching task <span class="subst">$&#123;task.taskId&#125;</span> on executor id: <span class="subst">$&#123;task.executorId&#125;</span> hostname: &quot;</span> +  </span><br><span class="line">        <span class="string">s&quot;<span class="subst">$&#123;executorData.executorHost&#125;</span>.&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">      executorData.executorEndpoint.send(<span class="type">LaunchTask</span>(<span class="keyword">new</span> <span class="type">SerializableBuffer</span>(serializedTask)))  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到最后的序列化后的 Task 会被发送到 Executor 进行执行。</p>
<p>其实本质上也就是会被发送到 CoarseGrainedExecutorBackend 中进行处理。</p>
<p>在 CoarseGrainedExecutorBackend 中的 receive 方法中就有关于 LaunchTask 消息的相应处理。</p>
<p>org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.LaunchTask$#unapply</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">LaunchTask</span>(data) =&gt;  </span><br><span class="line">  <span class="keyword">if</span> (executor == <span class="literal">null</span>) &#123;  </span><br><span class="line">    exitExecutor(<span class="number">1</span>, <span class="string">&quot;Received LaunchTask command but executor was null&quot;</span>)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="keyword">val</span> taskDesc = <span class="type">TaskDescription</span>.decode(data.value)  </span><br><span class="line">    logInfo(<span class="string">&quot;Got assigned task &quot;</span> + taskDesc.taskId)  </span><br><span class="line">    taskResources(taskDesc.taskId) = taskDesc.resources  </span><br><span class="line">    executor.launchTask(<span class="keyword">this</span>, taskDesc)  </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>可以看到首先是对 task 进行 decode，然后记录需要的 Resource，最后调用 executor 的 launchTask 方法对 Task 进行运行。</p>
<p>org.apache.spark.executor.Executor#launchTask</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">launchTask</span></span>(context: <span class="type">ExecutorBackend</span>, taskDescription: <span class="type">TaskDescription</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="keyword">val</span> taskId = taskDescription.taskId  </span><br><span class="line">  <span class="keyword">val</span> tr = createTaskRunner(context, taskDescription)  </span><br><span class="line">  runningTasks.put(taskId, tr)  </span><br><span class="line">  <span class="keyword">val</span> killMark = killMarks.get(taskId)  </span><br><span class="line">  <span class="keyword">if</span> (killMark != <span class="literal">null</span>) &#123;  </span><br><span class="line">    tr.kill(killMark._1, killMark._2)  </span><br><span class="line">    killMarks.remove(taskId)  </span><br><span class="line">  &#125;  </span><br><span class="line">  threadPool.execute(tr)  </span><br><span class="line">  <span class="keyword">if</span> (decommissioned) &#123;  </span><br><span class="line">    log.error(<span class="string">s&quot;Launching a task while in decommissioned state.&quot;</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到在 Executor 的 launchTask 内，其实首先是记录了 TaskId，然后将 taskDescription 和 Context（其实就是 CoarseGrainedExecutorBackend）封装成一个 TaskRunner，然后 TaskRunner 放在一个线程池内运行。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[executor] <span class="function"><span class="keyword">def</span> <span class="title">createTaskRunner</span></span>(context: <span class="type">ExecutorBackend</span>, taskDescription: <span class="type">TaskDescription</span>) = <span class="keyword">new</span> <span class="type">TaskRunner</span>(context, taskDescription, plugins)</span><br></pre></td></tr></table></figure>
<p>createTaskRunner 方法内部其实只是 new 一个 TaskRunner 出来。</p>
<h2 id="Task-运行"><a href="#Task-运行" class="headerlink" title="Task 运行"></a>Task 运行</h2><p>当 TaskRunner 被放在线程池内，当有线程去运行时，调用 TaskRunner 当 run 方法。</p>
<p>org.apache.spark.executor.Executor. TaskRunner#run</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  setMDCForTask(taskName, mdcProperties)  </span><br><span class="line">  threadId = <span class="type">Thread</span>.currentThread.getId  </span><br><span class="line">  <span class="type">Thread</span>.currentThread.setName(threadName)  </span><br><span class="line">  <span class="keyword">val</span> threadMXBean = <span class="type">ManagementFactory</span>.getThreadMXBean  </span><br><span class="line">  <span class="keyword">val</span> taskMemoryManager = <span class="keyword">new</span> <span class="type">TaskMemoryManager</span>(env.memoryManager, taskId)  </span><br><span class="line">  <span class="keyword">val</span> deserializeStartTimeNs = <span class="type">System</span>.nanoTime()  </span><br><span class="line">  <span class="keyword">val</span> deserializeStartCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;  </span><br><span class="line">    threadMXBean.getCurrentThreadCpuTime  </span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="number">0</span>L  </span><br><span class="line">  <span class="type">Thread</span>.currentThread.setContextClassLoader(replClassLoader)  </span><br><span class="line">  <span class="keyword">val</span> ser = env.closureSerializer.newInstance()  </span><br><span class="line">  logInfo(<span class="string">s&quot;Running <span class="subst">$taskName</span>&quot;</span>)  </span><br><span class="line">  execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">RUNNING</span>, <span class="type">EMPTY_BYTE_BUFFER</span>)  </span><br><span class="line">  <span class="keyword">var</span> taskStartTimeNs: <span class="type">Long</span> = <span class="number">0</span>  </span><br><span class="line">  <span class="keyword">var</span> taskStartCpu: <span class="type">Long</span> = <span class="number">0</span>  </span><br><span class="line">  startGCTime = computeTotalGcTime()  </span><br><span class="line">  <span class="keyword">var</span> taskStarted: <span class="type">Boolean</span> = <span class="literal">false</span>  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">try</span> &#123;  </span><br><span class="line">    <span class="comment">// Must be set before updateDependencies() is called, in case fetching dependencies  </span></span><br><span class="line">    <span class="comment">// requires access to properties contained within (e.g. for access control).    Executor.taskDeserializationProps.set(taskDescription.properties)  </span></span><br><span class="line">    <span class="comment">// 导入依赖file，jars  </span></span><br><span class="line">    updateDependencies(  </span><br><span class="line">      taskDescription.addedFiles, taskDescription.addedJars, taskDescription.addedArchives)  </span><br><span class="line">    <span class="comment">// decode Task  </span></span><br><span class="line">    task = ser.deserialize[<span class="type">Task</span>[<span class="type">Any</span>]](  </span><br><span class="line">      taskDescription.serializedTask, <span class="type">Thread</span>.currentThread.getContextClassLoader)  </span><br><span class="line">    task.localProperties = taskDescription.properties  </span><br><span class="line">    task.setTaskMemoryManager(taskMemoryManager)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// If this task has been killed before we deserialized it, let&#x27;s quit now. Otherwise,  </span></span><br><span class="line">    <span class="comment">// continue executing the task.    val killReason = reasonIfKilled  </span></span><br><span class="line">    <span class="keyword">if</span> (killReason.isDefined) &#123;  </span><br><span class="line">      <span class="comment">// Throw an exception rather than returning, because returning within a try&#123;&#125; block  </span></span><br><span class="line">      <span class="comment">// causes a NonLocalReturnControl exception to be thrown. The NonLocalReturnControl      // exception will be caught by the catch block, leading to an incorrect ExceptionFailure      // for the task.      throw new TaskKilledException(killReason.get)  </span></span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// The purpose of updating the epoch here is to invalidate executor map output status cache  </span></span><br><span class="line">    <span class="comment">// in case FetchFailures have occurred. In local mode `env.mapOutputTracker` will be    // MapOutputTrackerMaster and its cache invalidation is not based on epoch numbers so    // we don&#x27;t need to make any special calls here.    if (!isLocal) &#123;  </span></span><br><span class="line">      logDebug(<span class="string">s&quot;<span class="subst">$taskName</span>&#x27;s epoch is <span class="subst">$&#123;task.epoch&#125;</span>&quot;</span>)  </span><br><span class="line">      env.mapOutputTracker.asInstanceOf[<span class="type">MapOutputTrackerWorker</span>].updateEpoch(task.epoch)  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    metricsPoller.onTaskStart(taskId, task.stageId, task.stageAttemptId)  </span><br><span class="line">    taskStarted = <span class="literal">true</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Run the actual task and measure its runtime.  </span></span><br><span class="line">    taskStartTimeNs = <span class="type">System</span>.nanoTime()  </span><br><span class="line">    taskStartCpu = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;  </span><br><span class="line">      threadMXBean.getCurrentThreadCpuTime  </span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="number">0</span>L  </span><br><span class="line">    <span class="keyword">var</span> threwException = <span class="literal">true</span>  </span><br><span class="line">    <span class="comment">// 调用 Task 的run方法执行Task  </span></span><br><span class="line">    <span class="keyword">val</span> value = <span class="type">Utils</span>.tryWithSafeFinally &#123;  </span><br><span class="line">      <span class="keyword">val</span> res = task.run(  </span><br><span class="line">        taskAttemptId = taskId,  </span><br><span class="line">        attemptNumber = taskDescription.attemptNumber,  </span><br><span class="line">        metricsSystem = env.metricsSystem,  </span><br><span class="line">        cpus = taskDescription.cpus,  </span><br><span class="line">        resources = taskDescription.resources,  </span><br><span class="line">        plugins = plugins)  </span><br><span class="line">      threwException = <span class="literal">false</span>  </span><br><span class="line">      res  </span><br><span class="line">    &#125; &#123;  </span><br><span class="line">      <span class="keyword">val</span> releasedLocks = env.blockManager.releaseAllLocksForTask(taskId)  </span><br><span class="line">      <span class="keyword">val</span> freedMemory = taskMemoryManager.cleanUpAllAllocatedMemory()  </span><br><span class="line">  </span><br><span class="line">      <span class="keyword">if</span> (freedMemory &gt; <span class="number">0</span> &amp;&amp; !threwException) &#123;  </span><br><span class="line">        <span class="keyword">val</span> errMsg = <span class="string">s&quot;Managed memory leak detected; size = <span class="subst">$freedMemory</span> bytes, <span class="subst">$taskName</span>&quot;</span>  </span><br><span class="line">        <span class="keyword">if</span> (conf.get(<span class="type">UNSAFE_EXCEPTION_ON_MEMORY_LEAK</span>)) &#123;  </span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(errMsg)  </span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">          logWarning(errMsg)  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125;  </span><br><span class="line">  </span><br><span class="line">      <span class="keyword">if</span> (releasedLocks.nonEmpty &amp;&amp; !threwException) &#123;  </span><br><span class="line">        <span class="keyword">val</span> errMsg =  </span><br><span class="line">          <span class="string">s&quot;<span class="subst">$&#123;releasedLocks.size&#125;</span> block locks were not released by <span class="subst">$taskName</span>\n&quot;</span> +  </span><br><span class="line">            releasedLocks.mkString(<span class="string">&quot;[&quot;</span>, <span class="string">&quot;, &quot;</span>, <span class="string">&quot;]&quot;</span>)  </span><br><span class="line">        <span class="keyword">if</span> (conf.get(<span class="type">STORAGE_EXCEPTION_PIN_LEAK</span>)) &#123;  </span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(errMsg)  </span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">          logInfo(errMsg)  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    task.context.fetchFailed.foreach &#123; fetchFailure =&gt;  </span><br><span class="line">      <span class="comment">// uh-oh.  it appears the user code has caught the fetch-failure without throwing any  </span></span><br><span class="line">      <span class="comment">// other exceptions.  Its *possible* this is what the user meant to do (though highly      // unlikely).  So we will log an error and keep going.      logError(s&quot;$taskName completed successfully though internally it encountered &quot; +  </span></span><br><span class="line">        <span class="string">s&quot;unrecoverable fetch failures!  Most likely this means user code is incorrectly &quot;</span> +  </span><br><span class="line">        <span class="string">s&quot;swallowing Spark&#x27;s internal <span class="subst">$&#123;classOf[FetchFailedException]&#125;</span>&quot;</span>, fetchFailure)  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">val</span> taskFinishNs = <span class="type">System</span>.nanoTime()  </span><br><span class="line">    <span class="keyword">val</span> taskFinishCpu = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;  </span><br><span class="line">      threadMXBean.getCurrentThreadCpuTime  </span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="number">0</span>L  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// If the task has been killed, let&#x27;s fail it.  </span></span><br><span class="line">    task.context.killTaskIfInterrupted()  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">val</span> resultSer = env.serializer.newInstance()  </span><br><span class="line">    <span class="keyword">val</span> beforeSerializationNs = <span class="type">System</span>.nanoTime()  </span><br><span class="line">    <span class="keyword">val</span> valueBytes = resultSer.serialize(value)  </span><br><span class="line">    <span class="keyword">val</span> afterSerializationNs = <span class="type">System</span>.nanoTime()  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Deserialization happens in two parts: first, we deserialize a Task object, which  </span></span><br><span class="line">    <span class="comment">// includes the Partition. Second, Task.run() deserializes the RDD and function to be run.    task.metrics.setExecutorDeserializeTime(TimeUnit.NANOSECONDS.toMillis(  </span></span><br><span class="line">      (taskStartTimeNs - deserializeStartTimeNs) + task.executorDeserializeTimeNs))  </span><br><span class="line">    task.metrics.setExecutorDeserializeCpuTime(  </span><br><span class="line">      (taskStartCpu - deserializeStartCpuTime) + task.executorDeserializeCpuTime)  </span><br><span class="line">    <span class="comment">// We need to subtract Task.run()&#x27;s deserialization time to avoid double-counting  </span></span><br><span class="line">    task.metrics.setExecutorRunTime(<span class="type">TimeUnit</span>.<span class="type">NANOSECONDS</span>.toMillis(  </span><br><span class="line">      (taskFinishNs - taskStartTimeNs) - task.executorDeserializeTimeNs))  </span><br><span class="line">    task.metrics.setExecutorCpuTime(  </span><br><span class="line">      (taskFinishCpu - taskStartCpu) - task.executorDeserializeCpuTime)  </span><br><span class="line">    task.metrics.setJvmGCTime(computeTotalGcTime() - startGCTime)  </span><br><span class="line">    task.metrics.setResultSerializationTime(<span class="type">TimeUnit</span>.<span class="type">NANOSECONDS</span>.toMillis(  </span><br><span class="line">      afterSerializationNs - beforeSerializationNs))  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Expose task metrics using the Dropwizard metrics system.  </span></span><br><span class="line">    <span class="comment">// Update task metrics counters    executorSource.METRIC_CPU_TIME.inc(task.metrics.executorCpuTime)  </span></span><br><span class="line">    executorSource.<span class="type">METRIC_RUN_TIME</span>.inc(task.metrics.executorRunTime)  </span><br><span class="line">    executorSource.<span class="type">METRIC_JVM_GC_TIME</span>.inc(task.metrics.jvmGCTime)  </span><br><span class="line">    executorSource.<span class="type">METRIC_DESERIALIZE_TIME</span>.inc(task.metrics.executorDeserializeTime)  </span><br><span class="line">    executorSource.<span class="type">METRIC_DESERIALIZE_CPU_TIME</span>.inc(task.metrics.executorDeserializeCpuTime)  </span><br><span class="line">    executorSource.<span class="type">METRIC_RESULT_SERIALIZE_TIME</span>.inc(task.metrics.resultSerializationTime)  </span><br><span class="line">    executorSource.<span class="type">METRIC_SHUFFLE_FETCH_WAIT_TIME</span>  </span><br><span class="line">      .inc(task.metrics.shuffleReadMetrics.fetchWaitTime)  </span><br><span class="line">    executorSource.<span class="type">METRIC_SHUFFLE_WRITE_TIME</span>.inc(task.metrics.shuffleWriteMetrics.writeTime)  </span><br><span class="line">    executorSource.<span class="type">METRIC_SHUFFLE_TOTAL_BYTES_READ</span>  </span><br><span class="line">      .inc(task.metrics.shuffleReadMetrics.totalBytesRead)  </span><br><span class="line">    executorSource.<span class="type">METRIC_SHUFFLE_REMOTE_BYTES_READ</span>  </span><br><span class="line">      .inc(task.metrics.shuffleReadMetrics.remoteBytesRead)  </span><br><span class="line">    executorSource.<span class="type">METRIC_SHUFFLE_REMOTE_BYTES_READ_TO_DISK</span>  </span><br><span class="line">      .inc(task.metrics.shuffleReadMetrics.remoteBytesReadToDisk)  </span><br><span class="line">    executorSource.<span class="type">METRIC_SHUFFLE_LOCAL_BYTES_READ</span>  </span><br><span class="line">      .inc(task.metrics.shuffleReadMetrics.localBytesRead)  </span><br><span class="line">    executorSource.<span class="type">METRIC_SHUFFLE_RECORDS_READ</span>  </span><br><span class="line">      .inc(task.metrics.shuffleReadMetrics.recordsRead)  </span><br><span class="line">    executorSource.<span class="type">METRIC_SHUFFLE_REMOTE_BLOCKS_FETCHED</span>  </span><br><span class="line">      .inc(task.metrics.shuffleReadMetrics.remoteBlocksFetched)  </span><br><span class="line">    executorSource.<span class="type">METRIC_SHUFFLE_LOCAL_BLOCKS_FETCHED</span>  </span><br><span class="line">      .inc(task.metrics.shuffleReadMetrics.localBlocksFetched)  </span><br><span class="line">    executorSource.<span class="type">METRIC_SHUFFLE_BYTES_WRITTEN</span>  </span><br><span class="line">      .inc(task.metrics.shuffleWriteMetrics.bytesWritten)  </span><br><span class="line">    executorSource.<span class="type">METRIC_SHUFFLE_RECORDS_WRITTEN</span>  </span><br><span class="line">      .inc(task.metrics.shuffleWriteMetrics.recordsWritten)  </span><br><span class="line">    executorSource.<span class="type">METRIC_INPUT_BYTES_READ</span>  </span><br><span class="line">      .inc(task.metrics.inputMetrics.bytesRead)  </span><br><span class="line">    executorSource.<span class="type">METRIC_INPUT_RECORDS_READ</span>  </span><br><span class="line">      .inc(task.metrics.inputMetrics.recordsRead)  </span><br><span class="line">    executorSource.<span class="type">METRIC_OUTPUT_BYTES_WRITTEN</span>  </span><br><span class="line">      .inc(task.metrics.outputMetrics.bytesWritten)  </span><br><span class="line">    executorSource.<span class="type">METRIC_OUTPUT_RECORDS_WRITTEN</span>  </span><br><span class="line">      .inc(task.metrics.outputMetrics.recordsWritten)  </span><br><span class="line">    executorSource.<span class="type">METRIC_RESULT_SIZE</span>.inc(task.metrics.resultSize)  </span><br><span class="line">    executorSource.<span class="type">METRIC_DISK_BYTES_SPILLED</span>.inc(task.metrics.diskBytesSpilled)  </span><br><span class="line">    executorSource.<span class="type">METRIC_MEMORY_BYTES_SPILLED</span>.inc(task.metrics.memoryBytesSpilled)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Note: accumulator updates must be collected after TaskMetrics is updated  </span></span><br><span class="line">    <span class="comment">// 更新累加器，序列化结果  </span></span><br><span class="line">    <span class="keyword">val</span> accumUpdates = task.collectAccumulatorUpdates()  </span><br><span class="line">    <span class="keyword">val</span> metricPeaks = metricsPoller.getTaskMetricPeaks(taskId)  </span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> do not serialize value twice  </span></span><br><span class="line">    <span class="keyword">val</span> directResult = <span class="keyword">new</span> <span class="type">DirectTaskResult</span>(valueBytes, accumUpdates, metricPeaks)  </span><br><span class="line">    <span class="keyword">val</span> serializedDirectResult = ser.serialize(directResult)  </span><br><span class="line">    <span class="keyword">val</span> resultSize = serializedDirectResult.limit()  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// directSend = sending directly back to the driver  </span></span><br><span class="line">    <span class="comment">// 这里会判断结果的大小，如果小直接发送给driver  </span></span><br><span class="line">    <span class="comment">// 如果大于可以直接发送的结果会把结果储存在BlockManager中  </span></span><br><span class="line">    <span class="keyword">val</span> serializedResult: <span class="type">ByteBuffer</span> = &#123;  </span><br><span class="line">      <span class="keyword">if</span> (maxResultSize &gt; <span class="number">0</span> &amp;&amp; resultSize &gt; maxResultSize) &#123;  </span><br><span class="line">        logWarning(<span class="string">s&quot;Finished <span class="subst">$taskName</span>. Result is larger than maxResultSize &quot;</span> +  </span><br><span class="line">          <span class="string">s&quot;(<span class="subst">$&#123;Utils.bytesToString(resultSize)&#125;</span> &gt; <span class="subst">$&#123;Utils.bytesToString(maxResultSize)&#125;</span>), &quot;</span> +  </span><br><span class="line">          <span class="string">s&quot;dropping it.&quot;</span>)  </span><br><span class="line">        ser.serialize(<span class="keyword">new</span> <span class="type">IndirectTaskResult</span>[<span class="type">Any</span>](<span class="type">TaskResultBlockId</span>(taskId), resultSize))  </span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (resultSize &gt; maxDirectResultSize) &#123;  </span><br><span class="line">        <span class="keyword">val</span> blockId = <span class="type">TaskResultBlockId</span>(taskId)  </span><br><span class="line">        env.blockManager.putBytes(  </span><br><span class="line">          blockId,  </span><br><span class="line">          <span class="keyword">new</span> <span class="type">ChunkedByteBuffer</span>(serializedDirectResult.duplicate()),  </span><br><span class="line">          <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)  </span><br><span class="line">        logInfo(<span class="string">s&quot;Finished <span class="subst">$taskName</span>. <span class="subst">$resultSize</span> bytes result sent via BlockManager)&quot;</span>)  </span><br><span class="line">        ser.serialize(<span class="keyword">new</span> <span class="type">IndirectTaskResult</span>[<span class="type">Any</span>](blockId, resultSize))  </span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">        logInfo(<span class="string">s&quot;Finished <span class="subst">$taskName</span>. <span class="subst">$resultSize</span> bytes result sent to driver&quot;</span>)  </span><br><span class="line">        serializedDirectResult  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    executorSource.<span class="type">SUCCEEDED_TASKS</span>.inc(<span class="number">1</span>L)  </span><br><span class="line">    setTaskFinishedAndClearInterruptStatus()  </span><br><span class="line">    plugins.foreach(_.onTaskSucceeded())  </span><br><span class="line">    <span class="comment">// 向Driver发送消息，更新Task 的状态，其实会发送到TaskScheduler  </span></span><br><span class="line">    execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FINISHED</span>, serializedResult)  </span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">    <span class="keyword">case</span> t: <span class="type">TaskKilledException</span> =&gt;  </span><br><span class="line">      logInfo(<span class="string">s&quot;Executor killed <span class="subst">$taskName</span>, reason: <span class="subst">$&#123;t.reason&#125;</span>&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">      <span class="keyword">val</span> (accums, accUpdates) = collectAccumulatorsAndResetStatusOnFailure(taskStartTimeNs)  </span><br><span class="line">      <span class="comment">// Here and below, put task metric peaks in a WrappedArray to expose them as a Seq  </span></span><br><span class="line">      <span class="comment">// without requiring a copy.      val metricPeaks = WrappedArray.make(metricsPoller.getTaskMetricPeaks(taskId))  </span></span><br><span class="line">      <span class="keyword">val</span> reason = <span class="type">TaskKilled</span>(t.reason, accUpdates, accums, metricPeaks.toSeq)  </span><br><span class="line">      plugins.foreach(_.onTaskFailed(reason))  </span><br><span class="line">      execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">KILLED</span>, ser.serialize(reason))  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> _: <span class="type">InterruptedException</span> | <span class="type">NonFatal</span>(_) <span class="keyword">if</span>  </span><br><span class="line">        task != <span class="literal">null</span> &amp;&amp; task.reasonIfKilled.isDefined =&gt;  </span><br><span class="line">      <span class="keyword">val</span> killReason = task.reasonIfKilled.getOrElse(<span class="string">&quot;unknown reason&quot;</span>)  </span><br><span class="line">      logInfo(<span class="string">s&quot;Executor interrupted and killed <span class="subst">$taskName</span>, reason: <span class="subst">$killReason</span>&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">      <span class="keyword">val</span> (accums, accUpdates) = collectAccumulatorsAndResetStatusOnFailure(taskStartTimeNs)  </span><br><span class="line">      <span class="keyword">val</span> metricPeaks = <span class="type">WrappedArray</span>.make(metricsPoller.getTaskMetricPeaks(taskId))  </span><br><span class="line">      <span class="keyword">val</span> reason = <span class="type">TaskKilled</span>(killReason, accUpdates, accums, metricPeaks.toSeq)  </span><br><span class="line">      plugins.foreach(_.onTaskFailed(reason))  </span><br><span class="line">      execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">KILLED</span>, ser.serialize(reason))  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> t: <span class="type">Throwable</span> <span class="keyword">if</span> hasFetchFailure &amp;&amp; !<span class="type">Executor</span>.isFatalError(t, killOnFatalErrorDepth) =&gt;  </span><br><span class="line">      <span class="keyword">val</span> reason = task.context.fetchFailed.get.toTaskFailedReason  </span><br><span class="line">      <span class="keyword">if</span> (!t.isInstanceOf[<span class="type">FetchFailedException</span>]) &#123;  </span><br><span class="line">        <span class="comment">// there was a fetch failure in the task, but some user code wrapped that exception  </span></span><br><span class="line">        <span class="comment">// and threw something else.  Regardless, we treat it as a fetch failure.        val fetchFailedCls = classOf[FetchFailedException].getName  </span></span><br><span class="line">        logWarning(<span class="string">s&quot;<span class="subst">$taskName</span> encountered a <span class="subst">$&#123;fetchFailedCls&#125;</span> and &quot;</span> +  </span><br><span class="line">          <span class="string">s&quot;failed, but the <span class="subst">$&#123;fetchFailedCls&#125;</span> was hidden by another &quot;</span> +  </span><br><span class="line">          <span class="string">s&quot;exception.  Spark is handling this like a fetch failure and ignoring the &quot;</span> +  </span><br><span class="line">          <span class="string">s&quot;other exception: <span class="subst">$t</span>&quot;</span>)  </span><br><span class="line">      &#125;  </span><br><span class="line">      setTaskFinishedAndClearInterruptStatus()  </span><br><span class="line">      plugins.foreach(_.onTaskFailed(reason))  </span><br><span class="line">      execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FAILED</span>, ser.serialize(reason))  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">CausedBy</span>(cDE: <span class="type">CommitDeniedException</span>) =&gt;  </span><br><span class="line">      <span class="keyword">val</span> reason = cDE.toTaskCommitDeniedReason  </span><br><span class="line">      setTaskFinishedAndClearInterruptStatus()  </span><br><span class="line">      plugins.foreach(_.onTaskFailed(reason))  </span><br><span class="line">      execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">KILLED</span>, ser.serialize(reason))  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> t: <span class="type">Throwable</span> <span class="keyword">if</span> env.isStopped =&gt;  </span><br><span class="line">      <span class="comment">// Log the expected exception after executor.stop without stack traces  </span></span><br><span class="line">      <span class="comment">// see: SPARK-19147      logError(s&quot;Exception in $taskName: $&#123;t.getMessage&#125;&quot;)  </span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;  </span><br><span class="line">      <span class="comment">// Attempt to exit cleanly by informing the driver of our failure.  </span></span><br><span class="line">      <span class="comment">// If anything goes wrong (or this was a fatal exception), we will delegate to      // the default uncaught exception handler, which will terminate the Executor.      logError(s&quot;Exception in $taskName&quot;, t)  </span></span><br><span class="line">  </span><br><span class="line">      <span class="comment">// SPARK-20904: Do not report failure to driver if if happened during shut down. Because  </span></span><br><span class="line">      <span class="comment">// libraries may set up shutdown hooks that race with running tasks during shutdown,      // spurious failures may occur and can result in improper accounting in the driver (e.g.      // the task failure would not be ignored if the shutdown happened because of preemption,      // instead of an app issue).      if (!ShutdownHookManager.inShutdown()) &#123;  </span></span><br><span class="line">        <span class="keyword">val</span> (accums, accUpdates) = collectAccumulatorsAndResetStatusOnFailure(taskStartTimeNs)  </span><br><span class="line">        <span class="keyword">val</span> metricPeaks = <span class="type">WrappedArray</span>.make(metricsPoller.getTaskMetricPeaks(taskId))  </span><br><span class="line">  </span><br><span class="line">        <span class="keyword">val</span> (taskFailureReason, serializedTaskFailureReason) = &#123;  </span><br><span class="line">          <span class="keyword">try</span> &#123;  </span><br><span class="line">            <span class="keyword">val</span> ef = <span class="keyword">new</span> <span class="type">ExceptionFailure</span>(t, accUpdates).withAccums(accums)  </span><br><span class="line">              .withMetricPeaks(metricPeaks.toSeq)  </span><br><span class="line">            (ef, ser.serialize(ef))  </span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">            <span class="keyword">case</span> _: <span class="type">NotSerializableException</span> =&gt;  </span><br><span class="line">              <span class="comment">// t is not serializable so just send the stacktrace  </span></span><br><span class="line">              <span class="keyword">val</span> ef = <span class="keyword">new</span> <span class="type">ExceptionFailure</span>(t, accUpdates, <span class="literal">false</span>).withAccums(accums)  </span><br><span class="line">                .withMetricPeaks(metricPeaks.toSeq)  </span><br><span class="line">              (ef, ser.serialize(ef))  </span><br><span class="line">          &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">        setTaskFinishedAndClearInterruptStatus()  </span><br><span class="line">        plugins.foreach(_.onTaskFailed(taskFailureReason))  </span><br><span class="line">        execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FAILED</span>, serializedTaskFailureReason)  </span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">        logInfo(<span class="string">&quot;Not reporting error to driver during JVM shutdown.&quot;</span>)  </span><br><span class="line">      &#125;  </span><br><span class="line">  </span><br><span class="line">      <span class="comment">// Don&#x27;t forcibly exit unless the exception was inherently fatal, to avoid  </span></span><br><span class="line">      <span class="comment">// stopping other tasks unnecessarily.      if (Executor.isFatalError(t, killOnFatalErrorDepth)) &#123;  </span></span><br><span class="line">        uncaughtExceptionHandler.uncaughtException(<span class="type">Thread</span>.currentThread(), t)  </span><br><span class="line">      &#125;  </span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;  </span><br><span class="line">    <span class="comment">// 将Task从正在running的状态中移除  </span></span><br><span class="line">    runningTasks.remove(taskId)  </span><br><span class="line">    <span class="keyword">if</span> (taskStarted) &#123;  </span><br><span class="line">      <span class="comment">// This means the task was successfully deserialized, its stageId and stageAttemptId  </span></span><br><span class="line">      <span class="comment">// are known, and metricsPoller.onTaskStart was called.      metricsPoller.onTaskCompletion(taskId, task.stageId, task.stageAttemptId)  </span></span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 run 方法内部主要包含几个步骤。</p>
<ol>
<li>导入依赖 file，jars</li>
<li>反序列化 task</li>
<li>调用 Task 的 run 方法执行 Task</li>
<li>更新累加器，序列化 Task 运行结果</li>
<li>判断 Task 运行产生的结果大小，决定是序列化后直接发送给 driver 还是存在 blockmanager 后返回 chunk 信息。</li>
<li>向 Driver 发送消息，更新 Task 的状态，其实会发送到 TaskScheduler</li>
</ol>
<p>org.apache.spark.scheduler.Task#run</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Called by [[org.apache.spark.executor.Executor]] to run this task.  </span></span><br><span class="line"><span class="comment"> * * @param taskAttemptId an identifier for this task attempt that is unique within a SparkContext.  </span></span><br><span class="line"><span class="comment"> * @param attemptNumber how many times this task has been attempted (0 for the first attempt)  </span></span><br><span class="line"><span class="comment"> * @param resources other host resources (like gpus) that this task attempt can access  </span></span><br><span class="line"><span class="comment"> * @return the result of the task along with updates of Accumulators.  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(  </span><br><span class="line">    taskAttemptId: <span class="type">Long</span>,  </span><br><span class="line">    attemptNumber: <span class="type">Int</span>,  </span><br><span class="line">    metricsSystem: <span class="type">MetricsSystem</span>,  </span><br><span class="line">    cpus: <span class="type">Int</span>,  </span><br><span class="line">    resources: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">ResourceInformation</span>],  </span><br><span class="line">    plugins: <span class="type">Option</span>[<span class="type">PluginContainer</span>]): <span class="type">T</span> = &#123;  </span><br><span class="line">  </span><br><span class="line">  require(cpus &gt; <span class="number">0</span>, <span class="string">&quot;CPUs per task should be &gt; 0&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">  <span class="type">SparkEnv</span>.get.blockManager.registerTask(taskAttemptId)  </span><br><span class="line">  <span class="comment">// TODO SPARK-24874 Allow create BarrierTaskContext based on partitions, instead of whether  </span></span><br><span class="line">  <span class="comment">// the stage is barrier.  </span></span><br><span class="line">  <span class="keyword">val</span> taskContext = <span class="keyword">new</span> <span class="type">TaskContextImpl</span>(  </span><br><span class="line">    stageId,  </span><br><span class="line">    stageAttemptId, <span class="comment">// stageAttemptId and stageAttemptNumber are semantically equal  </span></span><br><span class="line">    partitionId,  </span><br><span class="line">    taskAttemptId,  </span><br><span class="line">    attemptNumber,  </span><br><span class="line">    taskMemoryManager,  </span><br><span class="line">    localProperties,  </span><br><span class="line">    metricsSystem,  </span><br><span class="line">    metrics,  </span><br><span class="line">    cpus,  </span><br><span class="line">    resources)  </span><br><span class="line">  </span><br><span class="line">  context = <span class="keyword">if</span> (isBarrier) &#123;  </span><br><span class="line">    <span class="keyword">new</span> <span class="type">BarrierTaskContext</span>(taskContext)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    taskContext  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="type">InputFileBlockHolder</span>.initialize()  </span><br><span class="line">  <span class="type">TaskContext</span>.setTaskContext(context)  </span><br><span class="line">  taskThread = <span class="type">Thread</span>.currentThread()  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (_reasonIfKilled != <span class="literal">null</span>) &#123;  </span><br><span class="line">    kill(interruptThread = <span class="literal">false</span>, _reasonIfKilled)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">new</span> <span class="type">CallerContext</span>(  </span><br><span class="line">    <span class="string">&quot;TASK&quot;</span>,  </span><br><span class="line">    <span class="type">SparkEnv</span>.get.conf.get(<span class="type">APP_CALLER_CONTEXT</span>),  </span><br><span class="line">    appId,  </span><br><span class="line">    appAttemptId,  </span><br><span class="line">    jobId,  </span><br><span class="line">    <span class="type">Option</span>(stageId),  </span><br><span class="line">    <span class="type">Option</span>(stageAttemptId),  </span><br><span class="line">    <span class="type">Option</span>(taskAttemptId),  </span><br><span class="line">    <span class="type">Option</span>(attemptNumber)).setCurrentContext()  </span><br><span class="line">  </span><br><span class="line">  plugins.foreach(_.onTaskStart())  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">try</span> &#123;  </span><br><span class="line">    runTask(context)  </span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;  </span><br><span class="line">      <span class="comment">// Catch all errors; run task failure callbacks, and rethrow the exception.  </span></span><br><span class="line">      <span class="keyword">try</span> &#123;  </span><br><span class="line">        context.markTaskFailed(e)  </span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">        <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;  </span><br><span class="line">          e.addSuppressed(t)  </span><br><span class="line">      &#125;  </span><br><span class="line">      context.markTaskCompleted(<span class="type">Some</span>(e))  </span><br><span class="line">      <span class="keyword">throw</span> e  </span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;  </span><br><span class="line">    <span class="keyword">try</span> &#123;  </span><br><span class="line">      <span class="comment">// Call the task completion callbacks. If &quot;markTaskCompleted&quot; is called twice, the second  </span></span><br><span class="line">      <span class="comment">// one is no-op.      context.markTaskCompleted(None)  </span></span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;  </span><br><span class="line">      <span class="keyword">try</span> &#123;  </span><br><span class="line">        <span class="type">Utils</span>.tryLogNonFatalError &#123;  </span><br><span class="line">          <span class="comment">// Release memory used by this thread for unrolling blocks  </span></span><br><span class="line">          <span class="type">SparkEnv</span>.get.blockManager.memoryStore.releaseUnrollMemoryForThisTask(<span class="type">MemoryMode</span>.<span class="type">ON_HEAP</span>)  </span><br><span class="line">          <span class="type">SparkEnv</span>.get.blockManager.memoryStore.releaseUnrollMemoryForThisTask(  </span><br><span class="line">            <span class="type">MemoryMode</span>.<span class="type">OFF_HEAP</span>)  </span><br><span class="line">          <span class="comment">// Notify any tasks waiting for execution memory to be freed to wake up and try to  </span></span><br><span class="line">          <span class="comment">// acquire memory again. This makes impossible the scenario where a task sleeps forever          // because there are no other tasks left to notify it. Since this is safe to do but may          // not be strictly necessary, we should revisit whether we can remove this in the          // future.          val memoryManager = SparkEnv.get.memoryManager  </span></span><br><span class="line">          memoryManager.synchronized &#123; memoryManager.notifyAll() &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;  </span><br><span class="line">        <span class="comment">// Though we unset the ThreadLocal here, the context member variable itself is still  </span></span><br><span class="line">        <span class="comment">// queried directly in the TaskRunner to check for FetchFailedExceptions.        TaskContext.unset()  </span></span><br><span class="line">        <span class="type">InputFileBlockHolder</span>.unset()  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实这里里面最重要的就是 runTask 方法。</p>
<p>runTask 方法是在 Task 这个类里面是一个抽象方法，其主要由两个 Task 的子类：ResultTask 和 ShuffleMapTask 实现。</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20240813145919.png" alt="image.png"></p>
<h3 id="ShuffleMapTask"><a href="#ShuffleMapTask" class="headerlink" title="ShuffleMapTask"></a>ShuffleMapTask</h3><p>下面的内容是 ShuffleMapTask 中的 runTask 方法。</p>
<p>org.apache.spark.scheduler.ShuffleMapTask#runTask</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span></span>(context: <span class="type">TaskContext</span>): <span class="type">MapStatus</span> = &#123;  </span><br><span class="line">  <span class="comment">// Deserialize the RDD using the broadcast variable.  </span></span><br><span class="line">  <span class="keyword">val</span> threadMXBean = <span class="type">ManagementFactory</span>.getThreadMXBean  </span><br><span class="line">  <span class="keyword">val</span> deserializeStartTimeNs = <span class="type">System</span>.nanoTime()  </span><br><span class="line">  <span class="keyword">val</span> deserializeStartCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;  </span><br><span class="line">    threadMXBean.getCurrentThreadCpuTime  </span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="number">0</span>L  </span><br><span class="line">  <span class="keyword">val</span> ser = <span class="type">SparkEnv</span>.get.closureSerializer.newInstance()  </span><br><span class="line">  <span class="comment">// 获得RDD和ShuffleDependency</span></span><br><span class="line">  <span class="keyword">val</span> rddAndDep = ser.deserialize[(<span class="type">RDD</span>[_], <span class="type">ShuffleDependency</span>[_, _, _])](  </span><br><span class="line">    <span class="type">ByteBuffer</span>.wrap(taskBinary.value), <span class="type">Thread</span>.currentThread.getContextClassLoader)  </span><br><span class="line">  _executorDeserializeTimeNs = <span class="type">System</span>.nanoTime() - deserializeStartTimeNs  </span><br><span class="line">  _executorDeserializeCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;  </span><br><span class="line">    threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime  </span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="number">0</span>L  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> rdd = rddAndDep._1  </span><br><span class="line">  <span class="keyword">val</span> dep = rddAndDep._2  </span><br><span class="line">  <span class="comment">// While we use the old shuffle fetch protocol, we use partitionId as mapId in the  </span></span><br><span class="line">  <span class="comment">// ShuffleBlockId construction.  val mapId = if (SparkEnv.get.conf.get(config.SHUFFLE_USE_OLD_FETCH_PROTOCOL)) &#123;  </span></span><br><span class="line">    partitionId  </span><br><span class="line">  &#125; <span class="keyword">else</span> context.taskAttemptId()  </span><br><span class="line">  dep.shuffleWriterProcessor.write(rdd, dep, mapId, context, partition)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 runTask 内部的就会调用 ShuffleDependency 的 shuffleWriterProcessor 中的 write 方法写数据。</p>
<p>org.apache.spark.shuffle.ShuffleWriteProcessor#write</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * The write process for particular partition, it controls the life circle of [[ShuffleWriter]]  </span></span><br><span class="line"><span class="comment"> * get from [[ShuffleManager]] and triggers rdd compute, finally return the [[MapStatus]] for  </span></span><br><span class="line"><span class="comment"> * this task. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(  </span><br><span class="line">    rdd: <span class="type">RDD</span>[_],  </span><br><span class="line">    dep: <span class="type">ShuffleDependency</span>[_, _, _],  </span><br><span class="line">    mapId: <span class="type">Long</span>,  </span><br><span class="line">    context: <span class="type">TaskContext</span>,  </span><br><span class="line">    partition: <span class="type">Partition</span>): <span class="type">MapStatus</span> = &#123;  </span><br><span class="line">  <span class="keyword">var</span> writer: <span class="type">ShuffleWriter</span>[<span class="type">Any</span>, <span class="type">Any</span>] = <span class="literal">null</span>  </span><br><span class="line">  <span class="keyword">try</span> &#123;  </span><br><span class="line">    <span class="keyword">val</span> manager = <span class="type">SparkEnv</span>.get.shuffleManager  </span><br><span class="line">    writer = manager.getWriter[<span class="type">Any</span>, <span class="type">Any</span>](  </span><br><span class="line">      dep.shuffleHandle,  </span><br><span class="line">      mapId,  </span><br><span class="line">      context,  </span><br><span class="line">      createMetricsReporter(context))  </span><br><span class="line">    <span class="comment">// 这里是真正的write，rdd.iterator中会对转换算子进行计算  </span></span><br><span class="line">    writer.write(  </span><br><span class="line">      rdd.iterator(partition, context).asInstanceOf[<span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]])  </span><br><span class="line">    <span class="keyword">val</span> mapStatus = writer.stop(success = <span class="literal">true</span>)  </span><br><span class="line">    <span class="keyword">if</span> (mapStatus.isDefined) &#123;  </span><br><span class="line">      <span class="comment">// Check if sufficient shuffle mergers are available now for the ShuffleMapTask to push  </span></span><br><span class="line">      <span class="keyword">if</span> (dep.shuffleMergeAllowed &amp;&amp; dep.getMergerLocs.isEmpty) &#123;  </span><br><span class="line">        <span class="keyword">val</span> mapOutputTracker = <span class="type">SparkEnv</span>.get.mapOutputTracker  </span><br><span class="line">        <span class="keyword">val</span> mergerLocs =  </span><br><span class="line">          mapOutputTracker.getShufflePushMergerLocations(dep.shuffleId)  </span><br><span class="line">        <span class="keyword">if</span> (mergerLocs.nonEmpty) &#123;  </span><br><span class="line">          dep.setMergerLocs(mergerLocs)  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125;  </span><br><span class="line">      <span class="comment">// Initiate shuffle push process if push based shuffle is enabled  </span></span><br><span class="line">      <span class="comment">// The map task only takes care of converting the shuffle data file into multiple      // block push requests. It delegates pushing the blocks to a different thread-pool -      // ShuffleBlockPusher.BLOCK_PUSHER_POOL.      if (!dep.shuffleMergeFinalized) &#123;  </span></span><br><span class="line">        manager.shuffleBlockResolver <span class="keyword">match</span> &#123;  </span><br><span class="line">          <span class="keyword">case</span> resolver: <span class="type">IndexShuffleBlockResolver</span> =&gt;  </span><br><span class="line">            logInfo(<span class="string">s&quot;Shuffle merge enabled with <span class="subst">$&#123;dep.getMergerLocs.size&#125;</span> merger locations &quot;</span> +  </span><br><span class="line">              <span class="string">s&quot; for stage <span class="subst">$&#123;context.stageId()&#125;</span> with shuffle ID <span class="subst">$&#123;dep.shuffleId&#125;</span>&quot;</span>)  </span><br><span class="line">            logDebug(<span class="string">s&quot;Starting pushing blocks for the task <span class="subst">$&#123;context.taskAttemptId()&#125;</span>&quot;</span>)  </span><br><span class="line">            <span class="keyword">val</span> dataFile = resolver.getDataFile(dep.shuffleId, mapId)  </span><br><span class="line">            <span class="keyword">new</span> <span class="type">ShuffleBlockPusher</span>(<span class="type">SparkEnv</span>.get.conf)  </span><br><span class="line">              .initiateBlockPush(dataFile, writer.getPartitionLengths(), dep, partition.index)  </span><br><span class="line">          <span class="keyword">case</span> _ =&gt;  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    mapStatus.get  </span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;  </span><br><span class="line">      <span class="keyword">try</span> &#123;  </span><br><span class="line">        <span class="keyword">if</span> (writer != <span class="literal">null</span>) &#123;  </span><br><span class="line">          writer.stop(success = <span class="literal">false</span>)  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;  </span><br><span class="line">          log.debug(<span class="string">&quot;Could not stop writer&quot;</span>, e)  </span><br><span class="line">      &#125;  </span><br><span class="line">      <span class="keyword">throw</span> e  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在内部其实会调 ShuffleWriter 的 write 方法，对执行 rdd.iterator ()方法后的结果输出到对应的位置，供后续的 Task （ResultTask）进行操作。</p>
<h4 id="rdd-iterator"><a href="#rdd-iterator" class="headerlink" title="rdd.iterator"></a>rdd.iterator</h4><p>我们先来看 ShuffleMapTask 中的 rdd 计算。</p>
<p>org.apache.spark.rdd.RDD#iterator</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should &#x27;&#x27;not&#x27;&#x27; be called by users directly, but is available for implementers of custom  </span></span><br><span class="line"><span class="comment"> * subclasses of RDD. */</span></span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;  </span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;  </span><br><span class="line">    getOrCompute(split, context)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    computeOrReadCheckpoint(split, context)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于无论是 getOrCompute 还是 computeOrReadCheckpoint 最终都会到 computeOrReadCheckpoint 中，所以我们直接看 computeOrReadCheckpoint 方法。</p>
<p>org.apache.spark.rdd.RDD#computeOrReadCheckpoint</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing. */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">computeOrReadCheckpoint</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] =  </span><br><span class="line">&#123;  </span><br><span class="line">  <span class="keyword">if</span> (isCheckpointedAndMaterialized) &#123;  </span><br><span class="line">    firstParent[<span class="type">T</span>].iterator(split, context)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    compute(split, context)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在内部首先其实就是判断是从 Checkpint 中读取还是通过计算而来。</p>
<p>compute () 方法在 RDD 类内是一个抽象方法，不同的 RDD 的子类会实现这个方法。</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20240813173351.png" alt="image.png"></p>
<p>我们以 MapPartitionsRDD 为例：</p>
<p>org.apache.spark.rdd.MapPartitionsRDD#compute</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">U</span>] =  </span><br><span class="line">  f(context, split.index, firstParent[<span class="type">T</span>].iterator(split, context))</span><br></pre></td></tr></table></figure>
<p>可以看到其实调用的是 f（也就是 map 中自定义的方法）。</p>
<h4 id="ShuffleWriter-write"><a href="#ShuffleWriter-write" class="headerlink" title="ShuffleWriter.write"></a>ShuffleWriter.write</h4><p>ShuffleWriter.write 中的 write 其实也是一个抽象方法，或者说 ShuffleWriter 本身就是一个抽象类。所以其实本质上调的是其实现类的 write 方法。</p>
<p>ShuffleWriter 的实现类其实就是我们知道的 Spark 中的几种 Shuffle 机制。</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20240813193644.png" alt="image.png"></p>
<p>这里我们以默认的 SortShuffleWriter 为例。</p>
<p>org.apache.spark.shuffle.sort.SortShuffleWriter#write</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Write a bunch of records to this task&#x27;s output */</span>  </span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  sorter = <span class="keyword">if</span> (dep.mapSideCombine) &#123;  </span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](  </span><br><span class="line">      context, dep.aggregator, <span class="type">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="comment">// In this case we pass neither an aggregator nor an ordering to the sorter, because we don&#x27;t  </span></span><br><span class="line">    <span class="comment">// care whether the keys get sorted in each partition; that will be done on the reduce side    // if the operation being run is sortByKey.    new ExternalSorter[K, V, V](  </span></span><br><span class="line">      context, aggregator = <span class="type">None</span>, <span class="type">Some</span>(dep.partitioner), ordering = <span class="type">None</span>, dep.serializer)  </span><br><span class="line">  &#125;  </span><br><span class="line">  sorter.insertAll(records)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Don&#x27;t bother including the time to open the merged output file in the shuffle write time,  </span></span><br><span class="line">  <span class="comment">// because it just opens a single file, so is typically too fast to measure accurately  // (see SPARK-3570).  val mapOutputWriter = shuffleExecutorComponents.createMapOutputWriter(  </span></span><br><span class="line">    dep.shuffleId, mapId, dep.partitioner.numPartitions)  </span><br><span class="line">  sorter.writePartitionedMapOutput(dep.shuffleId, mapId, mapOutputWriter)  </span><br><span class="line">  partitionLengths = mapOutputWriter.commitAllPartitions(sorter.getChecksums).getPartitionLengths  </span><br><span class="line">  mapStatus = <span class="type">MapStatus</span>(blockManager.shuffleServerId, partitionLengths, mapId)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到在这个 write 方法中其实就是调用 ExternalSorter 的 insertAll，将 RDD 计算得到的 records 结果全部写入到其中。</p>
<p>ExternalSorter 是 spark 中的外部排序器，主要负责对 RDD 计算产生的数据（map）进行排序和输出，同时当计算产生的数据量过大时会进行溢写操作（类似 mapreduce 中的 shuffle 操作，但是需要注意的是 mapreduce 中的 spill 时必须发生的，spark 中的 spill 是在超出一定设定的阈值之后才会发生）。</p>
<h4 id="ExternalSorter"><a href="#ExternalSorter" class="headerlink" title="ExternalSorter"></a>ExternalSorter</h4><p>org.apache.spark.util.collection.ExternalSorter#insertAll</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertAll</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> stop combining if we find that the reduction factor isn&#x27;t high  </span></span><br><span class="line">  <span class="keyword">val</span> shouldCombine = aggregator.isDefined  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (shouldCombine) &#123;  </span><br><span class="line">    <span class="comment">// Combine values in-memory first using our AppendOnlyMap  </span></span><br><span class="line">    <span class="keyword">val</span> mergeValue = aggregator.get.mergeValue  </span><br><span class="line">    <span class="keyword">val</span> createCombiner = aggregator.get.createCombiner  </span><br><span class="line">    <span class="keyword">var</span> kv: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>] = <span class="literal">null</span>  </span><br><span class="line">    <span class="keyword">val</span> update = (hadValue: <span class="type">Boolean</span>, oldValue: <span class="type">C</span>) =&gt; &#123;  </span><br><span class="line">      <span class="keyword">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class="keyword">else</span> createCombiner(kv._2)  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;  </span><br><span class="line">      addElementsRead()  </span><br><span class="line">      kv = records.next()  </span><br><span class="line">      map.changeValue((getPartition(kv._1), kv._1), update)  </span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">true</span>)  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="comment">// Stick values into our buffer  </span></span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;  </span><br><span class="line">      addElementsRead()  </span><br><span class="line">      <span class="keyword">val</span> kv = records.next()  </span><br><span class="line">      buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class="type">C</span>])  </span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">false</span>)  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到在 insertAll 内部，其实首先判断是否需要 map 端聚合，这里是否有 map-side combine 是由几个要素决定的：</p>
<ol>
<li>是否定义了 Aggregator。<code>Aggregator</code> 是用于定义如何对键值对进行聚合操作的类。在 <code>ShuffleMapTask</code> 中，如果为任务指定了 <code>Aggregator</code>，那么 Spark 会在 map 阶段对数据进行预聚合，即在 map 端进行 combine 操作。这个操作可以减少数据量，在发送数据到 reduce 阶段之前就尽可能合并相同 key 的数据。<br><code>Aggregator</code> 的关键作用是定义了如何将具有相同 key 的值进行局部聚合。对于常见的 reduceByKey、aggregateByKey 等操作，这个聚合器会被传递给 <code>ExternalSorter</code>，用于 map 端的 combine 操作。</li>
<li>是否启用了 map-side combine。是否启用其实还是由具体的 shuffle 算子决定。如果使用了启用 map-side combine 的 shuffle 操作，比如 <code>reduceByKey</code>、<code>aggregateByKey</code>，则会进行 map 端的 combine 操作；而某些操作如 <code>groupByKey</code> 并不会在 map 端进行 combine，因为它只是将数据分组，而不进行聚合。<br>对于 <code>reduceByKey</code> 或 <code>aggregateByKey</code>，因为它们具备聚合语义，因此 Spark 会自动尝试在 map 端对相同 key 的数据进行 combine，从而减少需要传输到 reduce 端的数据量。</li>
</ol>
<p>如果需要进行 map-side combine 那么就会调用其 changeValue 方法，将输入的 records 输入到一个 map（PartitionedAppendOnlyMap）进行 aggregate 操作，反之就只是简单得 insert 到一个 buffer（PartitionedPairBuffer）。</p>
<p>PartitionedAppendOnlyMap 是是一个按分区存储的 <code>AppendOnlyMap</code>。其内部实现了增量更新的逻辑。每次插入键值对时，它会先检查该键是否已经存在。如果存在，则根据 <code>Aggregator</code> 中的规则将新值与已有值进行聚合；如果不存在，则直接插入。这使得 <code>PartitionedAppendOnlyMap</code> 能够高效地进行 map 端的聚合操作。</p>
<p>org.apache. spark. util. collection. AppendOnlyMap#changeValue</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Set the value for key to updateFunc(hadValue, oldValue), where oldValue will be the old value * for key, if any, or null otherwise. Returns the newly updated value. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">changeValue</span></span>(key: <span class="type">K</span>, updateFunc: (<span class="type">Boolean</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">V</span> = &#123;  </span><br><span class="line">  assert(!destroyed, destructionMessage)  </span><br><span class="line">  <span class="keyword">val</span> k = key.asInstanceOf[<span class="type">AnyRef</span>]  </span><br><span class="line">  <span class="keyword">if</span> (k.eq(<span class="literal">null</span>)) &#123;  </span><br><span class="line">    <span class="keyword">if</span> (!haveNullValue) &#123;  </span><br><span class="line">      incrementSize()  </span><br><span class="line">    &#125;  </span><br><span class="line">    nullValue = updateFunc(haveNullValue, nullValue)  </span><br><span class="line">    haveNullValue = <span class="literal">true</span>  </span><br><span class="line">    <span class="keyword">return</span> nullValue  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">var</span> pos = rehash(k.hashCode) &amp; mask  </span><br><span class="line">  <span class="keyword">var</span> i = <span class="number">1</span>  </span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;  </span><br><span class="line">    <span class="keyword">val</span> curKey = data(<span class="number">2</span> * pos)  </span><br><span class="line">    <span class="keyword">if</span> (curKey.eq(<span class="literal">null</span>)) &#123;  </span><br><span class="line">      <span class="keyword">val</span> newValue = updateFunc(<span class="literal">false</span>, <span class="literal">null</span>.asInstanceOf[<span class="type">V</span>])  </span><br><span class="line">      data(<span class="number">2</span> * pos) = k  </span><br><span class="line">      data(<span class="number">2</span> * pos + <span class="number">1</span>) = newValue.asInstanceOf[<span class="type">AnyRef</span>]  </span><br><span class="line">      incrementSize()  </span><br><span class="line">      <span class="keyword">return</span> newValue  </span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (k.eq(curKey) || k.equals(curKey)) &#123;  </span><br><span class="line">      <span class="keyword">val</span> newValue = updateFunc(<span class="literal">true</span>, data(<span class="number">2</span> * pos + <span class="number">1</span>).asInstanceOf[<span class="type">V</span>])  </span><br><span class="line">      data(<span class="number">2</span> * pos + <span class="number">1</span>) = newValue.asInstanceOf[<span class="type">AnyRef</span>]  </span><br><span class="line">      <span class="keyword">return</span> newValue  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      <span class="keyword">val</span> delta = i  </span><br><span class="line">      pos = (pos + delta) &amp; mask  </span><br><span class="line">      i += <span class="number">1</span>  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="literal">null</span>.asInstanceOf[<span class="type">V</span>] <span class="comment">// Never reached but needed to keep compiler happy  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>org. apache. spark. util. collection. PartitionedPairBuffer#insert</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Add an element into the buffer */</span>  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(partition: <span class="type">Int</span>, key: <span class="type">K</span>, value: <span class="type">V</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="keyword">if</span> (curSize == capacity) &#123;  </span><br><span class="line">    growArray()  </span><br><span class="line">  &#125;  </span><br><span class="line">  data(<span class="number">2</span> * curSize) = (partition, key.asInstanceOf[<span class="type">AnyRef</span>])  </span><br><span class="line">  data(<span class="number">2</span> * curSize + <span class="number">1</span>) = value.asInstanceOf[<span class="type">AnyRef</span>]  </span><br><span class="line">  curSize += <span class="number">1</span>  </span><br><span class="line">  afterUpdate()  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到其实内部都是使用一个 data 数组来存储对应的值。</p>
<p>从 insertAll 方法中可以看到，无论是 map-side combine 还是 buffer 的 insert 之后都会调用 maybeSpillCollection 方法来判断是否需要进行溢写的操作。</p>
<p>org.apache.spark.util.collection.ExternalSorter#maybeSpillCollection<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Spill the current in-memory collection to disk if needed. * * @param usingMap whether we&#x27;re using a map or buffer as our current in-memory collection  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpillCollection</span></span>(usingMap: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="keyword">var</span> estimatedSize = <span class="number">0</span>L  </span><br><span class="line">  <span class="keyword">if</span> (usingMap) &#123;  </span><br><span class="line">    estimatedSize = map.estimateSize()  </span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(map, estimatedSize)) &#123;  </span><br><span class="line">      map = <span class="keyword">new</span> <span class="type">PartitionedAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    estimatedSize = buffer.estimateSize()  </span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(buffer, estimatedSize)) &#123;  </span><br><span class="line">      buffer = <span class="keyword">new</span> <span class="type">PartitionedPairBuffer</span>[<span class="type">K</span>, <span class="type">C</span>]  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;  </span><br><span class="line">    _peakMemoryUsedBytes = estimatedSize  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>可以看到在 maybeSpillCollection 内部其实就是调用了 maybeSpill 判断是否需要进行 spill，同时进行 spill。</p>
<p>org.apache.spark.util.collection.Spillable#maybeSpill</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Spills the current in-memory collection to disk if needed. Attempts to acquire more * memory before spilling. * * @param collection collection to spill to disk  </span></span><br><span class="line"><span class="comment"> * @param currentMemory estimated size of the collection in bytes  </span></span><br><span class="line"><span class="comment"> * @return true if `collection` was spilled to disk; false otherwise  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpill</span></span>(collection: <span class="type">C</span>, currentMemory: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;  </span><br><span class="line">  <span class="keyword">var</span> shouldSpill = <span class="literal">false</span>  </span><br><span class="line">  <span class="keyword">if</span> (elementsRead % <span class="number">32</span> == <span class="number">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;  </span><br><span class="line">    <span class="comment">// Claim up to double our current memory from the shuffle memory pool  </span></span><br><span class="line">    <span class="keyword">val</span> amountToRequest = <span class="number">2</span> * currentMemory - myMemoryThreshold  </span><br><span class="line">    <span class="keyword">val</span> granted = acquireMemory(amountToRequest)  </span><br><span class="line">    myMemoryThreshold += granted  </span><br><span class="line">    <span class="comment">// If we were granted too little memory to grow further (either tryToAcquire returned 0,  </span></span><br><span class="line">    <span class="comment">// or we already had more memory than myMemoryThreshold), spill the current collection    shouldSpill = currentMemory &gt;= myMemoryThreshold  </span></span><br><span class="line">  &#125;  </span><br><span class="line">  shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold  </span><br><span class="line">  <span class="comment">// Actually spill  </span></span><br><span class="line">  <span class="keyword">if</span> (shouldSpill) &#123;  </span><br><span class="line">    _spillCount += <span class="number">1</span>  </span><br><span class="line">    logSpillage(currentMemory)  </span><br><span class="line">    spill(collection)  </span><br><span class="line">    _elementsRead = <span class="number">0</span>  </span><br><span class="line">    _memoryBytesSpilled += currentMemory  </span><br><span class="line">    releaseMemory()  </span><br><span class="line">  &#125;  </span><br><span class="line">  shouldSpill  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 maybeSpill 方法内部其实就是判断当前需要的内存大小是否大于初始化分配的内存大小（currentMemory &gt;= myMemoryThreshold），而 myMemoryThreshold 其实是通过 spark 的参数 spark.shuffle.spill.initialMemoryThreshold 进行设置，默认为默认是5*1024*1024 (5MB)。之后，如果需要 spill，就会调用 spill 方法进行 spill。</p>
<p>org.apache.spark.util.collection.ExternalSorter#spill</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Spill our in-memory collection to a sorted file that we can merge later. * We add this file into `spilledFiles` to find it later.  </span></span><br><span class="line"><span class="comment"> * * @param collection whichever collection we&#x27;re using (map or buffer)  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">override</span> <span class="keyword">protected</span>[<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">spill</span></span>(collection: <span class="type">WritablePartitionedPairCollection</span>[<span class="type">K</span>, <span class="type">C</span>]): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="keyword">val</span> inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)  </span><br><span class="line">  <span class="keyword">val</span> spillFile = spillMemoryIteratorToDisk(inMemoryIterator)  </span><br><span class="line">  spills += spillFile</span><br></pre></td></tr></table></figure>
<p>在 spill 方法里首先会调用 destructiveSortedWritablePartitionedIterator 方法，进行排序，然后再进行溢写。具体来说，这里面会对缓存 A 的数据先进行排序，然后在写入缓存 B 中，满足条件在 flush 到磁盘中，条件是已经写到缓存 B 中的数据条数等于 <code>spark.shuffle.spill.batchSize</code>，默认是 10000 条。</p>
<p>org.apache.spark.util.collection.ExternalSorter#spillMemoryIteratorToDisk</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Spill contents of in-memory iterator to a temporary file on disk. */</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">spillMemoryIteratorToDisk</span></span>(inMemoryIterator: <span class="type">WritablePartitionedIterator</span>[<span class="type">K</span>, <span class="type">C</span>])  </span><br><span class="line">    : <span class="type">SpilledFile</span> = &#123;  </span><br><span class="line">  <span class="comment">// Because these files may be read during shuffle, their compression must be controlled by  </span></span><br><span class="line">  <span class="comment">// spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use  // createTempShuffleBlock here; see SPARK-3426 for more context.  val (blockId, file) = diskBlockManager.createTempShuffleBlock()  </span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">// These variables are reset after each flush  </span></span><br><span class="line">  <span class="keyword">var</span> objectsWritten: <span class="type">Long</span> = <span class="number">0</span>  </span><br><span class="line">  <span class="keyword">val</span> spillMetrics: <span class="type">ShuffleWriteMetrics</span> = <span class="keyword">new</span> <span class="type">ShuffleWriteMetrics</span>  </span><br><span class="line">  <span class="keyword">val</span> writer: <span class="type">DiskBlockObjectWriter</span> =  </span><br><span class="line">    blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// List of batch sizes (bytes) in the order they are written to disk  </span></span><br><span class="line">  <span class="keyword">val</span> batchSizes = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// How many elements we have in each partition  </span></span><br><span class="line">  <span class="keyword">val</span> elementsPerPartition = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Long</span>](numPartitions)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Flush the disk writer&#x27;s contents to disk, and update relevant variables.  </span></span><br><span class="line">  <span class="comment">// The writer is committed at the end of this process.  def flush(): Unit = &#123;  </span></span><br><span class="line">    <span class="keyword">val</span> segment = writer.commitAndGet()  </span><br><span class="line">    batchSizes += segment.length  </span><br><span class="line">    _diskBytesSpilled += segment.length  </span><br><span class="line">    objectsWritten = <span class="number">0</span>  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">var</span> success = <span class="literal">false</span>  </span><br><span class="line">  <span class="keyword">try</span> &#123;  </span><br><span class="line">    <span class="keyword">while</span> (inMemoryIterator.hasNext) &#123;  </span><br><span class="line">      <span class="keyword">val</span> partitionId = inMemoryIterator.nextPartition()  </span><br><span class="line">      require(partitionId &gt;= <span class="number">0</span> &amp;&amp; partitionId &lt; numPartitions,  </span><br><span class="line">        <span class="string">s&quot;partition Id: <span class="subst">$&#123;partitionId&#125;</span> should be in the range [0, <span class="subst">$&#123;numPartitions&#125;</span>)&quot;</span>)  </span><br><span class="line">      inMemoryIterator.writeNext(writer)  </span><br><span class="line">      elementsPerPartition(partitionId) += <span class="number">1</span>  </span><br><span class="line">      objectsWritten += <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">      <span class="keyword">if</span> (objectsWritten == serializerBatchSize) &#123;  </span><br><span class="line">        flush()  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">if</span> (objectsWritten &gt; <span class="number">0</span>) &#123;  </span><br><span class="line">      flush()  </span><br><span class="line">      writer.close()  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      writer.revertPartialWritesAndClose()  </span><br><span class="line">    &#125;  </span><br><span class="line">    success = <span class="literal">true</span>  </span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;  </span><br><span class="line">    <span class="keyword">if</span> (!success) &#123;  </span><br><span class="line">      <span class="comment">// This code path only happens if an exception was thrown above before we set success;  </span></span><br><span class="line">      <span class="comment">// close our stuff and let the exception be thrown further      writer.closeAndDelete()  </span></span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="type">SpilledFile</span>(file, blockId, batchSizes.toArray, elementsPerPartition)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>spillMemoryIteratorToDisk 这个方法才是最终进行溢写操作的方法。</p>
<h3 id="ResultTask"><a href="#ResultTask" class="headerlink" title="ResultTask"></a>ResultTask</h3><p>前面介绍的是 ShuflleMapTask，主要内容就是对 Task 产生的 records 进行一个 write 的过程。但是在 ResultTask 中其实是没有这样的一个过程的。</p>
<p>在 ResultTask 中其实就是调用 RDD 实现类封装的 func 方法，然后就得到的数据返回即可。</p>
<p>org.apache.spark.scheduler.ResultTask#runTask</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span></span>(context: <span class="type">TaskContext</span>): <span class="type">U</span> = &#123;  </span><br><span class="line">  <span class="comment">// Deserialize the RDD and the func using the broadcast variables.  </span></span><br><span class="line">  <span class="keyword">val</span> threadMXBean = <span class="type">ManagementFactory</span>.getThreadMXBean  </span><br><span class="line">  <span class="keyword">val</span> deserializeStartTimeNs = <span class="type">System</span>.nanoTime()  </span><br><span class="line">  <span class="keyword">val</span> deserializeStartCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;  </span><br><span class="line">    threadMXBean.getCurrentThreadCpuTime  </span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="number">0</span>L  </span><br><span class="line">  <span class="keyword">val</span> ser = <span class="type">SparkEnv</span>.get.closureSerializer.newInstance()  </span><br><span class="line">  <span class="keyword">val</span> (rdd, func) = ser.deserialize[(<span class="type">RDD</span>[<span class="type">T</span>], (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>)](  </span><br><span class="line">    <span class="type">ByteBuffer</span>.wrap(taskBinary.value), <span class="type">Thread</span>.currentThread.getContextClassLoader)  </span><br><span class="line">  _executorDeserializeTimeNs = <span class="type">System</span>.nanoTime() - deserializeStartTimeNs  </span><br><span class="line">  _executorDeserializeCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;  </span><br><span class="line">    threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime  </span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="number">0</span>L  </span><br><span class="line">  </span><br><span class="line">  func(context, rdd.iterator(partition, context))  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从 ResultTask 的 runTask 方法就可以看到其实就是将 Task 进行反序列化得到对应的 RDD 和 func，然后调用 func 进行计算即可（本质上还是调用 rdd 的 compute）。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.JPG" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/07/29/Spark-%E6%BA%90%E7%A0%81-SparkContext/" title="Spark-源码-SparkContext"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Spark-源码-SparkContext</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-Cache%E4%B8%8Echeckpoint/" title="Spark-基础-Cache 与 Checkpoint"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-Cache 与 Checkpoint</div></div></a></div><div><a href="/2024/03/20/Spark-%E5%9F%BA%E7%A1%80-Shuffle%E6%9C%BA%E5%88%B6/" title="Spark-基础-Shuffle机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-20</div><div class="title">Spark-基础-Shuffle机制</div></div></a></div><div><a href="/2024/03/20/Spark-%E5%9F%BA%E7%A1%80-%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/" title="Spark-基础-核心编程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-20</div><div class="title">Spark-基础-核心编程</div></div></a></div><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-%E7%B4%AF%E5%8A%A0%E5%99%A8%E4%B8%8E%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/" title="Spark-基础-累加器和Broadcast"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-累加器和Broadcast</div></div></a></div><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F/" title="Spark-基础-运行模式"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-运行模式</div></div></a></div><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-%E9%80%9A%E8%AE%AF%E6%9C%BA%E5%88%B6/" title="Spark-基础-通信机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-通信机制</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CHLi</div><div class="author-info__description">Welcome to Mr.Li's blog</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">72</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/MLiLay"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome! This is MLiLay's Blog.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-%E6%BA%90%E7%A0%81"><span class="toc-number">1.</span> <span class="toc-text">Spark 源码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Task-%E6%8F%90%E4%BA%A4"><span class="toc-number">1.1.</span> <span class="toc-text">Task 提交</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Task-%E8%BF%90%E8%A1%8C"><span class="toc-number">1.2.</span> <span class="toc-text">Task 运行</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ShuffleMapTask"><span class="toc-number">1.2.1.</span> <span class="toc-text">ShuffleMapTask</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#rdd-iterator"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">rdd.iterator</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ShuffleWriter-write"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">ShuffleWriter.write</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ExternalSorter"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">ExternalSorter</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResultTask"><span class="toc-number">1.2.2.</span> <span class="toc-text">ResultTask</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/12/Spark-%E6%BA%90%E7%A0%81-Task%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B(Map%E7%AB%AF)/" title="Spark-源码-Task执行过程（Map）">Spark-源码-Task执行过程（Map）</a><time datetime="2024-09-12T12:22:00.000Z" title="发表于 2024-09-12 20:22:00">2024-09-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/29/Spark-%E6%BA%90%E7%A0%81-SparkContext/" title="Spark-源码-SparkContext">Spark-源码-SparkContext</a><time datetime="2024-07-29T06:27:54.293Z" title="发表于 2024-07-29 14:27:54">2024-07-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/22/Spark-%E6%BA%90%E7%A0%81-persist%E4%B8%8Echeckpoint/" title="Spark-源码-persist与checkpoint">Spark-源码-persist与checkpoint</a><time datetime="2024-07-22T09:02:52.238Z" title="发表于 2024-07-22 17:02:52">2024-07-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/11/Celeborn/" title="Celeborn">Celeborn</a><time datetime="2024-07-11T12:46:48.425Z" title="发表于 2024-07-11 20:46:48">2024-07-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/11/Spark-%E6%BA%90%E7%A0%81-Broadcast/" title="Spark-源码-Broadcast">Spark-源码-Broadcast</a><time datetime="2024-07-11T12:29:06.531Z" title="发表于 2024-07-11 20:29:06">2024-07-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By CHLi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>