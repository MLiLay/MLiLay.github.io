<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Spark-源码-内存体系 | Mr.Li's blog</title><meta name="author" content="CHLi"><meta name="copyright" content="CHLi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="内存体系概述在 spark 中内存主要分为堆内内存和堆外内存。 堆外内存与堆内内存区别堆内内存（On-Heap Memory）定义堆内内存是 JVM 的堆内存，由 JVM 管理，用于存储对象和数据。Spark 的绝大部分运行时数据默认保存在堆内存中。 特点 管理方式： 完全由 JVM 和垃圾回收器（GC）管理，内存的分配和释放由 JVM 自动处理。 垃圾回收： 堆内存会受到 JVM 的 GC 机制">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark-源码-内存体系">
<meta property="og:url" content="http://example.com/2024/11/28/Spark-%E6%BA%90%E7%A0%81-%E5%86%85%E5%AD%98%E4%BD%93%E7%B3%BB/index.html">
<meta property="og:site_name" content="Mr.Li&#39;s blog">
<meta property="og:description" content="内存体系概述在 spark 中内存主要分为堆内内存和堆外内存。 堆外内存与堆内内存区别堆内内存（On-Heap Memory）定义堆内内存是 JVM 的堆内存，由 JVM 管理，用于存储对象和数据。Spark 的绝大部分运行时数据默认保存在堆内存中。 特点 管理方式： 完全由 JVM 和垃圾回收器（GC）管理，内存的分配和释放由 JVM 自动处理。 垃圾回收： 堆内存会受到 JVM 的 GC 机制">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/avatar.JPG">
<meta property="article:published_time" content="2024-11-28T08:32:57.784Z">
<meta property="article:modified_time" content="2024-11-28T08:40:11.758Z">
<meta property="article:author" content="CHLi">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/avatar.JPG"><link rel="shortcut icon" href="/img/spaceshuttle.png"><link rel="canonical" href="http://example.com/2024/11/28/Spark-%E6%BA%90%E7%A0%81-%E5%86%85%E5%AD%98%E4%BD%93%E7%B3%BB/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark-源码-内存体系',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-28 16:40:11'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/topGraph.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Mr.Li's blog"><span class="site-name">Mr.Li's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark-源码-内存体系</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-28T08:32:57.784Z" title="发表于 2024-11-28 16:32:57">2024-11-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-28T08:40:11.758Z" title="更新于 2024-11-28 16:40:11">2024-11-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark-源码-内存体系"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="内存体系概述"><a href="#内存体系概述" class="headerlink" title="内存体系概述"></a>内存体系概述</h1><p>在 spark 中内存主要分为堆内内存和堆外内存。</p>
<h2 id="堆外内存与堆内内存区别"><a href="#堆外内存与堆内内存区别" class="headerlink" title="堆外内存与堆内内存区别"></a>堆外内存与堆内内存区别</h2><h3 id="堆内内存（On-Heap-Memory）"><a href="#堆内内存（On-Heap-Memory）" class="headerlink" title="堆内内存（On-Heap Memory）"></a>堆内内存（On-Heap Memory）</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>堆内内存是 JVM 的堆内存，由 JVM 管理，用于存储对象和数据。Spark 的绝大部分运行时数据默认保存在堆内存中。</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li>管理方式： 完全由 JVM 和垃圾回收器（GC）管理，内存的分配和释放由 JVM 自动处理。</li>
<li>垃圾回收： 堆内存会受到 JVM 的 GC 机制影响。当对象过多时，会触发垃圾回收，从而可能导致性能下降（例如，长时间的 GC 暂停）。</li>
<li>配置参数：JVM 的堆内存大小由 -Xmx 和 -Xms 参数设置。</li>
<li>在 Spark 中，堆内内存的主要配置是 spark. executor. memory，用于设置每个 Executor 分配的堆内内存。</li>
<li>使用场景：默认情况下，所有数据（RDD、缓存数据）都存储在堆内内存中。</li>
</ul>
<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><ul>
<li>优点：使用方便，开发者无需直接管理内存。</li>
<li>缺点：容易受到 GC 的影响，特别是在大数据量、高频率对象分配的场景中。</li>
</ul>
<h3 id="堆外内存（Off-Heap-Memory）"><a href="#堆外内存（Off-Heap-Memory）" class="headerlink" title="堆外内存（Off-Heap Memory）"></a>堆外内存（Off-Heap Memory）</h3><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p>堆外内存是 JVM 堆之外的内存，由操作系统直接管理。Spark 可以通过配置将部分数据存储在堆外内存中。  </p>
<h4 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h4><ul>
<li>管理方式：由 Spark 和底层库（如 Netty、Unsafe）直接操作分配和释放，不受 JVM 垃圾回收的控制。</li>
<li>垃圾回收：不会受到 JVM 的 GC 影响，从而减少了垃圾回收暂停的影响。</li>
<li>配置参数：<ul>
<li>是否启用堆外内存：spark. memory. offHeap. enabled=true</li>
<li>堆外内存大小：spark. memory. offHeap. size，用于设置堆外内存的大小。</li>
</ul>
</li>
<li>使用场景：<ul>
<li>在缓存大数据量时，为减少 GC 影响，将数据存储在堆外内存。</li>
<li>与本地存储或外部系统交互时（如 DirectByteBuffer）。</li>
</ul>
</li>
</ul>
<h4 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h4><ul>
<li>优点：减少了 GC 带来的延迟，提高了内存管理的灵活性和性能。堆外内存可以有效避免堆内存溢出的问题（如 OutOfMemoryError: Java heap space）。</li>
<li>缺点：编码复杂度更高，内存分配和释放需要特别关注，可能导致内存泄漏。堆外内存的分配速度可能慢于堆内存。</li>
</ul>
<h3 id="StaticMemoryManager"><a href="#StaticMemoryManager" class="headerlink" title="StaticMemoryManager"></a>StaticMemoryManager</h3><p>在 Spark1.6 之前的版本中使用的都是 StaticMemoryManager，在 Spark 3.0 之后 UnifiedMemoryManager 完全取代 StaticMemoryManager。</p>
<p>整个内存区间分成了：存储内存（storage memory,）、执行内存（execution memory）和其他内存（other memory）的大小在应用程序处理过程中是固定的，但用户可以在应用程序启动之前进行配置。这三部分内存的作用及占比如下：storage memory：主要用于缓存数据块以提高性能，同时也用于连续不断地广播或发送大的任务结果。通过spark.storage.memoryFraction进行配置，默认为0.6。</p>
<p>整体分布如下图：</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20241127153350.png" alt="image.png"></p>
<h3 id="UnifiedMemoryManager"><a href="#UnifiedMemoryManager" class="headerlink" title="UnifiedMemoryManager"></a>UnifiedMemoryManager</h3><p>UnifiedMemoryManager 与 StaticMemoryManager 在内存的分配上大致相同，最大的区别其实是对于 Storage Memory 和 Execution Memory 两者在容量不足时，可以去侵占另一个的内存。</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20241127153833.png" alt="image.png"></p>
<p>其内存结构大致可以分为下面几类：</p>
<h4 id="系统预留内存-System-Reserved-Memory"><a href="#系统预留内存-System-Reserved-Memory" class="headerlink" title="系统预留内存 (System Reserved Memory)"></a>系统预留内存 (System Reserved Memory)</h4><p>系统预留内存用来存储 Spark 内部对象。其大小在代码中是写死的，其值等于 300 MB，这个值是不能修改的&lt;测试环境下，我们可以通过 spark.testing.reservedMemory 参数进行修改&gt;；如果 Executor 分配的内存小于 1.5 * 300 = 450 M 时，Executor 将无法执行。</p>
<h4 id="存储内存-Storage-Memory"><a href="#存储内存-Storage-Memory" class="headerlink" title="存储内存 (Storage Memory)"></a>存储内存 (Storage Memory)</h4><p>存储内存主要用于存储 spark 的 cache 数据，例如 RDD 的缓存、广播（Broadcast）数据、和 unroll 数据。内存占比为 UsableMemory <em> spark. memory. fraction </em> spark. memory. storageFraction，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的 30%[1 <em> 0.6 </em> 0.5 = 0.3]</p>
<h4 id="执行内存-Execution-Memory"><a href="#执行内存-Execution-Memory" class="headerlink" title="执行内存 (Execution Memory)"></a>执行内存 (Execution Memory)</h4><p>执行内存主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据。内存占比为 UsableMemory <em> spark. memory. fraction </em> (1 - spark. memory. storageFraction)，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的 30%（1 <em> 0.6 </em> (1 - 0.5) = 0.3</p>
<h4 id="其他-用户内存-Other-User-Memory"><a href="#其他-用户内存-Other-User-Memory" class="headerlink" title="其他/用户内存 (Other/User Memory)"></a>其他/用户内存 (Other/User Memory)</h4><p>主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息。内存占比为 <code>UsableMemory * (1 - spark.memory.fraction)</code>，在Spark2+ 中，默认占可用内存的40%（<code>1 * (1 - 0.6) = 0.4</code>）</p>
<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><p>以下内容 Spark 版本为 3.3.2</p>
<p>内存模型大致大致可以从下图出发：</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20241127154452.png" alt="image.png"></p>
<p>可以看到 Spark 的内存管理是由一个抽象类 MemoryManager 进行管理，而 UnifiedMemoryManager 其实就是 MemoryManager 的一个子类实现了其内部的抽象方法。</p>
<p>MemoryManager 将 BlockManager（存储）和 TaskMemoryManager（Task 内存管理）串联起来。</p>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>以 Executor 端为例。</p>
<p>当在 NM 中启动一个 Container 并在其中创建 CoarseGrainedExecutorBackend 时，其 run()方法就会创建 SparkEnv（Executor）。</p>
<p>org.apache.spark.executor.CoarseGrainedExecutorBackend#run</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(  </span><br><span class="line">    arguments: <span class="type">Arguments</span>,  </span><br><span class="line">    backendCreateFn: (<span class="type">RpcEnv</span>, <span class="type">Arguments</span>, <span class="type">SparkEnv</span>, <span class="type">ResourceProfile</span>) =&gt;  </span><br><span class="line">      <span class="type">CoarseGrainedExecutorBackend</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="type">Utils</span>.initDaemon(log)  </span><br><span class="line">  </span><br><span class="line">  <span class="type">SparkHadoopUtil</span>.get.runAsSparkUser &#123; () =&gt;  </span><br><span class="line"></span><br><span class="line">	... </span><br><span class="line">   </span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">SparkEnv</span>.createExecutorEnv(driverConf, arguments.executorId, arguments.bindAddress,  </span><br><span class="line">      arguments.hostname, arguments.cores, cfg.ioEncryptionKey, isLocal = <span class="literal">false</span>)  </span><br><span class="line"></span><br><span class="line">	...</span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>org.apache.spark.SparkEnv#createExecutorEnv</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">createExecutorEnv</span></span>(  </span><br><span class="line">    conf: <span class="type">SparkConf</span>,  </span><br><span class="line">    executorId: <span class="type">String</span>,  </span><br><span class="line">    bindAddress: <span class="type">String</span>,  </span><br><span class="line">    hostname: <span class="type">String</span>,  </span><br><span class="line">    numCores: <span class="type">Int</span>,  </span><br><span class="line">    ioEncryptionKey: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Byte</span>]],  </span><br><span class="line">    isLocal: <span class="type">Boolean</span>): <span class="type">SparkEnv</span> = &#123;  </span><br><span class="line">  <span class="keyword">val</span> env = create(  </span><br><span class="line">    conf,  </span><br><span class="line">    executorId,  </span><br><span class="line">    bindAddress,  </span><br><span class="line">    hostname,  </span><br><span class="line">    <span class="type">None</span>,  </span><br><span class="line">    isLocal,  </span><br><span class="line">    numCores,  </span><br><span class="line">    ioEncryptionKey  </span><br><span class="line">  )  </span><br><span class="line">  <span class="type">SparkEnv</span>.set(env)  </span><br><span class="line">  env  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>SparkEnv 的 createExecutorEnv () 方法会调用 create 方法。</p>
<p>org.apache.spark.SparkEnv#create</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(  </span><br><span class="line">    conf: <span class="type">SparkConf</span>,  </span><br><span class="line">    executorId: <span class="type">String</span>,  </span><br><span class="line">    bindAddress: <span class="type">String</span>,  </span><br><span class="line">    advertiseAddress: <span class="type">String</span>,  </span><br><span class="line">    port: <span class="type">Option</span>[<span class="type">Int</span>],  </span><br><span class="line">    isLocal: <span class="type">Boolean</span>,  </span><br><span class="line">    numUsableCores: <span class="type">Int</span>,  </span><br><span class="line">    ioEncryptionKey: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Byte</span>]],  </span><br><span class="line">    listenerBus: <span class="type">LiveListenerBus</span> = <span class="literal">null</span>,  </span><br><span class="line">    mockOutputCommitCoordinator: <span class="type">Option</span>[<span class="type">OutputCommitCoordinator</span>] = <span class="type">None</span>): <span class="type">SparkEnv</span> = &#123;  </span><br><span class="line">  </span><br><span class="line">  ...  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> memoryManager: <span class="type">MemoryManager</span> = <span class="type">UnifiedMemoryManager</span>(conf, numUsableCores)  </span><br><span class="line">  </span><br><span class="line">  ... </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> envInstance = <span class="keyword">new</span> <span class="type">SparkEnv</span>(  </span><br><span class="line">    executorId,  </span><br><span class="line">    rpcEnv,  </span><br><span class="line">    serializer,  </span><br><span class="line">    closureSerializer,  </span><br><span class="line">    serializerManager,  </span><br><span class="line">    mapOutputTracker,  </span><br><span class="line">    shuffleManager,  </span><br><span class="line">    broadcastManager,  </span><br><span class="line">    blockManager,  </span><br><span class="line">    securityManager,  </span><br><span class="line">    metricsSystem,  </span><br><span class="line">    memoryManager,  </span><br><span class="line">    outputCommitCoordinator,  </span><br><span class="line">    conf)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Add a reference to tmp dir created by driver, we will delete this tmp dir when stop() is  </span></span><br><span class="line">  <span class="comment">// called, and we only need to do it for driver. Because driver may run as a service, and if we  // don&#x27;t delete this tmp dir when sc is stopped, then will create too many tmp dirs.  if (isDriver) &#123;  </span></span><br><span class="line">    <span class="keyword">val</span> sparkFilesDir = <span class="type">Utils</span>.createTempDir(<span class="type">Utils</span>.getLocalDir(conf), <span class="string">&quot;userFiles&quot;</span>).getAbsolutePath  </span><br><span class="line">    envInstance.driverTmpDir = <span class="type">Some</span>(sparkFilesDir)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  envInstance  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到就会调用 UnifiedMemoryManager 的 apply 方法创建一个 UnifiedMemoryManager 返回。</p>
<p>在创建 UnifiedMemoryManager 的时候会调用 MemoryManager 的构造方法。这样也就会创建 StorageMemoryPool 和 ExecutionMemoryPool，当然堆内和堆外都有。</p>
<p>StorageMemoryPool 和 ExecutionMemoryPool 都继承自 MemoryPool，这两个类其实就是内存空间的一个抽象，内存有一些 acquireMemory 方法和 freeMemory 方法，同时也会记录可用的内存容量和 used Memory 容量。</p>
<h3 id="内存获取"><a href="#内存获取" class="headerlink" title="内存获取"></a>内存获取</h3><p>在此我们以 ExecutionMemory 的获取为例，StorageMemory 的获取过程类似。</p>
<p>我们在之前的<a href="/2024/08/12/Spark-%E6%BA%90%E7%A0%81-Task%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B(Map%E7%AB%AF)/" title="Spark-源码-Task执行过程（Map）">Spark-源码-Task执行过程（Map）</a> 中其实可以我们可以看到在 ShuffleMapStage 中 Shuffle 的 write 操作可以调用的 ShuffleWriter 有三种，分别是下图三种：</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20241128151600.png" alt="image.png"></p>
<p>ExecutionMemory 的申请往往发生在使用 UnsafeShuffleWriter 时，（<strong>注</strong>：采用哪个 ShuffleWriter 其实是由 org. apache. spark. shuffle. ShuffleWriteProcessor 的 write ()方法中的 getWriter 操作决定的，在这个方法其实中其实就是根据本次 Shuffle 的 hander 决定用哪个 ShuffleWriter，而 handle 其实在是由 RDD 的 Shuffle 算子、分区个数、各种参数开关决定的，具体内容看 Dependency 类中的 102 行和 103 行这句话 <code>val shuffleHandle: ShuffleHandle = \_rdd.context.env.shuffleManager.registerShuffle(shuffleId, this)</code> 这句话，最终的逻辑就在 registerShuffle 这个方法内）。</p>
<p>在 UnsafeShuffleWriter 的 write ()方法中会调用 insertRecordIntoSorter 方法将 Record 插入到 Sorter 中。</p>
<p>org. apache. spark. shuffle. sort. UnsafeShuffleWriter#write </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span>  </span><br><span class="line">public void write(scala.collection.<span class="type">Iterator</span>&lt;<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt;&gt; records) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;  </span><br><span class="line">  <span class="comment">// Keep track of success so we know if we encountered an exception  </span></span><br><span class="line">  <span class="comment">// We do this rather than a standard try/catch/re-throw to handle  // generic throwables.  boolean success = false;  </span></span><br><span class="line">  <span class="keyword">try</span> &#123;  </span><br><span class="line">    <span class="keyword">while</span> (records.hasNext()) &#123;  </span><br><span class="line">      insertRecordIntoSorter(records.next());  </span><br><span class="line">    &#125;  </span><br><span class="line">    closeAndWriteOutput();  </span><br><span class="line">    success = <span class="literal">true</span>;  </span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;  </span><br><span class="line">    <span class="keyword">if</span> (sorter != <span class="literal">null</span>) &#123;  </span><br><span class="line">      <span class="keyword">try</span> &#123;  </span><br><span class="line">        sorter.cleanupResources();  </span><br><span class="line">      &#125; <span class="keyword">catch</span> (<span class="type">Exception</span> e) &#123;  </span><br><span class="line">        <span class="comment">// Only throw this error if we won&#x27;t be masking another  </span></span><br><span class="line">        <span class="comment">// error.        if (success) &#123;  </span></span><br><span class="line">          <span class="keyword">throw</span> e;  </span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">          logger.error(<span class="string">&quot;In addition to a failure during writing, we failed during &quot;</span> +  </span><br><span class="line">                       <span class="string">&quot;cleanup.&quot;</span>, e);  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在内部其实也是调用 UnsafeShuffleWriter 类的 UnsafeShuffleWriter 类的 insertRecord ()方法，在 insertRecord 方法内会先判断是否还可以去获得更大的内存，最后就会调用 TaskMemoryManager 的 allocatePage 方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Write a record to the shuffle sorter. */</span></span><br><span class="line">public void insertRecord(<span class="type">Object</span> recordBase, long recordOffset, int length, int partitionId)  </span><br><span class="line">  <span class="keyword">throws</span> <span class="type">IOException</span> &#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// for tests  </span></span><br><span class="line">  assert(inMemSorter != <span class="literal">null</span>);  </span><br><span class="line">  <span class="keyword">if</span> (inMemSorter.numRecords() &gt;= numElementsForSpillThreshold) &#123;  </span><br><span class="line">    logger.info(<span class="string">&quot;Spilling data because number of spilledRecords crossed the threshold &quot;</span> +  </span><br><span class="line">      numElementsForSpillThreshold);  </span><br><span class="line">    spill();  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="comment">// 1方法</span></span><br><span class="line">  growPointerArrayIfNecessary();  </span><br><span class="line">  <span class="keyword">final</span> int uaoSize = <span class="type">UnsafeAlignedOffset</span>.getUaoSize();  </span><br><span class="line">  <span class="comment">// Need 4 or 8 bytes to store the record length.  </span></span><br><span class="line">  <span class="keyword">final</span> int required = length + uaoSize;  </span><br><span class="line">  <span class="comment">// 2方法</span></span><br><span class="line">  acquireNewPageIfNecessary(required);  </span><br><span class="line">  <span class="comment">// 1方法和2方法都可能会调用TaskMemoryManager 的 allocatePage 方法</span></span><br><span class="line">  </span><br><span class="line">  assert(currentPage != <span class="literal">null</span>);  </span><br><span class="line">  <span class="keyword">final</span> <span class="type">Object</span> base = currentPage.getBaseObject();  </span><br><span class="line">  <span class="keyword">final</span> long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);  </span><br><span class="line">  <span class="type">UnsafeAlignedOffset</span>.putSize(base, pageCursor, length);  </span><br><span class="line">  pageCursor += uaoSize;  </span><br><span class="line">  <span class="type">Platform</span>.copyMemory(recordBase, recordOffset, base, pageCursor, length);  </span><br><span class="line">  pageCursor += length;  </span><br><span class="line">  inMemSorter.insertRecord(recordAddress, partitionId);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那接下来就让我们看 TaskMemoryManager 的 allocatePage 方法。</p>
<p>在此之前我们先提一下 TaskMemoryManager 是什么，其实顾名思义 TaskMemoryManager 就是对于的单个 Task 的 MemoryManager，负责管理单个 Task 的内存区域。</p>
<p>言归正传，allocatePage ()方法中就会调用 acquireExecutionMemory ()方法，我们需要重点来看一下这个里面的逻辑。</p>
<p>org.apache.spark.memory.TaskMemoryManager#acquireExecutionMemory</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Acquire N bytes of memory for a consumer. If there is no enough memory, it will call * spill() of consumers to release more memory. * * @return number of bytes successfully granted (&lt;= N).  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> </span><br><span class="line">public long acquireExecutionMemory(long required, <span class="type">MemoryConsumer</span> requestingConsumer) &#123;  </span><br><span class="line">  assert(required &gt;= <span class="number">0</span>);  </span><br><span class="line">  assert(requestingConsumer != <span class="literal">null</span>);  </span><br><span class="line">  <span class="type">MemoryMode</span> mode = requestingConsumer.getMode();  </span><br><span class="line">  <span class="comment">// If we are allocating Tungsten pages off-heap and receive a request to allocate on-heap  </span></span><br><span class="line">  <span class="comment">// memory here, then it may not make sense to spill since that would only end up freeing  </span></span><br><span class="line">  <span class="comment">// off-heap memory. This is subject to change, though, so it may be risky to make this  </span></span><br><span class="line">  <span class="comment">// optimization now in case we forget to undo it late when making changes.  </span></span><br><span class="line">  synchronized (<span class="keyword">this</span>) &#123;  </span><br><span class="line">    <span class="comment">// 首先会向我们之前提到的memoryManager实现类UnifiedMemoryManager申请内存</span></span><br><span class="line">    long got = memoryManager.acquireExecutionMemory(required, taskAttemptId, mode);  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Try to release memory from other consumers first, then we can reduce the frequency of  </span></span><br><span class="line">    <span class="comment">// spilling, avoid to have too many spilled files.    if (got &lt; required) &#123;  </span></span><br><span class="line">      logger.debug(<span class="string">&quot;Task &#123;&#125; need to spill &#123;&#125; for &#123;&#125;&quot;</span>, taskAttemptId,  </span><br><span class="line">        <span class="type">Utils</span>.bytesToString(required - got), requestingConsumer);  </span><br><span class="line">      <span class="comment">// We need to call spill() on consumers to free up more memory. We want to optimize for two  </span></span><br><span class="line">      <span class="comment">// things:      </span></span><br><span class="line">      <span class="comment">// * Minimize the number of spill calls, to reduce the number of spill files and avoid small      </span></span><br><span class="line">      <span class="comment">//   spill files.      </span></span><br><span class="line">      <span class="comment">// * Avoid spilling more data than necessary - if we only need a little more memory, we may not want to spill as much data as possible. Many consumers spill more than the requested amount, so we can take that into account in our decisions. We use a heuristic that selects the smallest memory consumer with at least `required` bytes of memory in an attempt to balance these factors. It may work well if there are fewer larger requests, but can result in many small spills if there are many smaller requests.  </span></span><br><span class="line">      <span class="comment">// Build a map of consumer in order of memory usage to prioritize spilling. Assign current consumer (if present) a nominal memory usage of 0 so that it is always last in priority order. The map will include all consumers that have previously acquired memory.      </span></span><br><span class="line">      <span class="comment">// 会将当前的MemoryConsumer按照使用的内存大小排一个序</span></span><br><span class="line">      <span class="type">TreeMap</span>&lt;<span class="type">Long</span>, <span class="type">List</span>&lt;<span class="type">MemoryConsumer</span>&gt;&gt; sortedConsumers = <span class="keyword">new</span> <span class="type">TreeMap</span>&lt;&gt;();  </span><br><span class="line">      <span class="keyword">for</span> (<span class="type">MemoryConsumer</span> c: consumers) &#123;  </span><br><span class="line">        <span class="keyword">if</span> (c.getUsed() &gt; <span class="number">0</span> &amp;&amp; c.getMode() == mode) &#123;  </span><br><span class="line">          long key = c == requestingConsumer ? <span class="number">0</span> : c.getUsed();  </span><br><span class="line">          <span class="type">List</span>&lt;<span class="type">MemoryConsumer</span>&gt; list =  </span><br><span class="line">              sortedConsumers.computeIfAbsent(key, k -&gt; <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;&gt;(<span class="number">1</span>));  </span><br><span class="line">          list.add(c);  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125;  </span><br><span class="line">      <span class="comment">// Iteratively spill consumers until we&#x27;ve freed enough memory or run out of consumers.  </span></span><br><span class="line">      <span class="comment">// 如果通过UnifiedMemoryManager申请的到的内存不够，那么就要考虑从其他的MemoryConsumer中抽取内存，也就是将一些使用频率不高的内存进行释放</span></span><br><span class="line">      <span class="keyword">while</span> (got &lt; required &amp;&amp; !sortedConsumers.isEmpty()) &#123;  </span><br><span class="line">        <span class="comment">// Get the consumer using the least memory more than the remaining required memory.  </span></span><br><span class="line">        <span class="comment">// 这里会优先找到一个使用的内存大于还需要获取的内存的Consumer，这样就可以尽可能少影响其他它的MemoryConsumer</span></span><br><span class="line">        <span class="type">Map</span>.<span class="type">Entry</span>&lt;<span class="type">Long</span>, <span class="type">List</span>&lt;<span class="type">MemoryConsumer</span>&gt;&gt; currentEntry =  </span><br><span class="line">          sortedConsumers.ceilingEntry(required - got);  </span><br><span class="line">        <span class="comment">// No consumer has enough memory on its own, start with spilling the biggest consumer.  </span></span><br><span class="line">        <span class="keyword">if</span> (currentEntry == <span class="literal">null</span>) &#123;  </span><br><span class="line">          currentEntry = sortedConsumers.lastEntry();  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="type">List</span>&lt;<span class="type">MemoryConsumer</span>&gt; cList = currentEntry.getValue();  </span><br><span class="line">        <span class="comment">// 尝试将MemoryConsumer中的内存进行溢写到磁盘，从而将内存进行释放</span></span><br><span class="line">        got += trySpillAndAcquire(requestingConsumer, required - got, cList, cList.size() - <span class="number">1</span>);  </span><br><span class="line">        <span class="keyword">if</span> (cList.isEmpty()) &#123;  </span><br><span class="line">          sortedConsumers.remove(currentEntry.getKey());  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    consumers.add(requestingConsumer);  </span><br><span class="line">    logger.debug(<span class="string">&quot;Task &#123;&#125; acquired &#123;&#125; for &#123;&#125;&quot;</span>, taskAttemptId, <span class="type">Utils</span>.bytesToString(got),  </span><br><span class="line">            requestingConsumer);  </span><br><span class="line">    <span class="keyword">return</span> got;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码的流程大致可以分为两步：</p>
<ol>
<li>首先会向我们之前提到的 memoryManager 实现类 UnifiedMemoryManager 申请内存</li>
<li>如果申请到的内存不足，就将其他的 MemoryConsumer 的不常用内存进行释放（其实就是溢写到磁盘中）</li>
</ol>
<p>我们先来看 acquireExecutionMemory () 方法中的第一步先向 UnifiedMemoryManager 申请内存调用的 memoryManager. acquireExecutionMemory 方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Try to acquire up to `numBytes` of execution memory for the current task and return the  </span></span><br><span class="line"><span class="comment"> * number of bytes obtained, or 0 if none can be allocated. * * This call may block until there is enough free memory in some situations, to make sure each * task has a chance to ramp up to at least 1 / 2N of the total memory pool (where N is the # of * active tasks) before it is forced to spill. This can happen if the number of tasks increase * but an older task had a lot of memory already. */</span></span><br><span class="line"><span class="keyword">override</span> <span class="keyword">private</span>[memory] <span class="function"><span class="keyword">def</span> <span class="title">acquireExecutionMemory</span></span>(  </span><br><span class="line">    numBytes: <span class="type">Long</span>,  </span><br><span class="line">    taskAttemptId: <span class="type">Long</span>,  </span><br><span class="line">    memoryMode: <span class="type">MemoryMode</span>): <span class="type">Long</span> = synchronized &#123;  </span><br><span class="line">  assertInvariants()  </span><br><span class="line">  assert(numBytes &gt;= <span class="number">0</span>)  </span><br><span class="line">  <span class="comment">// 通过是使用堆外还是堆内内存获得相关信息</span></span><br><span class="line">  <span class="keyword">val</span> (executionPool, storagePool, storageRegionSize, maxMemory) = memoryMode <span class="keyword">match</span> &#123;  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">MemoryMode</span>.<span class="type">ON_HEAP</span> =&gt; (  </span><br><span class="line">      onHeapExecutionMemoryPool,  </span><br><span class="line">      onHeapStorageMemoryPool,  </span><br><span class="line">      onHeapStorageRegionSize,  </span><br><span class="line">      maxHeapMemory)  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">MemoryMode</span>.<span class="type">OFF_HEAP</span> =&gt; (  </span><br><span class="line">      offHeapExecutionMemoryPool,  </span><br><span class="line">      offHeapStorageMemoryPool,  </span><br><span class="line">      offHeapStorageMemory,  </span><br><span class="line">      maxOffHeapMemory)  </span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**  </span></span><br><span class="line"><span class="comment">   * Grow the execution pool by evicting cached blocks, thereby shrinking the storage pool.   *   * When acquiring memory for a task, the execution pool may need to make multiple   * attempts. Each attempt must be able to evict storage in case another task jumps in   * and caches a large block between the attempts. This is called once per attempt.   */</span>  </span><br><span class="line"><span class="comment">// 用于申请执行内存，这个方法当ExecutionMemory不足时从StorageMemory中侵占内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeGrowExecutionPool</span></span>(extraMemoryNeeded: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">    <span class="keyword">if</span> (extraMemoryNeeded &gt; <span class="number">0</span>) &#123;  </span><br><span class="line">      <span class="comment">// There is not enough free memory in the execution pool, so try to reclaim memory from storage. We can reclaim any free memory from the storage pool. If the storage pool has grown to become larger than `storageRegionSize`, we can evict blocks and reclaim the memory that storage has borrowed from execution.      </span></span><br><span class="line">    <span class="keyword">val</span> memoryReclaimableFromStorage = math.max(  </span><br><span class="line">        storagePool.memoryFree,  </span><br><span class="line">        storagePool.poolSize - storageRegionSize)  </span><br><span class="line">    <span class="keyword">if</span> (memoryReclaimableFromStorage &gt; <span class="number">0</span>) &#123;  </span><br><span class="line">        <span class="comment">// Only reclaim as much space as is necessary and available:  </span></span><br><span class="line">        <span class="keyword">val</span> spaceToReclaim = storagePool.freeSpaceToShrinkPool(  </span><br><span class="line">          math.min(extraMemoryNeeded, memoryReclaimableFromStorage))  </span><br><span class="line">        storagePool.decrementPoolSize(spaceToReclaim)  </span><br><span class="line">        executionPool.incrementPoolSize(spaceToReclaim)  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**  </span></span><br><span class="line"><span class="comment">   * The size the execution pool would have after evicting storage memory.   *   * The execution memory pool divides this quantity among the active tasks evenly to cap   * the execution memory allocation for each task. It is important to keep this greater   * than the execution pool size, which doesn&#x27;t take into account potential memory that   * could be freed by evicting storage. Otherwise we may hit SPARK-12155.   *   * Additionally, this quantity should be kept below `maxMemory` to arbitrate fairness  </span></span><br><span class="line"><span class="comment">   * in execution memory allocation across tasks, Otherwise, a task may occupy more than   * its fair share of execution memory, mistakenly thinking that other tasks can acquire   * the portion of storage memory that cannot be evicted.   */</span></span><br><span class="line"><span class="comment">// 计算最大可以被获得的ExecutionMemory</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeMaxExecutionPoolSize</span></span>(): <span class="type">Long</span> = &#123;  </span><br><span class="line">    maxMemory - math.min(storagePool.memoryUsed, storageRegionSize)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  executionPool.acquireMemory(  </span><br><span class="line">    numBytes, taskAttemptId, maybeGrowExecutionPool, () =&gt; computeMaxExecutionPoolSize)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>memoryManager. acquireExecutionMemory 方法内定义了几个方法，然后调用 executionPool 的 acquireMemory 方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Try to acquire up to `numBytes` of memory for the given task and return the number of bytes  </span></span><br><span class="line"><span class="comment"> * obtained, or 0 if none can be allocated. * * This call may block until there is enough free memory in some situations, to make sure each * task has a chance to ramp up to at least 1 / 2N of the total memory pool (where N is the # of * active tasks) before it is forced to spill. This can happen if the number of tasks increase * but an older task had a lot of memory already. * * @param numBytes number of bytes to acquire  </span></span><br><span class="line"><span class="comment"> * @param taskAttemptId the task attempt acquiring memory  </span></span><br><span class="line"><span class="comment"> * @param maybeGrowPool a callback that potentially grows the size of this pool. It takes in one parameter (Long) that represents the desired amount of memory by which this pool should be expanded. </span></span><br><span class="line"><span class="comment"> * @param computeMaxPoolSize a callback that returns the maximum allowable size of this pool at this given moment. This is not a field because the max pool size is variable in certain cases. For instance, in unified memory management, the execution pool can be expanded by evicting cached blocks, thereby shrinking the storage pool. </span></span><br><span class="line"><span class="comment"> * @return the number of bytes granted to the task.  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[memory] <span class="function"><span class="keyword">def</span> <span class="title">acquireMemory</span></span>(  </span><br><span class="line">    numBytes: <span class="type">Long</span>,  </span><br><span class="line">    taskAttemptId: <span class="type">Long</span>,  </span><br><span class="line">    maybeGrowPool: <span class="type">Long</span> =&gt; <span class="type">Unit</span> = (additionalSpaceNeeded: <span class="type">Long</span>) =&gt; (),  </span><br><span class="line">    computeMaxPoolSize: () =&gt; <span class="type">Long</span> = () =&gt; poolSize): <span class="type">Long</span> = lock.synchronized &#123;  </span><br><span class="line">  assert(numBytes &gt; <span class="number">0</span>, <span class="string">s&quot;invalid number of bytes requested: <span class="subst">$numBytes</span>&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> clean up this clunky method signature  </span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Add this task to the taskMemory map just so we can keep an accurate count of the number  </span></span><br><span class="line">  <span class="comment">// of active tasks, to let other tasks ramp down their memory in calls to `acquireMemory`  if (!memoryForTask.contains(taskAttemptId)) &#123;  </span></span><br><span class="line">    memoryForTask(taskAttemptId) = <span class="number">0</span>L  </span><br><span class="line">    <span class="comment">// This will later cause waiting tasks to wake up and check numTasks again  </span></span><br><span class="line">    lock.notifyAll()  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Keep looping until we&#x27;re either sure that we don&#x27;t want to grant this request (because this  </span></span><br><span class="line">  <span class="comment">// task would have more than 1 / numActiveTasks of the memory) or we have enough free memory to give it (we always let each task get at least 1 / (2 * numActiveTasks)).  </span></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> simplify this to limit each task to its own slot  </span></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;  </span><br><span class="line">    <span class="keyword">val</span> numActiveTasks = memoryForTask.keys.size  </span><br><span class="line">    <span class="keyword">val</span> curMem = memoryForTask(taskAttemptId)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// In every iteration of this loop, we should first try to reclaim any borrowed execution  </span></span><br><span class="line">    <span class="comment">// space from storage. This is necessary because of the potential race condition where new storage blocks may steal the free execution memory that this task was waiting for. maybeGrowPool(numBytes - memoryFree)  </span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Maximum size the pool would have after potentially growing the pool.  </span></span><br><span class="line">    <span class="comment">// This is used to compute the upper bound of how much memory each task can occupy. This must take into account potential free memory as well as the amount this pool currently occupies. Otherwise, we may run into SPARK-12155 where, in unified memory management, we did not take into account space that could have been freed by evicting cached blocks.    </span></span><br><span class="line">    <span class="keyword">val</span> maxPoolSize = computeMaxPoolSize()  </span><br><span class="line">    <span class="comment">// 需要注意每个Task的运行内存容量是均分的  </span></span><br><span class="line">    <span class="comment">// 同时每个Task的最大和最小的内存容量的计算方法是不同的  </span></span><br><span class="line">    <span class="keyword">val</span> maxMemoryPerTask = maxPoolSize / numActiveTasks  </span><br><span class="line">    <span class="keyword">val</span> minMemoryPerTask = poolSize / (<span class="number">2</span> * numActiveTasks)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// How much we can grant this task; keep its share within 0 &lt;= X &lt;= 1 / numActiveTasks  </span></span><br><span class="line">    <span class="keyword">val</span> maxToGrant = math.min(numBytes, math.max(<span class="number">0</span>, maxMemoryPerTask - curMem))  </span><br><span class="line">    <span class="comment">// Only give it as much memory as is free, which might be none if it reached 1 / numTasks  </span></span><br><span class="line">    <span class="keyword">val</span> toGrant = math.min(maxToGrant, memoryFree)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// We want to let each task get at least 1 / (2 * numActiveTasks) before blocking;  </span></span><br><span class="line">    <span class="comment">// if we can&#x27;t give it this much now, wait for other tasks to free up memory    // (this happens if older tasks allocated lots of memory before N grew)    </span></span><br><span class="line">    <span class="comment">// 获取到的Task运行内存小于最小值也会进行等待  </span></span><br><span class="line">    <span class="keyword">if</span> (toGrant &lt; numBytes &amp;&amp; curMem + toGrant &lt; minMemoryPerTask) &#123;  </span><br><span class="line">      logInfo(<span class="string">s&quot;TID <span class="subst">$taskAttemptId</span> waiting for at least 1/2N of <span class="subst">$poolName</span> pool to be free&quot;</span>)  </span><br><span class="line">      lock.wait()  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      memoryForTask(taskAttemptId) += toGrant  </span><br><span class="line">      <span class="keyword">return</span> toGrant  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="number">0</span>L  <span class="comment">// Never reached  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>内部其实就是一个简单的内存分配操作，自行阅读。</p>
<p>接下来，我们看第二步，也就是当申请的内存不够时，将其他 MemoryConsumer 的内存进行释放。</p>
<p><strong>基本概念：</strong></p>
<ul>
<li>MemoryConsumer 是 Spark 内部抽象的一个类，表示使用内存的任务组件，比如 ExternalSorter、UnsafeRow 等。</li>
<li>TaskMemoryManager 管理一个任务中的所有 MemoryConsumer，在内存不足时通过溢出（spill）机制释放内存。</li>
<li>溢出的优先级：<strong>尽量先选择内存使用量大、且当前不活跃（非关键计算）的</strong> MemoryConsumer。</li>
</ul>
<p>在 Spark 的 TaskMemoryManager 中，<strong>优先溢出非活跃的</strong> MemoryConsumer 的逻辑主要是通过 MemoryConsumer 的排序和管理实现的。以下是这个逻辑的详细分析：</p>
<p><strong>数据结构sortedConsumers：</strong></p>
<p>TaskMemoryManager 使用一个按内存使用量排序的 TreeMap 来管理所有的 MemoryConsumer：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">TreeMap</span>&lt;<span class="type">Long</span>, <span class="type">List</span>&lt;<span class="type">MemoryConsumer</span>&gt;&gt; sortedConsumers = <span class="keyword">new</span> <span class="type">TreeMap</span>&lt;&gt;();</span><br></pre></td></tr></table></figure>
<p>键 (Long)：每个 MemoryConsumer 的当前内存使用量（used () 返回的值）。<br>值 (List<MemoryConsumer>)：使用该内存量的 MemoryConsumer 列表。</p>
<p>排序逻辑：</p>
<ul>
<li><p>使用 TreeMap 的排序特性，按内存使用量从小到大管理 MemoryConsumer。</p>
</li>
<li><p>优先溢出占用较少内存但仍满足需求的消费者，减少整体溢出成本。</p>
</li>
</ul>
<p><strong>溢出的选择逻辑：</strong></p>
<p>溢出逻辑发生在 TaskMemoryManager#acquireExecutionMemory 方法中。当某个 MemoryConsumer 请求内存时，系统会尝试优先溢出非活跃的 MemoryConsumer：</p>
<p>代码逻辑：</p>
<pre><code class="lang-scala">
while (got &lt; required &amp;&amp; !sortedConsumers.isEmpty ()) &#123;

  // 找到使用内存最少但满足需求的 MemoryConsumer_

  Map. Entry&lt;Long, List&lt;MemoryConsumer&gt;&gt; currentEntry = sortedConsumers.ceilingEntry (required - got);

  // 如果没有满足条件的消费者，选择占用内存最大的消费者_
  if (currentEntry == null) &#123;
    currentEntry = sortedConsumers.lastEntry ();
  &#125;
  // 从选中的消费者列表中，逐个溢出_
  List&lt;MemoryConsumer&gt; cList = currentEntry.getValue ();
  got += trySpillAndAcquire (requestingConsumer, required - got, cList, cList.size () - 1);
  // 如果当前消费者列表为空，从 TreeMap 中移除对应条目_
  if (cList.isEmpty ()) &#123;
    sortedConsumers.remove (currentEntry.getKey ());
 &#125;
&#125;
</code></pre>
<p><strong>具体实现步骤：</strong></p>
<p><strong>定位候选</strong> MemoryConsumer：</p>
<ul>
<li><p>使用 TreeMap #ceilingEntry 查找内存使用量满足需求的消费者。</p>
</li>
<li><p>如果找不到满足需求的消费者，则选择 TreeMap #lastEntry （占用最多内存的消费者）进行溢出。</p>
</li>
</ul>
<p><strong>调用溢出逻辑</strong>：</p>
<ul>
<li><p>trySpillAndAcquire 是核心方法，通过调用 MemoryConsumer #spill 将数据从内存溢出到磁盘。</p>
</li>
<li><p>同时释放的内存会增加到 got 中，用于满足当前请求。</p>
</li>
</ul>
<p><strong>更新</strong> sortedConsumers：</p>
<p>如果某个 MemoryConsumer 的内存使用量变为 0，或者它从 sortedConsumers 中被移除，则不再参与溢出逻辑。</p>
<p><strong>注</strong>：Storage 和 unroll 申请内存的逻辑大致相同。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.JPG" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/08/12/Spark-%E6%BA%90%E7%A0%81-Task%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B(Map%E7%AB%AF)/" title="Spark-源码-Task执行过程（Map）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Spark-源码-Task执行过程（Map）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-Cache%E4%B8%8Echeckpoint/" title="Spark-基础-Cache 与 Checkpoint"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-Cache 与 Checkpoint</div></div></a></div><div><a href="/2024/03/20/Spark-%E5%9F%BA%E7%A1%80-Shuffle%E6%9C%BA%E5%88%B6/" title="Spark-基础-Shuffle机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-20</div><div class="title">Spark-基础-Shuffle机制</div></div></a></div><div><a href="/2024/03/20/Spark-%E5%9F%BA%E7%A1%80-%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/" title="Spark-基础-核心编程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-20</div><div class="title">Spark-基础-核心编程</div></div></a></div><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-%E7%B4%AF%E5%8A%A0%E5%99%A8%E4%B8%8E%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/" title="Spark-基础-累加器和Broadcast"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-累加器和Broadcast</div></div></a></div><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F/" title="Spark-基础-运行模式"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-运行模式</div></div></a></div><div><a href="/2024/03/21/Spark-%E5%9F%BA%E7%A1%80-%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6/" title="Spark-基础-任务调度机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-21</div><div class="title">Spark-基础-任务调度机制</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CHLi</div><div class="author-info__description">Welcome to Mr.Li's blog</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/MLiLay"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome! This is MLiLay's Blog.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E4%BD%93%E7%B3%BB%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">内存体系概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98%E4%B8%8E%E5%A0%86%E5%86%85%E5%86%85%E5%AD%98%E5%8C%BA%E5%88%AB"><span class="toc-number">1.1.</span> <span class="toc-text">堆外内存与堆内内存区别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A0%86%E5%86%85%E5%86%85%E5%AD%98%EF%BC%88On-Heap-Memory%EF%BC%89"><span class="toc-number">1.1.1.</span> <span class="toc-text">堆内内存（On-Heap Memory）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E7%82%B9"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">特点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">优缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98%EF%BC%88Off-Heap-Memory%EF%BC%89"><span class="toc-number">1.1.2.</span> <span class="toc-text">堆外内存（Off-Heap Memory）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-1"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E7%82%B9-1"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">特点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9-1"><span class="toc-number">1.1.2.3.</span> <span class="toc-text">优缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#StaticMemoryManager"><span class="toc-number">1.1.3.</span> <span class="toc-text">StaticMemoryManager</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UnifiedMemoryManager"><span class="toc-number">1.1.4.</span> <span class="toc-text">UnifiedMemoryManager</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E9%A2%84%E7%95%99%E5%86%85%E5%AD%98-System-Reserved-Memory"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">系统预留内存 (System Reserved Memory)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E5%86%85%E5%AD%98-Storage-Memory"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">存储内存 (Storage Memory)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E5%86%85%E5%AD%98-Execution-Memory"><span class="toc-number">1.1.4.3.</span> <span class="toc-text">执行内存 (Execution Memory)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96-%E7%94%A8%E6%88%B7%E5%86%85%E5%AD%98-Other-User-Memory"><span class="toc-number">1.1.4.4.</span> <span class="toc-text">其他&#x2F;用户内存 (Other&#x2F;User Memory)</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%BA%90%E7%A0%81"><span class="toc-number">2.</span> <span class="toc-text">源码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.0.1.</span> <span class="toc-text">初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E8%8E%B7%E5%8F%96"><span class="toc-number">2.0.2.</span> <span class="toc-text">内存获取</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/28/Spark-%E6%BA%90%E7%A0%81-%E5%86%85%E5%AD%98%E4%BD%93%E7%B3%BB/" title="Spark-源码-内存体系">Spark-源码-内存体系</a><time datetime="2024-11-28T08:32:57.784Z" title="发表于 2024-11-28 16:32:57">2024-11-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/12/Spark-%E6%BA%90%E7%A0%81-Task%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B(Map%E7%AB%AF)/" title="Spark-源码-Task执行过程（Map）">Spark-源码-Task执行过程（Map）</a><time datetime="2024-08-12T08:47:58.560Z" title="发表于 2024-08-12 16:47:58">2024-08-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/29/Spark-%E6%BA%90%E7%A0%81-SparkContext/" title="Spark-源码-SparkContext">Spark-源码-SparkContext</a><time datetime="2024-07-29T06:27:54.293Z" title="发表于 2024-07-29 14:27:54">2024-07-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/22/Spark-%E6%BA%90%E7%A0%81-persist%E4%B8%8Echeckpoint/" title="Spark-源码-persist与checkpoint">Spark-源码-persist与checkpoint</a><time datetime="2024-07-22T09:02:52.238Z" title="发表于 2024-07-22 17:02:52">2024-07-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/11/Celeborn/" title="Celeborn">Celeborn</a><time datetime="2024-07-11T12:46:48.425Z" title="发表于 2024-07-11 20:46:48">2024-07-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By CHLi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>