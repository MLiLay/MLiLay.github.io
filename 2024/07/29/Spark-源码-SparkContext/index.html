<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Spark-源码-SparkContext | Mr.Li's blog</title><meta name="author" content="CHLi"><meta name="copyright" content="CHLi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="以下版本为 Spark 3.3.2 概述SparkContext 是什么？首先，Spark 程序的运行离不开 Driver，SparkDriver 的初始化始终围绕着 SparkContext 的初始化。SparkContext 可以算得上是 Spark 应用程序的发动机引擎，轿车要想跑起来，首先要启动发动机。SparkContext 初始化完毕，才能向 Spark 集群提交应用程序。而 Spar">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark-源码-SparkContext">
<meta property="og:url" content="http://example.com/2024/07/29/Spark-%E6%BA%90%E7%A0%81-SparkContext/index.html">
<meta property="og:site_name" content="Mr.Li&#39;s blog">
<meta property="og:description" content="以下版本为 Spark 3.3.2 概述SparkContext 是什么？首先，Spark 程序的运行离不开 Driver，SparkDriver 的初始化始终围绕着 SparkContext 的初始化。SparkContext 可以算得上是 Spark 应用程序的发动机引擎，轿车要想跑起来，首先要启动发动机。SparkContext 初始化完毕，才能向 Spark 集群提交应用程序。而 Spar">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/avatar.JPG">
<meta property="article:published_time" content="2024-07-29T06:27:54.293Z">
<meta property="article:modified_time" content="2024-07-29T06:37:55.348Z">
<meta property="article:author" content="CHLi">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/avatar.JPG"><link rel="shortcut icon" href="/img/spaceshuttle.png"><link rel="canonical" href="http://example.com/2024/07/29/Spark-%E6%BA%90%E7%A0%81-SparkContext/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark-源码-SparkContext',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-29 14:37:55'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">71</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/topGraph.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Mr.Li's blog"><span class="site-name">Mr.Li's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark-源码-SparkContext</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-29T06:27:54.293Z" title="发表于 2024-07-29 14:27:54">2024-07-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-29T06:37:55.348Z" title="更新于 2024-07-29 14:37:55">2024-07-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark-源码-SparkContext"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>以下版本为 Spark 3.3.2</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h3 id="SparkContext-是什么？"><a href="#SparkContext-是什么？" class="headerlink" title="SparkContext 是什么？"></a>SparkContext 是什么？</h3><p>首先，Spark 程序的运行离不开 Driver，SparkDriver 的初始化始终围绕着 SparkContext 的初始化。SparkContext 可以算得上是 Spark 应用程序的发动机引擎，轿车要想跑起来，首先要启动发动机。SparkContext 初始化完毕，才能向 Spark 集群提交应用程序。而 SparkContext 的配置参数则由 SparkConf 负责，SparkConf 就是你的操作面板。</p>
<h3 id="SparkConf-组成"><a href="#SparkConf-组成" class="headerlink" title="SparkConf 组成"></a>SparkConf 组成</h3><p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20240726151344.png" alt="image.png"></p>
<p><strong>SparkEnv</strong>: Spark 运行时环境。Executor 是处理任务的执行器，它依赖于 SparkEnv 提供的运行时环境。此外，在 Driver 中也包含了 SparkEnv，这是为了保证 local 模式下任务的执行。SparkEnv 内部包含了很多组件，例如，SerializerManager 、 RPCEnv 、BlockManager 、MapOutputTracker 等</p>
<p><strong>LiveListenerBus</strong>: SparkContext 中的事件总线，可以接收各个使用方的事件，并且通过异步方式对事件进行匹配后调用 SparkListener 的不同方法。</p>
<p><strong>SparkUI</strong>: Spark 的用户界面。SparkUI 间接依赖于计算引擎、调度系统、存储体系，作业 ( J o b )、阶段 ( Stage )、存储、执行器 ( Executor ) 等组件的监控数据都会以 SparkListenerEvent 的形式投递到 LiveListenerBus 中，SparkUI 将从各个 SparkListener 中读取数据并显示到 Web 界面。</p>
<p> <strong>SparkStatusTracker</strong>: 提供对作业、 Stage (阶段 ) 等的监控信息。 SparkStatusTracker 是一个低级的 API，这意味着只能提供非常脆弱的一致性机制。</p>
<p><strong>ConsoleProgressBar</strong> : 利用 SparkStatusTracker 的 API，在控制台展示 Stage 的进度。由于 SparkStatusTracker 存在的一致性问题，所以 ConsoleProgressBar 在控制台的显示往往有一定的时延。</p>
<p><strong>DAGScheduler</strong>: DAG 调度器，是调度系统中的重要组件之一，负责创建 Job，将 DAG 中的 RDD 划分到不同的 Stage 、提交 Stage 等。SparkUI 中有关 Job 和 Stage 的监控数据都来自 DAGScheduler。</p>
<p><strong>Taskscheduler</strong> : 任务调度器，是调度系统中的重要组件之一。TaskScheduler 按照调度算法对集群管理器已经分配给应用程序的资源进行二次调度后分配给任务。 TaskScheduler 调度的 Task 是由 DAGScheduler 创建的，所以 DAGScheduler 是 TaskScheduler 的前置调度。</p>
<p><strong>HeartbeatReceiver</strong> : 心跳接收器。所有 Executor 都会向 HeartbeatReceiver 发送心跳信息，HeartbeatReceiver 接收到 Execut or 的心跳信息后，首先更新 Execut or 的最后可见时间，然后将此信息交给 TaskScheduler 作进一步处理。</p>
<p> <strong>ContextCleaner</strong> : 上下文清理器。ContextCleaner 实际用异步方式清理那些超出应用作用域范围的 RDD、ShuffleDependency 和 Broadcast 等信息。</p>
<p><strong>JobProgressListener</strong> : 作业进度监听器。</p>
<p><strong>EventLoggingListener</strong> : 将事件持久化到存储的监听器，是 SparkContext 中的可选组件。当 spark. eventLog. enabled 属性 true 时启用。</p>
<p><strong>ExecutorAllocationManager</strong>: Executor 动态分配管理器。顾名思义，可以根据工作负载动态调整 Executor 的数量。在配置 Spark.dynamicalAllocation.enabled 属性为 true 的前提下，在非 local 模式下或者当 spark. dynamicAllocation. testing 属性为 true 时启用。</p>
<p><strong>ShutdownHookManager</strong> : 用于设置关闭钩子的管理器。可以给应用设置关闭钩子，这样就可以在 JVM 进程退出时，执行一些清理工作。</p>
<h1 id="SparkContext-的初始化"><a href="#SparkContext-的初始化" class="headerlink" title="SparkContext 的初始化"></a>SparkContext 的初始化</h1><h3 id="SparkConf-创建"><a href="#SparkConf-创建" class="headerlink" title="SparkConf 创建"></a>SparkConf 创建</h3><p>在 Spark 自己的构造方法中首先会根据传入的参数等创建一个SparkConf</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Create a SparkContext that loads settings from system properties (for instance, when * launching with ./bin/spark-submit). */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>() = <span class="keyword">this</span>(<span class="keyword">new</span> <span class="type">SparkConf</span>())  </span><br><span class="line">  </span><br><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).  </span></span><br><span class="line"><span class="comment"> * @param appName A name for your application, to display on the cluster web UI  </span></span><br><span class="line"><span class="comment"> * @param conf a [[org.apache.spark.SparkConf]] object specifying other Spark parameters  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(master: <span class="type">String</span>, appName: <span class="type">String</span>, conf: <span class="type">SparkConf</span>) =  </span><br><span class="line">  <span class="keyword">this</span>(<span class="type">SparkContext</span>.updatedConf(conf, master, appName))</span><br><span class="line"><span class="comment">// 还有很多不同的this方法，由不同传入的参数决定</span></span><br><span class="line">....</span><br></pre></td></tr></table></figure>
<h3 id="SparkEnv-创建"><a href="#SparkEnv-创建" class="headerlink" title="SparkEnv 创建"></a>SparkEnv 创建</h3><p>在 Spark 中凡是需要执行任务的地方就会有 SparkEnv，所以 Driver 和 Executor 都需要 SparkEnv。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create the Spark execution environment (cache, map output tracker, etc)  </span></span><br><span class="line">_env = createSparkEnv(_conf, isLocal, listenerBus)  </span><br><span class="line"><span class="type">SparkEnv</span>.set(_env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// This function allows components created by SparkEnv to be mocked in unit tests:  </span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">createSparkEnv</span></span>(  </span><br><span class="line">    conf: <span class="type">SparkConf</span>,  </span><br><span class="line">    isLocal: <span class="type">Boolean</span>,  </span><br><span class="line">    listenerBus: <span class="type">LiveListenerBus</span>): <span class="type">SparkEnv</span> = &#123;  </span><br><span class="line">  <span class="type">SparkEnv</span>.createDriverEnv(conf, isLocal, listenerBus, <span class="type">SparkContext</span>.numDriverCores(master, conf))  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到在 SparkContex 初始化的过程中其实是初始化出 Driver 的 SparkEnv，Executor 的 SparkEnv 的初始化在后续 Executor 的创建过程中。</p>
<p>在初始化的过程中，调用的方法是 SparkEnv.createDriverEnv</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Create a SparkEnv for the driver. */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">createDriverEnv</span></span>(  </span><br><span class="line">    conf: <span class="type">SparkConf</span>,  </span><br><span class="line">    isLocal: <span class="type">Boolean</span>,  </span><br><span class="line">    listenerBus: <span class="type">LiveListenerBus</span>,  </span><br><span class="line">    numCores: <span class="type">Int</span>,  </span><br><span class="line">    mockOutputCommitCoordinator: <span class="type">Option</span>[<span class="type">OutputCommitCoordinator</span>] = <span class="type">None</span>): <span class="type">SparkEnv</span> = &#123;  </span><br><span class="line">  assert(conf.contains(<span class="type">DRIVER_HOST_ADDRESS</span>),  </span><br><span class="line">    <span class="string">s&quot;<span class="subst">$&#123;DRIVER_HOST_ADDRESS.key&#125;</span> is not set on the driver!&quot;</span>)  </span><br><span class="line">  assert(conf.contains(<span class="type">DRIVER_PORT</span>), <span class="string">s&quot;<span class="subst">$&#123;DRIVER_PORT.key&#125;</span> is not set on the driver!&quot;</span>)  </span><br><span class="line">  <span class="keyword">val</span> bindAddress = conf.get(<span class="type">DRIVER_BIND_ADDRESS</span>)  </span><br><span class="line">  <span class="keyword">val</span> advertiseAddress = conf.get(<span class="type">DRIVER_HOST_ADDRESS</span>)  </span><br><span class="line">  <span class="keyword">val</span> port = conf.get(<span class="type">DRIVER_PORT</span>)  </span><br><span class="line">  <span class="keyword">val</span> ioEncryptionKey = <span class="keyword">if</span> (conf.get(<span class="type">IO_ENCRYPTION_ENABLED</span>)) &#123;  </span><br><span class="line">    <span class="type">Some</span>(<span class="type">CryptoStreamUtils</span>.createKey(conf))  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="type">None</span>  </span><br><span class="line">  &#125;  </span><br><span class="line">  create(  </span><br><span class="line">    conf,  </span><br><span class="line">    <span class="type">SparkContext</span>.<span class="type">DRIVER_IDENTIFIER</span>,  </span><br><span class="line">    bindAddress,  </span><br><span class="line">    advertiseAddress,  </span><br><span class="line">    <span class="type">Option</span>(port),  </span><br><span class="line">    isLocal,  </span><br><span class="line">    numCores,  </span><br><span class="line">    ioEncryptionKey,  </span><br><span class="line">    listenerBus = listenerBus,  </span><br><span class="line">    mockOutputCommitCoordinator = mockOutputCommitCoordinator  </span><br><span class="line">  )  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到首先是对 Driver 的地址、端口号等信息进行绑定。，之后在创建环境。</p>
<p>在 create 内部其实就是包含了 SerializerManager 、 RPCEnv 、BlockManager 、MapOutputTracker 等一系列组件的创建。</p>
<p>而且这个 create 方法 Driver 和 Executor 是共用的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Helper method to create a SparkEnv for a driver or an executor. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(  </span><br><span class="line">    conf: <span class="type">SparkConf</span>,  </span><br><span class="line">    executorId: <span class="type">String</span>,  </span><br><span class="line">    bindAddress: <span class="type">String</span>,  </span><br><span class="line">    advertiseAddress: <span class="type">String</span>,  </span><br><span class="line">    port: <span class="type">Option</span>[<span class="type">Int</span>],  </span><br><span class="line">    isLocal: <span class="type">Boolean</span>,  </span><br><span class="line">    numUsableCores: <span class="type">Int</span>,  </span><br><span class="line">    ioEncryptionKey: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Byte</span>]],  </span><br><span class="line">    listenerBus: <span class="type">LiveListenerBus</span> = <span class="literal">null</span>,  </span><br><span class="line">    mockOutputCommitCoordinator: <span class="type">Option</span>[<span class="type">OutputCommitCoordinator</span>] = <span class="type">None</span>): <span class="type">SparkEnv</span> = &#123;  </span><br><span class="line">  <span class="comment">// 判断是否是Driver</span></span><br><span class="line">  <span class="keyword">val</span> isDriver = executorId == <span class="type">SparkContext</span>.<span class="type">DRIVER_IDENTIFIER</span>  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Listener bus is only used on the driver  </span></span><br><span class="line">  <span class="keyword">if</span> (isDriver) &#123;  </span><br><span class="line">    assert(listenerBus != <span class="literal">null</span>, <span class="string">&quot;Attempted to create driver SparkEnv with null listener bus!&quot;</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">val</span> authSecretFileConf = <span class="keyword">if</span> (isDriver) <span class="type">AUTH_SECRET_FILE_DRIVER</span> <span class="keyword">else</span> <span class="type">AUTH_SECRET_FILE_EXECUTOR</span>  </span><br><span class="line">  <span class="keyword">val</span> securityManager = <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf, ioEncryptionKey, authSecretFileConf)  </span><br><span class="line">  <span class="keyword">if</span> (isDriver) &#123;  </span><br><span class="line">    securityManager.initializeAuth()  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  ioEncryptionKey.foreach &#123; _ =&gt;  </span><br><span class="line">    <span class="keyword">if</span> (!securityManager.isEncryptionEnabled()) &#123;  </span><br><span class="line">      logWarning(<span class="string">&quot;I/O encryption enabled without RPC encryption: keys will be visible on the &quot;</span> +  </span><br><span class="line">        <span class="string">&quot;wire.&quot;</span>)  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> systemName = <span class="keyword">if</span> (isDriver) driverSystemName <span class="keyword">else</span> executorSystemName  </span><br><span class="line">  <span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(systemName, bindAddress, advertiseAddress, port.getOrElse(<span class="number">-1</span>), conf,  </span><br><span class="line">    securityManager, numUsableCores, !isDriver)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Figure out which port RpcEnv actually bound to in case the original port is 0 or occupied.  </span></span><br><span class="line">  <span class="keyword">if</span> (isDriver) &#123;  </span><br><span class="line">    conf.set(<span class="type">DRIVER_PORT</span>, rpcEnv.address.port)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> serializer = <span class="type">Utils</span>.instantiateSerializerFromConf[<span class="type">Serializer</span>](<span class="type">SERIALIZER</span>, conf, isDriver)  </span><br><span class="line">  logDebug(<span class="string">s&quot;Using serializer: <span class="subst">$&#123;serializer.getClass&#125;</span>&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> serializerManager = <span class="keyword">new</span> <span class="type">SerializerManager</span>(serializer, conf, ioEncryptionKey)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> closureSerializer = <span class="keyword">new</span> <span class="type">JavaSerializer</span>(conf)  </span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">registerOrLookupEndpoint</span></span>(  </span><br><span class="line">      name: <span class="type">String</span>, endpointCreator: =&gt; <span class="type">RpcEndpoint</span>):  </span><br><span class="line">    <span class="type">RpcEndpointRef</span> = &#123;  </span><br><span class="line">    <span class="keyword">if</span> (isDriver) &#123;  </span><br><span class="line">      logInfo(<span class="string">&quot;Registering &quot;</span> + name)  </span><br><span class="line">      rpcEnv.setupEndpoint(name, endpointCreator)  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      <span class="type">RpcUtils</span>.makeDriverRef(name, conf, rpcEnv)  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> broadcastManager = <span class="keyword">new</span> <span class="type">BroadcastManager</span>(isDriver, conf)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> mapOutputTracker = <span class="keyword">if</span> (isDriver) &#123;  </span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapOutputTrackerMaster</span>(conf, broadcastManager, isLocal)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapOutputTrackerWorker</span>(conf)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Have to assign trackerEndpoint after initialization as MapOutputTrackerEndpoint  </span></span><br><span class="line">  <span class="comment">// requires the MapOutputTracker itself  mapOutputTracker.trackerEndpoint = registerOrLookupEndpoint(MapOutputTracker.ENDPOINT_NAME,  </span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapOutputTrackerMasterEndpoint</span>(  </span><br><span class="line">      rpcEnv, mapOutputTracker.asInstanceOf[<span class="type">MapOutputTrackerMaster</span>], conf))  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Let the user specify short names for shuffle managers  </span></span><br><span class="line">  <span class="keyword">val</span> shortShuffleMgrNames = <span class="type">Map</span>(  </span><br><span class="line">    <span class="string">&quot;sort&quot;</span> -&gt; classOf[org.apache.spark.shuffle.sort.<span class="type">SortShuffleManager</span>].getName,  </span><br><span class="line">    <span class="string">&quot;tungsten-sort&quot;</span> -&gt; classOf[org.apache.spark.shuffle.sort.<span class="type">SortShuffleManager</span>].getName)  </span><br><span class="line">  <span class="keyword">val</span> shuffleMgrName = conf.get(config.<span class="type">SHUFFLE_MANAGER</span>)  </span><br><span class="line">  <span class="keyword">val</span> shuffleMgrClass =  </span><br><span class="line">    shortShuffleMgrNames.getOrElse(shuffleMgrName.toLowerCase(<span class="type">Locale</span>.<span class="type">ROOT</span>), shuffleMgrName)  </span><br><span class="line">  <span class="keyword">val</span> shuffleManager = <span class="type">Utils</span>.instantiateSerializerOrShuffleManager[<span class="type">ShuffleManager</span>](  </span><br><span class="line">    shuffleMgrClass, conf, isDriver)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> memoryManager: <span class="type">MemoryManager</span> = <span class="type">UnifiedMemoryManager</span>(conf, numUsableCores)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> blockManagerPort = <span class="keyword">if</span> (isDriver) &#123;  </span><br><span class="line">    conf.get(<span class="type">DRIVER_BLOCK_MANAGER_PORT</span>)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    conf.get(<span class="type">BLOCK_MANAGER_PORT</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> externalShuffleClient = <span class="keyword">if</span> (conf.get(config.<span class="type">SHUFFLE_SERVICE_ENABLED</span>)) &#123;  </span><br><span class="line">    <span class="keyword">val</span> transConf = <span class="type">SparkTransportConf</span>.fromSparkConf(conf, <span class="string">&quot;shuffle&quot;</span>, numUsableCores)  </span><br><span class="line">    <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ExternalBlockStoreClient</span>(transConf, securityManager,  </span><br><span class="line">      securityManager.isAuthenticationEnabled(), conf.get(config.<span class="type">SHUFFLE_REGISTRATION_TIMEOUT</span>)))  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="type">None</span>  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Mapping from block manager id to the block manager&#x27;s information.  </span></span><br><span class="line">  <span class="keyword">val</span> blockManagerInfo = <span class="keyword">new</span> concurrent.<span class="type">TrieMap</span>[<span class="type">BlockManagerId</span>, <span class="type">BlockManagerInfo</span>]()  </span><br><span class="line">  <span class="keyword">val</span> blockManagerMaster = <span class="keyword">new</span> <span class="type">BlockManagerMaster</span>(  </span><br><span class="line">    registerOrLookupEndpoint(  </span><br><span class="line">      <span class="type">BlockManagerMaster</span>.<span class="type">DRIVER_ENDPOINT_NAME</span>,  </span><br><span class="line">      <span class="keyword">new</span> <span class="type">BlockManagerMasterEndpoint</span>(  </span><br><span class="line">        rpcEnv,  </span><br><span class="line">        isLocal,  </span><br><span class="line">        conf,  </span><br><span class="line">        listenerBus,  </span><br><span class="line">        <span class="keyword">if</span> (conf.get(config.<span class="type">SHUFFLE_SERVICE_ENABLED</span>)) &#123;  </span><br><span class="line">          externalShuffleClient  </span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">          <span class="type">None</span>  </span><br><span class="line">        &#125;, blockManagerInfo,  </span><br><span class="line">        mapOutputTracker.asInstanceOf[<span class="type">MapOutputTrackerMaster</span>],  </span><br><span class="line">        shuffleManager,  </span><br><span class="line">        isDriver)),  </span><br><span class="line">    registerOrLookupEndpoint(  </span><br><span class="line">      <span class="type">BlockManagerMaster</span>.<span class="type">DRIVER_HEARTBEAT_ENDPOINT_NAME</span>,  </span><br><span class="line">      <span class="keyword">new</span> <span class="type">BlockManagerMasterHeartbeatEndpoint</span>(rpcEnv, isLocal, blockManagerInfo)),  </span><br><span class="line">    conf,  </span><br><span class="line">    isDriver)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> blockTransferService =  </span><br><span class="line">    <span class="keyword">new</span> <span class="type">NettyBlockTransferService</span>(conf, securityManager, bindAddress, advertiseAddress,  </span><br><span class="line">      blockManagerPort, numUsableCores, blockManagerMaster.driverEndpoint)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// NB: blockManager is not valid until initialize() is called later.  </span></span><br><span class="line">  <span class="keyword">val</span> blockManager = <span class="keyword">new</span> <span class="type">BlockManager</span>(  </span><br><span class="line">    executorId,  </span><br><span class="line">    rpcEnv,  </span><br><span class="line">    blockManagerMaster,  </span><br><span class="line">    serializerManager,  </span><br><span class="line">    conf,  </span><br><span class="line">    memoryManager,  </span><br><span class="line">    mapOutputTracker,  </span><br><span class="line">    shuffleManager,  </span><br><span class="line">    blockTransferService,  </span><br><span class="line">    securityManager,  </span><br><span class="line">    externalShuffleClient)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> metricsSystem = <span class="keyword">if</span> (isDriver) &#123;  </span><br><span class="line">    <span class="comment">// Don&#x27;t start metrics system right now for Driver.  </span></span><br><span class="line">    <span class="comment">// We need to wait for the task scheduler to give us an app ID.    // Then we can start the metrics system.    MetricsSystem.createMetricsSystem(MetricsSystemInstances.DRIVER, conf)  </span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="comment">// We need to set the executor ID before the MetricsSystem is created because sources and  </span></span><br><span class="line">    <span class="comment">// sinks specified in the metrics configuration file will want to incorporate this executor&#x27;s    // ID into the metrics they report.    conf.set(EXECUTOR_ID, executorId)  </span></span><br><span class="line">    <span class="keyword">val</span> ms = <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="type">MetricsSystemInstances</span>.<span class="type">EXECUTOR</span>, conf)  </span><br><span class="line">    ms.start(conf.get(<span class="type">METRICS_STATIC_SOURCES_ENABLED</span>))  </span><br><span class="line">    ms  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> outputCommitCoordinator = mockOutputCommitCoordinator.getOrElse &#123;  </span><br><span class="line">    <span class="keyword">new</span> <span class="type">OutputCommitCoordinator</span>(conf, isDriver)  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">val</span> outputCommitCoordinatorRef = registerOrLookupEndpoint(<span class="string">&quot;OutputCommitCoordinator&quot;</span>,  </span><br><span class="line">    <span class="keyword">new</span> <span class="type">OutputCommitCoordinatorEndpoint</span>(rpcEnv, outputCommitCoordinator))  </span><br><span class="line">  outputCommitCoordinator.coordinatorRef = <span class="type">Some</span>(outputCommitCoordinatorRef)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> envInstance = <span class="keyword">new</span> <span class="type">SparkEnv</span>(  </span><br><span class="line">    executorId,  </span><br><span class="line">    rpcEnv,  </span><br><span class="line">    serializer,  </span><br><span class="line">    closureSerializer,  </span><br><span class="line">    serializerManager,  </span><br><span class="line">    mapOutputTracker,  </span><br><span class="line">    shuffleManager,  </span><br><span class="line">    broadcastManager,  </span><br><span class="line">    blockManager,  </span><br><span class="line">    securityManager,  </span><br><span class="line">    metricsSystem,  </span><br><span class="line">    memoryManager,  </span><br><span class="line">    outputCommitCoordinator,  </span><br><span class="line">    conf)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Add a reference to tmp dir created by driver, we will delete this tmp dir when stop() is  </span></span><br><span class="line">  <span class="comment">// called, and we only need to do it for driver. Because driver may run as a service, and if we  // don&#x27;t delete this tmp dir when sc is stopped, then will create too many tmp dirs.  if (isDriver) &#123;  </span></span><br><span class="line">    <span class="keyword">val</span> sparkFilesDir = <span class="type">Utils</span>.createTempDir(<span class="type">Utils</span>.getLocalDir(conf), <span class="string">&quot;userFiles&quot;</span>).getAbsolutePath  </span><br><span class="line">    envInstance.driverTmpDir = <span class="type">Some</span>(sparkFilesDir)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  envInstance  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="创建heartbeatReceiver"><a href="#创建heartbeatReceiver" class="headerlink" title="创建heartbeatReceiver"></a>创建heartbeatReceiver</h2><p>心跳接收器主要用来接受 Executor 发来的心跳，监控 Executor 的存活情况。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// We need to register &quot;HeartbeatReceiver&quot; before &quot;createTaskScheduler&quot; because Executor will  </span></span><br><span class="line"><span class="comment">// retrieve &quot;HeartbeatReceiver&quot; in the constructor. (SPARK-6640)  </span></span><br><span class="line">_heartbeatReceiver = env.rpcEnv.setupEndpoint(  </span><br><span class="line">  <span class="type">HeartbeatReceiver</span>.<span class="type">ENDPOINT_NAME</span>, <span class="keyword">new</span> <span class="type">HeartbeatReceiver</span>(<span class="keyword">this</span>))</span><br></pre></td></tr></table></figure>
<h2 id="创建-DAGScheduler-和TaskScheduler"><a href="#创建-DAGScheduler-和TaskScheduler" class="headerlink" title="创建 DAGScheduler 和TaskScheduler"></a>创建 DAGScheduler 和TaskScheduler</h2><p>DAGScheduler 主要负责在 RDD 划分 stage、Task 的工作，TaskScheduler 主要负责 Task 任务的调度。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create and start the scheduler  </span></span><br><span class="line"><span class="keyword">val</span> (sched, ts) = <span class="type">SparkContext</span>.createTaskScheduler(<span class="keyword">this</span>, master)  </span><br><span class="line">_schedulerBackend = sched  </span><br><span class="line">_taskScheduler = ts  </span><br><span class="line">_dagScheduler = <span class="keyword">new</span> <span class="type">DAGScheduler</span>(<span class="keyword">this</span>)  </span><br><span class="line">_heartbeatReceiver.ask[<span class="type">Boolean</span>](<span class="type">TaskSchedulerIsSet</span>)</span><br></pre></td></tr></table></figure>
<p>在内部会根据使用模式的的不同部署模式（depolyMode）创建不同的 TaskSchedulerImp</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Create a task scheduler based on a given master URL. * Return a 2-tuple of the scheduler backend and the task scheduler. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createTaskScheduler</span></span>(  </span><br><span class="line">    sc: <span class="type">SparkContext</span>,  </span><br><span class="line">    master: <span class="type">String</span>): (<span class="type">SchedulerBackend</span>, <span class="type">TaskScheduler</span>) = &#123;  </span><br><span class="line">  <span class="keyword">import</span> <span class="type">SparkMasterRegex</span>._  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// When running locally, don&#x27;t try to re-execute tasks on failure.  </span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">MAX_LOCAL_TASK_FAILURES</span> = <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Ensure that default executor&#x27;s resources satisfies one or more tasks requirement.  </span></span><br><span class="line">  <span class="comment">// This function is for cluster managers that don&#x27;t set the executor cores config, for  // others its checked in ResourceProfile.  def checkResourcesPerTask(executorCores: Int): Unit = &#123;  </span></span><br><span class="line">    <span class="keyword">val</span> taskCores = sc.conf.get(<span class="type">CPUS_PER_TASK</span>)  </span><br><span class="line">    <span class="keyword">if</span> (!sc.conf.get(<span class="type">SKIP_VALIDATE_CORES_TESTING</span>)) &#123;  </span><br><span class="line">      validateTaskCpusLargeEnough(sc.conf, executorCores, taskCores)  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">val</span> defaultProf = sc.resourceProfileManager.defaultResourceProfile  </span><br><span class="line">    <span class="type">ResourceUtils</span>.warnOnWastedResources(defaultProf, sc.conf, <span class="type">Some</span>(executorCores))  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  master <span class="keyword">match</span> &#123;  </span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;local&quot;</span> =&gt;  </span><br><span class="line">      checkResourcesPerTask(<span class="number">1</span>)  </span><br><span class="line">      <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, <span class="type">MAX_LOCAL_TASK_FAILURES</span>, isLocal = <span class="literal">true</span>)  </span><br><span class="line">      <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalSchedulerBackend</span>(sc.getConf, scheduler, <span class="number">1</span>)  </span><br><span class="line">      scheduler.initialize(backend)  </span><br><span class="line">      (backend, scheduler)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">LOCAL_N_REGEX</span>(threads) =&gt;  </span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">localCpuCount</span></span>: <span class="type">Int</span> = <span class="type">Runtime</span>.getRuntime.availableProcessors()  </span><br><span class="line">      <span class="comment">// local[*] estimates the number of cores on the machine; local[N] uses exactly N threads.  </span></span><br><span class="line">      <span class="keyword">val</span> threadCount = <span class="keyword">if</span> (threads == <span class="string">&quot;*&quot;</span>) localCpuCount <span class="keyword">else</span> threads.toInt  </span><br><span class="line">      <span class="keyword">if</span> (threadCount &lt;= <span class="number">0</span>) &#123;  </span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Asked to run locally with <span class="subst">$threadCount</span> threads&quot;</span>)  </span><br><span class="line">      &#125;  </span><br><span class="line">      checkResourcesPerTask(threadCount)  </span><br><span class="line">      <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, <span class="type">MAX_LOCAL_TASK_FAILURES</span>, isLocal = <span class="literal">true</span>)  </span><br><span class="line">      <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalSchedulerBackend</span>(sc.getConf, scheduler, threadCount)  </span><br><span class="line">      scheduler.initialize(backend)  </span><br><span class="line">      (backend, scheduler)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">LOCAL_N_FAILURES_REGEX</span>(threads, maxFailures) =&gt;  </span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">localCpuCount</span></span>: <span class="type">Int</span> = <span class="type">Runtime</span>.getRuntime.availableProcessors()  </span><br><span class="line">      <span class="comment">// local[*, M] means the number of cores on the computer with M failures  </span></span><br><span class="line">      <span class="comment">// local[N, M] means exactly N threads with M failures      val threadCount = if (threads == &quot;*&quot;) localCpuCount else threads.toInt  </span></span><br><span class="line">      checkResourcesPerTask(threadCount)  </span><br><span class="line">      <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, maxFailures.toInt, isLocal = <span class="literal">true</span>)  </span><br><span class="line">      <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalSchedulerBackend</span>(sc.getConf, scheduler, threadCount)  </span><br><span class="line">      scheduler.initialize(backend)  </span><br><span class="line">      (backend, scheduler)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">SPARK_REGEX</span>(sparkUrl) =&gt;  </span><br><span class="line">      <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc)  </span><br><span class="line">      <span class="keyword">val</span> masterUrls = sparkUrl.split(<span class="string">&quot;,&quot;</span>).map(<span class="string">&quot;spark://&quot;</span> + _)  </span><br><span class="line">      <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">StandaloneSchedulerBackend</span>(scheduler, sc, masterUrls)  </span><br><span class="line">      scheduler.initialize(backend)  </span><br><span class="line">      (backend, scheduler)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> <span class="type">LOCAL_CLUSTER_REGEX</span>(numWorkers, coresPerWorker, memoryPerWorker) =&gt;  </span><br><span class="line">      checkResourcesPerTask(coresPerWorker.toInt)  </span><br><span class="line">      <span class="comment">// Check to make sure memory requested &lt;= memoryPerWorker. Otherwise Spark will just hang.  </span></span><br><span class="line">      <span class="keyword">val</span> memoryPerWorkerInt = memoryPerWorker.toInt  </span><br><span class="line">      <span class="keyword">if</span> (sc.executorMemory &gt; memoryPerWorkerInt) &#123;  </span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(  </span><br><span class="line">          <span class="string">&quot;Asked to launch cluster with %d MiB/worker but requested %d MiB/executor&quot;</span>.format(  </span><br><span class="line">            memoryPerWorkerInt, sc.executorMemory))  </span><br><span class="line">      &#125;  </span><br><span class="line">  </span><br><span class="line">      <span class="comment">// For host local mode setting the default of SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED  </span></span><br><span class="line">      <span class="comment">// to false because this mode is intended to be used for testing and in this case all the      // executors are running on the same host. So if host local reading was enabled here then      // testing of the remote fetching would be secondary as setting this config explicitly to      // false would be required in most of the unit test (despite the fact that remote fetching      // is much more frequent in production).      sc.conf.setIfMissing(SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED, false)  </span></span><br><span class="line">  </span><br><span class="line">      <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc)  </span><br><span class="line">      <span class="keyword">val</span> localCluster = <span class="type">LocalSparkCluster</span>(  </span><br><span class="line">        numWorkers.toInt, coresPerWorker.toInt, memoryPerWorkerInt, sc.conf)  </span><br><span class="line">      <span class="keyword">val</span> masterUrls = localCluster.start()  </span><br><span class="line">      <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">StandaloneSchedulerBackend</span>(scheduler, sc, masterUrls)  </span><br><span class="line">      scheduler.initialize(backend)  </span><br><span class="line">      backend.shutdownCallback = (backend: <span class="type">StandaloneSchedulerBackend</span>) =&gt; &#123;  </span><br><span class="line">        localCluster.stop()  </span><br><span class="line">      &#125;  </span><br><span class="line">      (backend, scheduler)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">case</span> masterUrl =&gt;  </span><br><span class="line">      <span class="keyword">val</span> cm = getClusterManager(masterUrl) <span class="keyword">match</span> &#123;  </span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(clusterMgr) =&gt; clusterMgr  </span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">&quot;Could not parse Master URL: &#x27;&quot;</span> + master + <span class="string">&quot;&#x27;&quot;</span>)  </span><br><span class="line">      &#125;  </span><br><span class="line">      <span class="keyword">try</span> &#123;  </span><br><span class="line">        <span class="keyword">val</span> scheduler = cm.createTaskScheduler(sc, masterUrl)  </span><br><span class="line">        <span class="keyword">val</span> backend = cm.createSchedulerBackend(sc, masterUrl, scheduler)  </span><br><span class="line">        cm.initialize(scheduler, backend)  </span><br><span class="line">        (backend, scheduler)  </span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">        <span class="keyword">case</span> se: <span class="type">SparkException</span> =&gt; <span class="keyword">throw</span> se  </span><br><span class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;  </span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">&quot;External scheduler cannot be instantiated&quot;</span>, e)  </span><br><span class="line">      &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="初始化BlockManager"><a href="#初始化BlockManager" class="headerlink" title="初始化BlockManager"></a>初始化BlockManager</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">_env.blockManager.initialize(_applicationId)  </span><br><span class="line"><span class="type">FallbackStorage</span>.registerBlockManagerIfNeeded(_env.blockManager.master, _conf)</span><br></pre></td></tr></table></figure>
<h2 id="启动度量系统"><a href="#启动度量系统" class="headerlink" title="启动度量系统"></a>启动度量系统</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The metrics system for Driver need to be set spark.app.id to app ID.  </span></span><br><span class="line"><span class="comment">// So it should start after we get app ID from the task scheduler and set spark.app.id.  </span></span><br><span class="line">_env.metricsSystem.start(_conf.get(<span class="type">METRICS_STATIC_SOURCES_ENABLED</span>))</span><br></pre></td></tr></table></figure>
<h2 id="创建日志监听器"><a href="#创建日志监听器" class="headerlink" title="创建日志监听器"></a>创建日志监听器</h2><p>EventLoggingListener 是将事件持久化到存储的监听器，是 SparkCont ext 中的可选组件。当 spark. eventLog. enabled 属性为 true 时启用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">_eventLogger =  </span><br><span class="line">  <span class="keyword">if</span> (isEventLogEnabled) &#123;  </span><br><span class="line">    <span class="keyword">val</span> logger =  </span><br><span class="line">      <span class="keyword">new</span> <span class="type">EventLoggingListener</span>(_applicationId, _applicationAttemptId, _eventLogDir.get,  </span><br><span class="line">        _conf, _hadoopConfiguration)  </span><br><span class="line">    logger.start()  </span><br><span class="line">    listenerBus.addToEventLogQueue(logger)  </span><br><span class="line">    <span class="type">Some</span>(logger)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="type">None</span>  </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>EventLoggingListener 也将参与到对事件总线中事件的监听中，并把感兴趣的事件记录到日志。</p>
<p>以 EventLoggingListener 感兴趣的 SparkListenerBlockManagerRemoved 事件为例， EventlLoggingListener 重写的 onBlockManagerRemoved 方法将对 SparkListenerBlockManagerRemoved 事件进行处理，代码如下。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockManagerRemoved</span></span>(event: <span class="type">SparkListenerBlockManagerRemoved</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  logEvent(event, flushLogger = <span class="literal">true</span>)  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Log the event as JSON. */</span>  </span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">logEvent</span></span>(event: <span class="type">SparkListenerEvent</span>, flushLogger: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="keyword">val</span> eventJson = <span class="type">JsonProtocol</span>.sparkEventToJson(event)  </span><br><span class="line">  logWriter.writeEvent(compact(render(eventJson)), flushLogger)  </span><br><span class="line">  <span class="keyword">if</span> (testing) &#123;  </span><br><span class="line">    loggedEvents += eventJson  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="创建和启动-ExecutorAllocationManager"><a href="#创建和启动-ExecutorAllocationManager" class="headerlink" title="创建和启动 ExecutorAllocationManager"></a>创建和启动 ExecutorAllocationManager</h2><p>ExecutorAllocationManager 是基于工作负载动态分配和删除 Executor 的代理。简单来讲，ExecutorAllocationManager 与集群管理器之间的关系如下图所示。</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20240729104132.png" alt="image.png"></p>
<p>ExecutorAllocationManager 内部会定时根据工作负载计算所需的 Executor 数量，如果对 Executor 需求数量大于之前向集群管理器申请的 Executor 数量，那么向集群管理器申请添加 Executor ;如果对 Executor 需求数量小于之前向集群管理器申请的 Executor 数量，那么向集群管理器申请取消部分 Executor。此外，ExecutorAllocationManager 内部还会定时向集群管理器申请移除 (“ kill ” ) 过期的 Executor 。</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20240729104339.png" alt="image.png"></p>
<h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dynamicAllocationEnabled = <span class="type">Utils</span>.isDynamicAllocationEnabled(_conf)  </span><br><span class="line">_executorAllocationManager =  </span><br><span class="line">  <span class="keyword">if</span> (dynamicAllocationEnabled) &#123;  </span><br><span class="line">    schedulerBackend <span class="keyword">match</span> &#123;  </span><br><span class="line">      <span class="keyword">case</span> b: <span class="type">ExecutorAllocationClient</span> =&gt;  </span><br><span class="line">        <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ExecutorAllocationManager</span>(  </span><br><span class="line">          schedulerBackend.asInstanceOf[<span class="type">ExecutorAllocationClient</span>], listenerBus, _conf,  </span><br><span class="line">          cleaner = cleaner, resourceProfileManager = resourceProfileManager))  </span><br><span class="line">      <span class="keyword">case</span> _ =&gt;  </span><br><span class="line">        <span class="type">None</span>  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="type">None</span>  </span><br><span class="line">  &#125;  </span><br><span class="line">_executorAllocationManager.foreach(_.start())</span><br></pre></td></tr></table></figure>
<p>首先是判断是否启用 Executor 的动态分配，主要由参数 spark. dynamicAllocation. enabled 控制，当然 AQE 也可以。在非 Local 模式下或者当 spark. dynamicAllocation.testing 属性为 true 时启用 executorAllocationManager。</p>
<p>其次，在 SchedulerBackend 的实现类同时实现了特质 ExecutorAllocationClient 的情况下，才会创建 ExecutorAllocationManager 。</p>
<p>最后，调用 ExecutorAllocationManager 的 start 方法启动ExecutorAllocationManager。</p>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>ExecutorAllocationManager 的 start 方法的实现如下。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Register for scheduler callbacks to decide when to add and remove executors, and start * the scheduling task. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  listenerBus.addToManagementQueue(listener)  </span><br><span class="line">  listenerBus.addToManagementQueue(executorMonitor)  </span><br><span class="line">  cleaner.foreach(_.attachListener(executorMonitor))  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> scheduleTask = <span class="keyword">new</span> <span class="type">Runnable</span>() &#123;  </span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;  </span><br><span class="line">      <span class="keyword">try</span> &#123;  </span><br><span class="line">        schedule()  </span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">        <span class="keyword">case</span> ct: <span class="type">ControlThrowable</span> =&gt;  </span><br><span class="line">          <span class="keyword">throw</span> ct  </span><br><span class="line">        <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;  </span><br><span class="line">          logWarning(<span class="string">s&quot;Uncaught exception in thread <span class="subst">$&#123;Thread.currentThread().getName&#125;</span>&quot;</span>, t)  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (!testing || conf.get(<span class="type">TEST_DYNAMIC_ALLOCATION_SCHEDULE_ENABLED</span>)) &#123;  </span><br><span class="line">    executor.scheduleWithFixedDelay(scheduleTask, <span class="number">0</span>, intervalMillis, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// copy the maps inside synchronize to ensure not being modified  </span></span><br><span class="line">  <span class="keyword">val</span> (numExecutorsTarget, numLocalityAware) = synchronized &#123;  </span><br><span class="line">    <span class="keyword">val</span> numTarget = numExecutorsTargetPerResourceProfileId.toMap  </span><br><span class="line">    <span class="keyword">val</span> numLocality = numLocalityAwareTasksPerResourceProfileId.toMap  </span><br><span class="line">    (numTarget, numLocality)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  client.requestTotalExecutors(numExecutorsTarget, numLocalityAware, rpIdToHostToLocalTaskCount)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 start 方法内的执行步骤主要是：</p>
<ol>
<li>向事件总线添加 ExecutorAllocationListener 。</li>
<li>创建定时调度的任务 scheduleTask ，此任务主要调用 schedule 方法。</li>
<li>将 scheduleTask 提交给 executor (executor 是只有一个线程的 ScheduledThreadPoolExecutor)，以固定的间隔 intervalMillis (值为 100)进行调度。</li>
<li>) 用 ExecutorAllocationClient 的 requestTotalExecutors 方法清求所有的 Executor。 numExecutorsTarget 是动态分配 Executor 的总数，取 spark. dynamicAllocation. initialExecutors、 spark. dynamicAllocation. minExecutors 、spark. executor. instance 三个属性配置的最大值。 localityAwareTasks 是由本地性偏好的 Task 数量。hostToLocalTask Count 是 Host 与想要在此节点上运行的 Task 的效量之间的映射关系。</li>
</ol>
<p>定时任务 scheduleTask 会按照固定的时间间隔调用 ExecutorAllocationManager 的 schedule 方法，以调整待执行 Executor 请求的数量和运行的 Executor 的数量。schedule 方法的实现如下。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * This is called at a fixed interval to regulate the number of pending executor requests * and number of executors running. * * First, adjust our requested executors based on the add time and our current needs. * Then, if the remove time for an existing executor has expired, kill the executor. * * This is factored out into its own method for testing. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">schedule</span></span>(): <span class="type">Unit</span> = synchronized &#123;  </span><br><span class="line">  <span class="keyword">val</span> executorIdsToBeRemoved = executorMonitor.timedOutExecutors()  </span><br><span class="line">  <span class="keyword">if</span> (executorIdsToBeRemoved.nonEmpty) &#123;  </span><br><span class="line">    initializing = <span class="literal">false</span>  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Update executor target number only after initializing flag is unset  </span></span><br><span class="line">  updateAndSyncNumExecutorsTarget(clock.nanoTime())  </span><br><span class="line">  <span class="keyword">if</span> (executorIdsToBeRemoved.nonEmpty) &#123;  </span><br><span class="line">    removeExecutors(executorIdsToBeRemoved)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行的逻辑如下：</p>
<ol>
<li>调用 updateAndSyncNumExecutorsTarget 方法重新计算所需的 Executor 数量，并更新请求的 Executor 数量。</li>
<li>对过期的 Executor 进行删除。removeExecutors 方法将利用 ExecutorAllocationClient 的 kil Executors 方法通知集群管理器 “杀死” Executor。killExecutors 方法需要 ExecutorAllocationClient 的实现类去实现。</li>
</ol>
<p>updateAndSyncNumExecutorsTarget 方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Updates our target number of executors for each ResourceProfile and then syncs the result * with the cluster manager. * * Check to see whether our existing allocation and the requests we&#x27;ve made previously exceed our * current needs. If so, truncate our target and let the cluster manager know so that it can * cancel pending requests that are unneeded. * * If not, and the add time has expired, see if we can request new executors and refresh the add * time. * * @return the delta in the target number of executors.  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateAndSyncNumExecutorsTarget</span></span>(now: <span class="type">Long</span>): <span class="type">Int</span> = synchronized &#123;  </span><br><span class="line">  <span class="keyword">if</span> (initializing) &#123;  </span><br><span class="line">    <span class="comment">// Do not change our target while we are still initializing,  </span></span><br><span class="line">    <span class="comment">// Otherwise the first job may have to ramp up unnecessarily    </span></span><br><span class="line">    <span class="number">0</span>  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="keyword">val</span> updatesNeeded = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">ExecutorAllocationManager</span>.<span class="type">TargetNumUpdates</span>]  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Update targets for all ResourceProfiles then do a single request to the cluster manager  </span></span><br><span class="line">    numExecutorsTargetPerResourceProfileId.foreach &#123; <span class="keyword">case</span> (rpId, targetExecs) =&gt;  </span><br><span class="line">      <span class="keyword">val</span> maxNeeded = maxNumExecutorsNeededPerResourceProfile(rpId)  </span><br><span class="line">      <span class="keyword">if</span> (maxNeeded &lt; targetExecs) &#123;  </span><br><span class="line">        <span class="comment">// The target number exceeds the number we actually need, so stop adding new  </span></span><br><span class="line">        <span class="comment">// executors and inform the cluster manager to cancel the extra pending requests  </span></span><br><span class="line">        <span class="comment">// We lower the target number of executors but don&#x27;t actively kill any yet.  Killing is        // controlled separately by an idle timeout.  It&#x27;s still helpful to reduce        // the target number in case an executor just happens to get lost (e.g., bad hardware,        // or the cluster manager preempts it) -- in that case, there is no point in trying        // to immediately  get a new executor, since we wouldn&#x27;t even use it yet.        </span></span><br><span class="line">        decrementExecutorsFromTarget(maxNeeded, rpId, updatesNeeded)  </span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (addTime != <span class="type">NOT_SET</span> &amp;&amp; now &gt;= addTime) &#123;  </span><br><span class="line">        addExecutorsToTarget(maxNeeded, rpId, updatesNeeded)  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    doUpdateRequest(updatesNeeded.toMap, now)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行步骤如下：</p>
<ol>
<li>如果 ExecutorAllocationManager 还在初始化，则返回 0。</li>
<li>调用 maxNumExecutorsNeededPerResourceProfile 方法获得实际需要的 Executor 的最大数量 maxNeeded。</li>
<li>如果 targetExecs 大于 maxNeeded，就执行 decrementExecutorsFromTarget 方法，对 targetExecs 数量进行削减，但是这个时候并不会真正 kill Executor，Executor 只有在超出空闲时间后才回被 kill。</li>
</ol>
<h2 id="创建与启动ContextCleaner"><a href="#创建与启动ContextCleaner" class="headerlink" title="创建与启动ContextCleaner"></a>创建与启动ContextCleaner</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">_cleaner =  </span><br><span class="line">  <span class="keyword">if</span> (_conf.get(<span class="type">CLEANER_REFERENCE_TRACKING</span>)) &#123;  </span><br><span class="line">    <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ContextCleaner</span>(<span class="keyword">this</span>, _shuffleDriverComponents))  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    <span class="type">None</span>  </span><br><span class="line">  &#125;  </span><br><span class="line">_cleaner.foreach(_.start())</span><br></pre></td></tr></table></figure>
<p>通过配置属性 spark. cleaner. referenceTracking (默认是 true) 来决定是否启用 ContextCleaner。</p>
<p>在 start 方法中有三个步骤：</p>
<ol>
<li>将 cleaningThread 设置为守护线程，并命名为”Spark Context Cleaner”</li>
<li>启动 cleaningThread</li>
<li>给 periodicGCService 设置以 periodicGCInterval 作为时间间隔定时进行 GC 操作的任务。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Start the cleaner. */</span>  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  cleaningThread.setDaemon(<span class="literal">true</span>)  </span><br><span class="line">  cleaningThread.setName(<span class="string">&quot;Spark Context Cleaner&quot;</span>)  </span><br><span class="line">  cleaningThread.start()  </span><br><span class="line">  periodicGCService.scheduleAtFixedRate(() =&gt; <span class="type">System</span>.gc(),  </span><br><span class="line">    periodicGCInterval, periodicGCInterval, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>除了 G C 的定时器， ContextCleaner 其余部分的工作原理和 listenerBus 一样 ( 也采用监听器模式，由异步线程来处理）。</p>
<p>异步线程实际只是调用 keepCleaning 方法，代码如下。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> cleaningThread = <span class="keyword">new</span> <span class="type">Thread</span>() &#123; <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = keepCleaning() &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Keep cleaning RDD, shuffle, and broadcast state. */</span>  </span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">keepCleaning</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryOrStopSparkContext(sc) &#123;  </span><br><span class="line">  <span class="keyword">while</span> (!stopped) &#123;  </span><br><span class="line">    <span class="keyword">try</span> &#123;  </span><br><span class="line">      <span class="keyword">val</span> reference = <span class="type">Option</span>(referenceQueue.remove(<span class="type">ContextCleaner</span>.<span class="type">REF_QUEUE_POLL_TIMEOUT</span>))  </span><br><span class="line">        .map(_.asInstanceOf[<span class="type">CleanupTaskWeakReference</span>])  </span><br><span class="line">      <span class="comment">// Synchronize here to avoid being interrupted on stop()  </span></span><br><span class="line">      synchronized &#123;  </span><br><span class="line">        reference.foreach &#123; ref =&gt;  </span><br><span class="line">          logDebug(<span class="string">&quot;Got cleaning task &quot;</span> + ref.task)  </span><br><span class="line">          referenceBuffer.remove(ref)  </span><br><span class="line">          ref.task <span class="keyword">match</span> &#123;  </span><br><span class="line">            <span class="keyword">case</span> <span class="type">CleanRDD</span>(rddId) =&gt;  </span><br><span class="line">              doCleanupRDD(rddId, blocking = blockOnCleanupTasks)  </span><br><span class="line">            <span class="keyword">case</span> <span class="type">CleanShuffle</span>(shuffleId) =&gt;  </span><br><span class="line">              doCleanupShuffle(shuffleId, blocking = blockOnShuffleCleanupTasks)  </span><br><span class="line">            <span class="keyword">case</span> <span class="type">CleanBroadcast</span>(broadcastId) =&gt;  </span><br><span class="line">              doCleanupBroadcast(broadcastId, blocking = blockOnCleanupTasks)  </span><br><span class="line">            <span class="keyword">case</span> <span class="type">CleanAccum</span>(accId) =&gt;  </span><br><span class="line">              doCleanupAccum(accId, blocking = blockOnCleanupTasks)  </span><br><span class="line">            <span class="keyword">case</span> <span class="type">CleanCheckpoint</span>(rddId) =&gt;  </span><br><span class="line">              doCleanCheckpoint(rddId)  </span><br><span class="line">            <span class="keyword">case</span> <span class="type">CleanSparkListener</span>(listener) =&gt;  </span><br><span class="line">              doCleanSparkListener(listener)  </span><br><span class="line">          &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">      <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> <span class="keyword">if</span> stopped =&gt; <span class="comment">// ignore  </span></span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logError(<span class="string">&quot;Error in cleaning thread&quot;</span>, e)  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>异步线程会不断匹配各种引用（弱引用），当对应对象没有被引用时，就会被 GC 清理。</p>
<p>以 doCleanupRDD 为例。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Perform RDD cleanup. */</span>  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doCleanupRDD</span></span>(rddId: <span class="type">Int</span>, blocking: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="keyword">try</span> &#123;  </span><br><span class="line">    logDebug(<span class="string">&quot;Cleaning RDD &quot;</span> + rddId)  </span><br><span class="line">    sc.unpersistRDD(rddId, blocking)  </span><br><span class="line">    listeners.asScala.foreach(_.rddCleaned(rddId))  </span><br><span class="line">    logDebug(<span class="string">&quot;Cleaned RDD &quot;</span> + rddId)  </span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;  </span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logError(<span class="string">&quot;Error cleaning RDD &quot;</span> + rddId, e)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>在方法里会调用 unpersistRDD，清理所有 blockmanager 中（内存或磁盘上）移除 RDD 相关的 RDD blocks。</li>
<li>从 persistentRdds 中移除对 RDD 的跟踪</li>
<li>调用所有监听器的 rdd Cleaned 方法。</li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="环境更新"><a href="#环境更新" class="headerlink" title="环境更新"></a>环境更新</h3><ul>
<li>在 SparkContext 中还有很多关于 jar 的提交和相关环境信息更新的操作，感兴趣的读者可以自行查看。</li>
<li>还有一些常用的方法：braodcast、runjob（spark 任务提交的起点）、setCheckpoint 等等。</li>
</ul>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.JPG" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/07/22/Spark-%E6%BA%90%E7%A0%81-persist%E4%B8%8Echeckpoint/" title="Spark-源码-persist与checkpoint"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Spark-源码-persist与checkpoint</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-Cache%E4%B8%8Echeckpoint/" title="Spark-基础-Cache 与 Checkpoint"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-Cache 与 Checkpoint</div></div></a></div><div><a href="/2024/03/20/Spark-%E5%9F%BA%E7%A1%80-Shuffle%E6%9C%BA%E5%88%B6/" title="Spark-基础-Shuffle机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-20</div><div class="title">Spark-基础-Shuffle机制</div></div></a></div><div><a href="/2024/03/20/Spark-%E5%9F%BA%E7%A1%80-%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/" title="Spark-基础-核心编程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-20</div><div class="title">Spark-基础-核心编程</div></div></a></div><div><a href="/2024/03/21/Spark-%E5%9F%BA%E7%A1%80-%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6/" title="Spark-基础-任务调度机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-21</div><div class="title">Spark-基础-任务调度机制</div></div></a></div><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-%E7%B4%AF%E5%8A%A0%E5%99%A8%E4%B8%8E%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/" title="Spark-基础-累加器和Broadcast"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-累加器和Broadcast</div></div></a></div><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F/" title="Spark-基础-运行模式"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-运行模式</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CHLi</div><div class="author-info__description">Welcome to Mr.Li's blog</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">71</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/MLiLay"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome! This is MLiLay's Blog.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkContext-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.0.1.</span> <span class="toc-text">SparkContext 是什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkConf-%E7%BB%84%E6%88%90"><span class="toc-number">1.0.2.</span> <span class="toc-text">SparkConf 组成</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SparkContext-%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">SparkContext 的初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkConf-%E5%88%9B%E5%BB%BA"><span class="toc-number">2.0.1.</span> <span class="toc-text">SparkConf 创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkEnv-%E5%88%9B%E5%BB%BA"><span class="toc-number">2.0.2.</span> <span class="toc-text">SparkEnv 创建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAheartbeatReceiver"><span class="toc-number">2.1.</span> <span class="toc-text">创建heartbeatReceiver</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-DAGScheduler-%E5%92%8CTaskScheduler"><span class="toc-number">2.2.</span> <span class="toc-text">创建 DAGScheduler 和TaskScheduler</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96BlockManager"><span class="toc-number">2.3.</span> <span class="toc-text">初始化BlockManager</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E5%BA%A6%E9%87%8F%E7%B3%BB%E7%BB%9F"><span class="toc-number">2.4.</span> <span class="toc-text">启动度量系统</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%97%A5%E5%BF%97%E7%9B%91%E5%90%AC%E5%99%A8"><span class="toc-number">2.5.</span> <span class="toc-text">创建日志监听器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%92%8C%E5%90%AF%E5%8A%A8-ExecutorAllocationManager"><span class="toc-number">2.6.</span> <span class="toc-text">创建和启动 ExecutorAllocationManager</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA"><span class="toc-number">2.6.1.</span> <span class="toc-text">创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8"><span class="toc-number">2.6.2.</span> <span class="toc-text">启动</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%8E%E5%90%AF%E5%8A%A8ContextCleaner"><span class="toc-number">2.7.</span> <span class="toc-text">创建与启动ContextCleaner</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">2.8.</span> <span class="toc-text">其他</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E6%9B%B4%E6%96%B0"><span class="toc-number">2.8.1.</span> <span class="toc-text">环境更新</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/29/Spark-%E6%BA%90%E7%A0%81-SparkContext/" title="Spark-源码-SparkContext">Spark-源码-SparkContext</a><time datetime="2024-07-29T06:27:54.293Z" title="发表于 2024-07-29 14:27:54">2024-07-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/22/Spark-%E6%BA%90%E7%A0%81-persist%E4%B8%8Echeckpoint/" title="Spark-源码-persist与checkpoint">Spark-源码-persist与checkpoint</a><time datetime="2024-07-22T09:02:52.238Z" title="发表于 2024-07-22 17:02:52">2024-07-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/11/Celeborn/" title="Celeborn">Celeborn</a><time datetime="2024-07-11T12:46:48.425Z" title="发表于 2024-07-11 20:46:48">2024-07-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/11/Spark-%E6%BA%90%E7%A0%81-Broadcast/" title="Spark-源码-Broadcast">Spark-源码-Broadcast</a><time datetime="2024-07-11T12:29:06.531Z" title="发表于 2024-07-11 20:29:06">2024-07-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/01/Spark-%E6%BA%90%E7%A0%81-%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/" title="Spark-源码-任务提交流程（Yarn）">Spark-源码-任务提交流程（Yarn）</a><time datetime="2024-07-01T13:56:32.213Z" title="发表于 2024-07-01 21:56:32">2024-07-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By CHLi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>