<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Spark-源码-persist与checkpoint | Mr.Li's blog</title><meta name="author" content="CHLi"><meta name="copyright" content="CHLi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="以下 spark 版本信息 3.0.0+。4.0.0 代码不同 persist概述cache 和 persist 都是将 RDD 进行缓存，提升数据的复用，在 Spark 任务计算过程中不用反复根据血缘关系计算某一个 RDD。 在 Spark 任务中即使不认为使用，Spark 在获取执行计划时也会根据 RDD 的依赖关系，对 RDD 进行persist Cache 是只存储在内存中，persist">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark-源码-persist与checkpoint">
<meta property="og:url" content="http://example.com/2024/07/22/Spark-%E6%BA%90%E7%A0%81-persist%E4%B8%8Echeckpoint/index.html">
<meta property="og:site_name" content="Mr.Li&#39;s blog">
<meta property="og:description" content="以下 spark 版本信息 3.0.0+。4.0.0 代码不同 persist概述cache 和 persist 都是将 RDD 进行缓存，提升数据的复用，在 Spark 任务计算过程中不用反复根据血缘关系计算某一个 RDD。 在 Spark 任务中即使不认为使用，Spark 在获取执行计划时也会根据 RDD 的依赖关系，对 RDD 进行persist Cache 是只存储在内存中，persist">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/avatar.JPG">
<meta property="article:published_time" content="2024-07-22T09:02:52.238Z">
<meta property="article:modified_time" content="2024-07-29T06:29:16.694Z">
<meta property="article:author" content="CHLi">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/avatar.JPG"><link rel="shortcut icon" href="/img/spaceshuttle.png"><link rel="canonical" href="http://example.com/2024/07/22/Spark-%E6%BA%90%E7%A0%81-persist%E4%B8%8Echeckpoint/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark-源码-persist与checkpoint',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-29 14:29:16'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">77</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/topGraph.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Mr.Li's blog"><span class="site-name">Mr.Li's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark-源码-persist与checkpoint</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-22T09:02:52.238Z" title="发表于 2024-07-22 17:02:52">2024-07-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-29T06:29:16.694Z" title="更新于 2024-07-29 14:29:16">2024-07-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark-源码-persist与checkpoint"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>以下 spark 版本信息 3.0.0+。4.0.0 代码不同</p>
<h1 id="persist"><a href="#persist" class="headerlink" title="persist"></a>persist</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>cache 和 persist 都是将 RDD 进行缓存，提升数据的复用，在 Spark 任务计算过程中不用反复根据血缘关系计算某一个 RDD。</p>
<p>在 Spark 任务中即使不认为使用，Spark 在获取执行计划时也会根据 RDD 的依赖关系，对 RDD 进行persist</p>
<p>Cache 是只存储在内存中，persist 可以根据自己设定的存储等级进行存储。</p>
<h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><p>对于下面的 SQL 是一个最简单的 Cache 例子。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span>  </span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span>  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDCache</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> spark:<span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()  </span><br><span class="line">    .master(<span class="string">&quot;local[1]&quot;</span>)  </span><br><span class="line">    .appName(<span class="string">&quot;SparkByExamples.com&quot;</span>)  </span><br><span class="line">    .getOrCreate()  </span><br><span class="line">  <span class="keyword">val</span> sc = spark.sparkContext  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;src/main/resources/zipcodes-noheader.csv&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> rdd2:<span class="type">RDD</span>[<span class="type">ZipCode</span>] = rdd.map(row=&gt;&#123;  </span><br><span class="line">    <span class="keyword">val</span> strArray = row.split(<span class="string">&quot;,&quot;</span>)  </span><br><span class="line">    <span class="type">ZipCode</span>(strArray(<span class="number">0</span>).toInt,strArray(<span class="number">1</span>),strArray(<span class="number">3</span>),strArray(<span class="number">4</span>))  </span><br><span class="line">  &#125;)  </span><br><span class="line">  </span><br><span class="line">  rdd2.cache()  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  println(rdd2.count())  <span class="comment">// 懒执行，现在才会触发执行</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看到在 Cache 方法内部其实是调用了 persist 方法，persist 方法的默认存储等级是 MEMORY-ONLY。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Persist this RDD with the default storage level (`MEMORY_ONLY`).  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Persist this RDD with the default storage level (`MEMORY_ONLY`).  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br></pre></td></tr></table></figure>
<p>所以我们只看 persist 方法即可。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Set this RDD&#x27;s storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet. Local checkpointing is an exception. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;  </span><br><span class="line">  <span class="keyword">if</span> (isLocallyCheckpointed) &#123;  </span><br><span class="line">    <span class="comment">// This means the user previously called localCheckpoint(), which should have already  </span></span><br><span class="line">    <span class="comment">// marked this RDD for persisting. Here we should override the old storage level with    </span></span><br><span class="line">    <span class="comment">// one that is explicitly requested by the user (after adapting it to use disk).    persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true)  </span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    persist(newLevel, allowOverride = <span class="literal">false</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实首先是判断 RDD 是否被存储过，如果被标记过则需要更改其存储等级再继续 persist。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Mark this RDD for persisting using the specified level. * * @param newLevel the target storage level  </span></span><br><span class="line"><span class="comment"> * @param allowOverride whether to override any existing level with the new one  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>, allowOverride: <span class="type">Boolean</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;  </span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Handle changes of StorageLevel  </span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span> &amp;&amp; newLevel != storageLevel &amp;&amp; !allowOverride) &#123;  </span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(  </span><br><span class="line">      <span class="string">&quot;Cannot change storage level of an RDD after it was already assigned a level&quot;</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="comment">// If this is the first time this RDD is marked for persisting, register it  </span></span><br><span class="line">  <span class="comment">// with the SparkContext for cleanups and accounting. Do this only once.  </span></span><br><span class="line"><span class="keyword">if</span> (storageLevel == <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;  </span><br><span class="line">    sc.cleaner.foreach(_.registerRDDForCleanup(<span class="keyword">this</span>))  </span><br><span class="line">    sc.persistRDD(<span class="keyword">this</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">  storageLevel = newLevel  </span><br><span class="line">  <span class="keyword">this</span>  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>persist 方法的内部其实很简单：</p>
<ul>
<li>在判断是第一次存储后就首先在 cleaner 内部对 RDD 进行注册，用于后续 Task 执行完毕后对其的清除操作。</li>
<li>之后调用 SparkContext 的 persistRDD 方法对其进行 persist。<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Register an RDD to be persisted in memory and/or disk storage */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">persistRDD</span></span>(rdd: <span class="type">RDD</span>[_]): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  persistentRdds(rdd.id) = rdd  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Keeps track of all persisted RDDs  </span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">val</span> persistentRdds = &#123;  </span><br><span class="line">  <span class="keyword">val</span> map: <span class="type">ConcurrentMap</span>[<span class="type">Int</span>, <span class="type">RDD</span>[_]] = <span class="keyword">new</span> <span class="type">MapMaker</span>().weakValues().makeMap[<span class="type">Int</span>, <span class="type">RDD</span>[_]]()  </span><br><span class="line">  map.asScala  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>SparkContext 的 persistRDD 方法在内部其实并没有真实得对 RDD 进行缓存，只是通过一个软链将其放在 persistentRdds 中，persistentRdds 本质上是一个 ConcurrentMap。</p>
<p>所以其实在 persist 方法被调用时，spark 只是对其进行标记，并没有真正对其进行存储。</p>
<p>真正的缓存过程其实是在 Executor 完成任务后，根据 RDD 的依赖关系及其存储等级将其存储在其 BlockManager 中，存储后向 Driver 的 BlockManagerMaster 进行元信息的存储。同时，后续依赖这个 RDD 的 Task 会根据本地性原则优先发往该 Executor 进行执行。</p>
<p>Executor 调取对应 Block 数据的流程如下：</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20240718193014.png" alt="image.png"></p>
<h1 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h1><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p><strong>checkpoint 是一个 transformation 算子，并不是 action 算子</strong></p>
<p>Checkpoint 和 persist 的区别：</p>
<ul>
<li>存储方面：persist 通过设置存储级别将 RDD 存储到内存或者是磁盘中，Checkpoint 是将其存储到 HDFS 上。所以，因为 HDFS 的可靠性，Checkpoint 的可靠性会高于 persist。</li>
<li>RDD 血缘方面：persist 只是将中间的 RDD 进行持久化到 Blockmanager 中，并不会切断 RDD 之间的血缘关系，Checkpoint 会向上溯源，当有 Checkpoint 时会对切断血缘关系，并将父 RDD 依赖填充为一个 Checkpoint。</li>
</ul>
<p>Checkpont 的运行流程：</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20240721160540.png" alt="image.png"></p>
<p>对于 checkpoint 的存储其实是开启了一个 job。</p>
<p><strong>注</strong>：在使用 checkpoint 前最好是进行 <code>rdd.presist(StoreLevel.DISK_ONLY)</code>，因为不先进行 persist，会从头开始计算 RDD，然后进行 checkpoint 的存储。如果首先对 RDD 使用 <code>rdd.presist(StoreLevel.DISK_ONLY)</code>，就会直接从硬盘中读取数据，就不需要进行重新的计算操作。</p>
<h2 id="源码-1"><a href="#源码-1" class="headerlink" title="源码"></a>源码</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20240721161800.png" alt="image.png"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.setCheckpointDir(<span class="string">&quot;hdfs://localhost:9000/checkpoint0727&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在使用 checkpoint 时，首先需要调用 SparkContext 的 setCheckpointDir 方法对存储的位置进行设定。</p>
<p>setCheckpointDir 方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Set the directory under which RDDs are going to be checkpointed. * @param directory path to the directory where checkpoint files will be stored  </span></span><br><span class="line"><span class="comment"> * (must be HDFS path if running in cluster) */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setCheckpointDir</span></span>(directory: <span class="type">String</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// If we are running on a cluster, log a warning if the directory is local.  </span></span><br><span class="line">  <span class="comment">// Otherwise, the driver may attempt to reconstruct the checkpointed RDD from  // its own local file system, which is incorrect because the checkpoint files  // are actually on the executor machines.  if (!isLocal &amp;&amp; Utils.nonLocalPaths(directory).isEmpty) &#123;  </span></span><br><span class="line">    logWarning(<span class="string">&quot;Spark is not running in local mode, therefore the checkpoint directory &quot;</span> +  </span><br><span class="line">      <span class="string">s&quot;must not be on the local filesystem. Directory &#x27;<span class="subst">$directory</span>&#x27; &quot;</span> +  </span><br><span class="line">      <span class="string">&quot;appears to be on the local filesystem.&quot;</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  checkpointDir = <span class="type">Option</span>(directory).map &#123; dir =&gt;  </span><br><span class="line">    <span class="keyword">val</span> path = <span class="keyword">new</span> <span class="type">Path</span>(dir, <span class="type">UUID</span>.randomUUID().toString)  </span><br><span class="line">    <span class="keyword">val</span> fs = path.getFileSystem(hadoopConfiguration)  </span><br><span class="line">    fs.mkdirs(path)  </span><br><span class="line">    fs.getFileStatus(path).getPath.toString  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在内部其实就是通过 Hadoop 的 API 在 HDFS 上创建了一个对应目录。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">10000</span>)</span><br><span class="line">rdd1.checkpoint</span><br></pre></td></tr></table></figure>
<p>当我们创建一个 RDD，然后使用 checkpoint，因为 checkpoint 其实是个 transformation 算子，所以调用后在 HDFS 上其实并不会对相应的 RDD 进行计算和存储。</p>
<p>checkpoint 方法：</p>
<p>位置：org.apache.spark.rdd.RDD.scala</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent  </span></span><br><span class="line"><span class="comment"> * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>(): <span class="type">Unit</span> = <span class="type">RDDCheckpointData</span>.synchronized &#123;  </span><br><span class="line">  <span class="comment">// <span class="doctag">NOTE:</span> we use a global lock here due to complexities downstream with ensuring  </span></span><br><span class="line">  <span class="comment">// children RDD partitions point to the correct parent partitions. In the future  // we should revisit this consideration.  if (context.checkpointDir.isEmpty) &#123;  </span></span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">&quot;Checkpoint directory has not been set in the SparkContext&quot;</span>)  </span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (checkpointData.isEmpty) &#123;  </span><br><span class="line">    checkpointData = <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ReliableRDDCheckpointData</span>(<span class="keyword">this</span>))  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到在其中会创建一个 ReliableRDDCheckpointData 对象。</p>
<p>ReliableRDDCheckpointData 其实是继承于 RDDCheckpointData 抽象类，最关键的是他重写了 doCheckpoint 方法，也是这个方法会在 Executor 在执行 Task 时，通过调用进行 checkpoint 操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="keyword">package</span> org.apache.spark.rdd  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> scala.reflect.<span class="type">ClassTag</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.<span class="type">Path</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> org.apache.spark._  </span><br><span class="line"><span class="keyword">import</span> org.apache.spark.internal.<span class="type">Logging</span>  </span><br><span class="line"><span class="keyword">import</span> org.apache.spark.internal.config.<span class="type">CLEANER_REFERENCE_TRACKING_CLEAN_CHECKPOINTS</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * An implementation of checkpointing that writes the RDD data to reliable storage. * This allows drivers to be restarted on failure with previously computed state. */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ReliableRDDCheckpointData</span>[<span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params">@transient private val rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>)  <span class="keyword">extends</span> <span class="title">RDDCheckpointData</span>[<span class="type">T</span>](<span class="params">rdd</span>) <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// The directory to which the associated RDD has been checkpointed to  </span></span><br><span class="line">  <span class="comment">// This is assumed to be a non-local path that points to some reliable storage  private val cpDir: String =  </span></span><br><span class="line">    <span class="type">ReliableRDDCheckpointData</span>.checkpointPath(rdd.context, rdd.id)  </span><br><span class="line">      .map(_.toString)  </span><br><span class="line">      .getOrElse &#123; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">&quot;Checkpoint dir must be specified.&quot;</span>) &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**  </span></span><br><span class="line"><span class="comment">   * Return the directory to which this RDD was checkpointed.   * If the RDD is not checkpointed yet, return None.   */</span>  <span class="function"><span class="keyword">def</span> <span class="title">getCheckpointDir</span></span>: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">RDDCheckpointData</span>.synchronized &#123;  </span><br><span class="line">    <span class="keyword">if</span> (isCheckpointed) &#123;  </span><br><span class="line">      <span class="type">Some</span>(cpDir.toString)  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      <span class="type">None</span>  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**  </span></span><br><span class="line"><span class="comment">   * Materialize this RDD and write its content to a reliable DFS.   * This is called immediately after the first action invoked on this RDD has completed.   */</span>  <span class="keyword">protected</span> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doCheckpoint</span></span>(): <span class="type">CheckpointRDD</span>[<span class="type">T</span>] = &#123;  </span><br><span class="line">    <span class="keyword">val</span> newRDD = <span class="type">ReliableCheckpointRDD</span>.writeRDDToCheckpointDirectory(rdd, cpDir)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Optionally clean our checkpoint files if the reference is out of scope  </span></span><br><span class="line">    <span class="keyword">if</span> (rdd.conf.get(<span class="type">CLEANER_REFERENCE_TRACKING_CLEAN_CHECKPOINTS</span>)) &#123;  </span><br><span class="line">      rdd.context.cleaner.foreach &#123; cleaner =&gt;  </span><br><span class="line">        cleaner.registerRDDCheckpointDataForCleanup(newRDD, rdd.id)  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    logInfo(<span class="string">s&quot;Done checkpointing RDD <span class="subst">$&#123;rdd.id&#125;</span> to <span class="subst">$cpDir</span>, new parent is RDD <span class="subst">$&#123;newRDD.id&#125;</span>&quot;</span>)  </span><br><span class="line">    newRDD  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">object</span> <span class="title">ReliableRDDCheckpointData</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** Return the path of the directory to which this RDD&#x27;s checkpoint data is written. */</span>  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">checkpointPath</span></span>(sc: <span class="type">SparkContext</span>, rddId: <span class="type">Int</span>): <span class="type">Option</span>[<span class="type">Path</span>] = &#123;  </span><br><span class="line">    sc.checkpointDir.map &#123; dir =&gt; <span class="keyword">new</span> <span class="type">Path</span>(dir, <span class="string">s&quot;rdd-<span class="subst">$rddId</span>&quot;</span>) &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** Clean up the files associated with the checkpoint data for this RDD. */</span>  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cleanCheckpoint</span></span>(sc: <span class="type">SparkContext</span>, rddId: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">    checkpointPath(sc, rddId).foreach &#123; path =&gt;  </span><br><span class="line">      path.getFileSystem(sc.hadoopConfiguration).delete(path, <span class="literal">true</span>)  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>checkpoint 对初始化到这里也就结束了。</p>
<h3 id="数据写入"><a href="#数据写入" class="headerlink" title="数据写入"></a>数据写入</h3><p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20240722155551.png" alt="image.png"></p>
<p>spark job 在执行的时候会调用 SparkContext 的 runJob 方法对任务进行执行。</p>
<p>在执行完成后会执行 RDD 的 doCheckpoint 方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Run a function on a given set of partitions in an RDD and pass the results to the given * handler function. This is the main entry point for all actions in Spark. * * @param rdd target RDD to run tasks on  </span></span><br><span class="line"><span class="comment"> * @param func a function to run on each partition of the RDD  </span></span><br><span class="line"><span class="comment"> * @param partitions set of partitions to run on; some jobs may not want to compute on all  </span></span><br><span class="line"><span class="comment"> * partitions of the target RDD, e.g. for operations like `first()`  </span></span><br><span class="line"><span class="comment"> * @param resultHandler callback to pass each result to  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](  </span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],  </span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,  </span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],  </span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="keyword">if</span> (stopped.get()) &#123;  </span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;SparkContext has been shutdown&quot;</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">val</span> callSite = getCallSite  </span><br><span class="line">  <span class="keyword">val</span> cleanedFunc = clean(func)  </span><br><span class="line">  logInfo(<span class="string">&quot;Starting job: &quot;</span> + callSite.shortForm)  </span><br><span class="line">  <span class="keyword">if</span> (conf.getBoolean(<span class="string">&quot;spark.logLineage&quot;</span>, <span class="literal">false</span>)) &#123;  </span><br><span class="line">    logInfo(<span class="string">&quot;RDD&#x27;s recursive dependencies:\n&quot;</span> + rdd.toDebugString)  </span><br><span class="line">  &#125;  </span><br><span class="line">  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)   <span class="comment">// 任务提交，执行</span></span><br><span class="line">  progressBar.foreach(_.finishAll())  </span><br><span class="line">  rdd.doCheckpoint()  <span class="comment">// checkpoint</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 doCheckpoint()内部其实是向上迭代，如果当前的 RDD 是 checkpointData（也就是前面在执行 RDD 的 checkpoint 方法时，返回了一个 ReliableRDDCheckpointData）就调用 checkpoint 方法，这个时候调用的是 RDDCheckpointData 的 checkpoint 方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD * has completed (therefore the RDD has been materialized and potentially stored in memory). * doCheckpoint() is called recursively on the parent RDDs. */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">doCheckpoint</span></span>(): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="type">RDDOperationScope</span>.withScope(sc, <span class="string">&quot;checkpoint&quot;</span>, allowNesting = <span class="literal">false</span>, ignoreParent = <span class="literal">true</span>) &#123;  </span><br><span class="line">    <span class="keyword">if</span> (!doCheckpointCalled) &#123;  </span><br><span class="line">      doCheckpointCalled = <span class="literal">true</span>  </span><br><span class="line">      <span class="keyword">if</span> (checkpointData.isDefined) &#123;  <span class="comment">// 是否调用checkpoint</span></span><br><span class="line">        <span class="keyword">if</span> (checkpointAllMarkedAncestors) &#123;  </span><br><span class="line">          <span class="comment">// TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint  </span></span><br><span class="line">          <span class="comment">// them in parallel.          // Checkpoint parents first because our lineage will be truncated after we          // checkpoint ourselves          </span></span><br><span class="line">          dependencies.foreach(_.rdd.doCheckpoint())  <span class="comment">// 向上迭代</span></span><br><span class="line">        &#125;  </span><br><span class="line">        checkpointData.get.checkpoint()  <span class="comment">// 进行checkpint</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">        dependencies.foreach(_.rdd.doCheckpoint())  <span class="comment">// 向上迭代</span></span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 RDDCheckpointData 的 checkpoint 方法内其实本质上就是对 RDD 进行标记与调用 ReliableRDDCheckpointData 的 doCheckpoint 方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Materialize this RDD and persist its content. * This is called immediately after the first action invoked on this RDD has completed. */</span></span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>(): <span class="type">Unit</span> = &#123;  </span><br><span class="line">  <span class="comment">// Guard against multiple threads checkpointing the same RDD by  </span></span><br><span class="line">  <span class="comment">// atomically flipping the state of this RDDCheckpointData  RDDCheckpointData.synchronized &#123;  </span></span><br><span class="line">    <span class="keyword">if</span> (cpState == <span class="type">Initialized</span>) &#123;  </span><br><span class="line">      <span class="comment">// 标记当前的RDD状态为checkpoint进行中</span></span><br><span class="line">      cpState = <span class="type">CheckpointingInProgress</span>  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      <span class="keyword">return</span>  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> newRDD = doCheckpoint()  <span class="comment">// </span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Update our state and truncate the RDD lineage  </span></span><br><span class="line">  <span class="type">RDDCheckpointData</span>.synchronized &#123;  </span><br><span class="line">	<span class="comment">// 标记当前RDD以及checkpoint完成</span></span><br><span class="line">	cpRDD = <span class="type">Some</span>(newRDD)  </span><br><span class="line">    cpState = <span class="type">Checkpointed</span>  </span><br><span class="line">    rdd.markCheckpointed()  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 ReliableRDDCheckpointData 的 doCheckpoint 方法内部其实就只是调用 ReliableCheckpointRDD 对象的 writeRDDToCheckpointDirectory 方法将需要 checkpoint 的 RDD 写入到 HDFS 上。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**  </span></span><br><span class="line"><span class="comment">   * Materialize this RDD and write its content to a reliable DFS.   * This is called immediately after the first action invoked on this RDD has completed.   */</span>  </span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doCheckpoint</span></span>(): <span class="type">CheckpointRDD</span>[<span class="type">T</span>] = &#123;  </span><br><span class="line">	<span class="comment">// 写入到HDFS上，得到一个新的RDD</span></span><br><span class="line">	<span class="comment">// 新的RDD是ReliableCheckpointRDD，内部保存了HDFS的地址和分区信息</span></span><br><span class="line">    <span class="keyword">val</span> newRDD = <span class="type">ReliableCheckpointRDD</span>.writeRDDToCheckpointDirectory(rdd, cpDir)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Optionally clean our checkpoint files if the reference is out of scope</span></span><br><span class="line">    <span class="comment">// 在 cleaner 中添加，在RDD没有被引用时，对其进行GC</span></span><br><span class="line">    <span class="keyword">if</span> (rdd.conf.get(<span class="type">CLEANER_REFERENCE_TRACKING_CLEAN_CHECKPOINTS</span>)) &#123;  </span><br><span class="line">      rdd.context.cleaner.foreach &#123; cleaner =&gt;  </span><br><span class="line">        cleaner.registerRDDCheckpointDataForCleanup(newRDD, rdd.id)  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    logInfo(<span class="string">s&quot;Done checkpointing RDD <span class="subst">$&#123;rdd.id&#125;</span> to <span class="subst">$cpDir</span>, new parent is RDD <span class="subst">$&#123;newRDD.id&#125;</span>&quot;</span>)  </span><br><span class="line">    newRDD  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 writeRDDToCheckpointDirectory 方法内部其实本质上是起了一个 job 来负责对 RDD 进行 checkpoint。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Write RDD to checkpoint files and return a ReliableCheckpointRDD representing the RDD. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeRDDToCheckpointDirectory</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](  </span><br><span class="line">    originalRDD: <span class="type">RDD</span>[<span class="type">T</span>],  </span><br><span class="line">    checkpointDir: <span class="type">String</span>,  </span><br><span class="line">    blockSize: <span class="type">Int</span> = <span class="number">-1</span>): <span class="type">ReliableCheckpointRDD</span>[<span class="type">T</span>] = &#123;  </span><br><span class="line">  <span class="keyword">val</span> checkpointStartTimeNs = <span class="type">System</span>.nanoTime()  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> sc = originalRDD.sparkContext  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Create the output path for the checkpoint  </span></span><br><span class="line">  <span class="keyword">val</span> checkpointDirPath = <span class="keyword">new</span> <span class="type">Path</span>(checkpointDir)  </span><br><span class="line">  <span class="keyword">val</span> fs = checkpointDirPath.getFileSystem(sc.hadoopConfiguration)  </span><br><span class="line">  <span class="keyword">if</span> (!fs.mkdirs(checkpointDirPath)) &#123;  </span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Failed to create checkpoint path <span class="subst">$checkpointDirPath</span>&quot;</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Save to file, and reload it as an RDD  </span></span><br><span class="line">  <span class="comment">// 广播配置信息到所有的executor上</span></span><br><span class="line">  <span class="keyword">val</span> broadcastedConf = sc.broadcast(  </span><br><span class="line">    <span class="keyword">new</span> <span class="type">SerializableConfiguration</span>(sc.hadoopConfiguration))  </span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> This is expensive because it computes the RDD again unnecessarily (SPARK-8582)  </span></span><br><span class="line">  <span class="comment">// 开一个job负责checkpoint，但是这样会导致从头开始对RDD进行计算这里的花销是巨大的</span></span><br><span class="line">  <span class="comment">// 如果cache了就会节省时间，这个在spark4.0.0中被优化了</span></span><br><span class="line">  sc.runJob(originalRDD,  </span><br><span class="line">    writePartitionToCheckpointFile[<span class="type">T</span>](checkpointDirPath.toString, broadcastedConf) _)  </span><br><span class="line">  <span class="comment">// 保持分区和原来的RDD一致</span></span><br><span class="line">  <span class="keyword">if</span> (originalRDD.partitioner.nonEmpty) &#123;  </span><br><span class="line">    writePartitionerToCheckpointDir(sc, originalRDD.partitioner.get, checkpointDirPath)  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> checkpointDurationMs =  </span><br><span class="line">    <span class="type">TimeUnit</span>.<span class="type">NANOSECONDS</span>.toMillis(<span class="type">System</span>.nanoTime() - checkpointStartTimeNs)  </span><br><span class="line">  logInfo(<span class="string">s&quot;Checkpointing took <span class="subst">$checkpointDurationMs</span> ms.&quot;</span>)  </span><br><span class="line">  <span class="comment">// 创建新的RDD用于返回</span></span><br><span class="line">  <span class="keyword">val</span> newRDD = <span class="keyword">new</span> <span class="type">ReliableCheckpointRDD</span>[<span class="type">T</span>](  </span><br><span class="line">    sc, checkpointDirPath.toString, originalRDD.partitioner)  </span><br><span class="line">  <span class="keyword">if</span> (newRDD.partitions.length != originalRDD.partitions.length) &#123;  </span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(  </span><br><span class="line">      <span class="string">&quot;Checkpoint RDD has a different number of partitions from original RDD. Original &quot;</span> +  </span><br><span class="line">        <span class="string">s&quot;RDD [ID: <span class="subst">$&#123;originalRDD.id&#125;</span>, num of partitions: <span class="subst">$&#123;originalRDD.partitions.length&#125;</span>]; &quot;</span> +  </span><br><span class="line">        <span class="string">s&quot;Checkpoint RDD [ID: <span class="subst">$&#123;newRDD.id&#125;</span>, num of partitions: &quot;</span> +  </span><br><span class="line">        <span class="string">s&quot;<span class="subst">$&#123;newRDD.partitions.length&#125;</span>].&quot;</span>)  </span><br><span class="line">  &#125;  </span><br><span class="line">  newRDD  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>至此，RDD checkpoint 的写入就完成了。</p>
<h3 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h3><p>数据的读取本质上是在 Executor 对 Task 进行执行时，调用 RDD 的 iterator 方法，读取需要的分区数据。</p>
<p><img src="https://raw.githubusercontent.com/MLiLay/img/main/img/20240722165555.png" alt="image.png"></p>
<p>在 iterator 内部首先需要判断 RDD 的存储几倍，如果存储级别为 None，则代表其为 Checkpointed RDD 就调用 computeOrReadCheckpoint 方法进行读取或者是计算。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should &#x27;&#x27;not&#x27;&#x27; be called by users directly, but is available for implementors of custom  </span></span><br><span class="line"><span class="comment"> * subclasses of RDD. */</span></span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123; </span><br><span class="line">  <span class="comment">// 判断RDD的存储级别</span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;  </span><br><span class="line">    getOrCompute(split, context)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    computeOrReadCheckpoint(split, context)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在 computeOrReadCheckpoint 方法内，需要判断 RDD 是否已经 Checkpointed，如果是就直接调用该 rdd 的 parent rdd 的 iterator() 也就是 CheckpointRDD.iterator()，否则直接调用该 RDD 的 compute。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing. */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">computeOrReadCheckpoint</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] =  </span><br><span class="line">&#123;  </span><br><span class="line">  <span class="comment">// 判断是否已经Checkpointed</span></span><br><span class="line">  <span class="keyword">if</span> (isCheckpointedAndMaterialized) &#123;  </span><br><span class="line">    firstParent[<span class="type">T</span>].iterator(split, context)  </span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">    compute(split, context)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>CheckpointRDD(ReliableCheckpointRDD) 的 compute 方法内部其实就只是在按照对应的 HDFS 地址进行读取</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**  </span></span><br><span class="line"><span class="comment">   * Read the content of the checkpoint file associated with the given partition.   */</span>  </span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123; </span><br><span class="line">	<span class="comment">// 获取文件地址</span></span><br><span class="line">    <span class="keyword">val</span> file = <span class="keyword">new</span> <span class="type">Path</span>(checkpointPath, <span class="type">ReliableCheckpointRDD</span>.checkpointFileName(split.index))  </span><br><span class="line">    <span class="type">ReliableCheckpointRDD</span>.readCheckpointFile(file, broadcastedConf, context)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * Read the content of the specified checkpoint file. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readCheckpointFile</span></span>[<span class="type">T</span>](  </span><br><span class="line">    path: <span class="type">Path</span>,  </span><br><span class="line">    broadcastedConf: <span class="type">Broadcast</span>[<span class="type">SerializableConfiguration</span>],  </span><br><span class="line">    context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;  </span><br><span class="line">  <span class="keyword">val</span> env = <span class="type">SparkEnv</span>.get  </span><br><span class="line">  <span class="keyword">val</span> fs = path.getFileSystem(broadcastedConf.value.value)  </span><br><span class="line">  <span class="keyword">val</span> bufferSize = env.conf.get(<span class="type">BUFFER_SIZE</span>)  </span><br><span class="line">  <span class="keyword">val</span> fileInputStream = &#123;  </span><br><span class="line">    <span class="keyword">val</span> fileStream = fs.open(path, bufferSize)  </span><br><span class="line">    <span class="keyword">if</span> (env.conf.get(<span class="type">CHECKPOINT_COMPRESS</span>)) &#123;  </span><br><span class="line">      <span class="type">CompressionCodec</span>.createCodec(env.conf).compressedInputStream(fileStream)  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">      fileStream  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">val</span> serializer = env.serializer.newInstance()  </span><br><span class="line">  <span class="keyword">val</span> deserializeStream = serializer.deserializeStream(fileInputStream)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Register an on-task-completion callback to close the input stream.  </span></span><br><span class="line">  context.addTaskCompletionListener[<span class="type">Unit</span>](context =&gt; deserializeStream.close())  </span><br><span class="line">  ![img.png](img.png)</span><br><span class="line">  deserializeStream.asIterator.asInstanceOf[<span class="type">Iterator</span>[<span class="type">T</span>]]  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>checkpoint 信息整理自:</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/jiaojiao521765146514/article/details/86288967">https://blog.csdn.net/jiaojiao521765146514/article/details/86288967</a></p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.JPG" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/29/Spark-%E6%BA%90%E7%A0%81-SparkContext/" title="Spark-源码-SparkContext"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Spark-源码-SparkContext</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/11/Celeborn/" title="Celeborn"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Celeborn</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-Cache%E4%B8%8Echeckpoint/" title="Spark-基础-Cache 与 Checkpoint"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-Cache 与 Checkpoint</div></div></a></div><div><a href="/2024/03/20/Spark-%E5%9F%BA%E7%A1%80-Shuffle%E6%9C%BA%E5%88%B6/" title="Spark-基础-Shuffle机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-20</div><div class="title">Spark-基础-Shuffle机制</div></div></a></div><div><a href="/2024/03/21/Spark-%E5%9F%BA%E7%A1%80-%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6/" title="Spark-基础-任务调度机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-21</div><div class="title">Spark-基础-任务调度机制</div></div></a></div><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F/" title="Spark-基础-运行模式"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-运行模式</div></div></a></div><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-%E7%B4%AF%E5%8A%A0%E5%99%A8%E4%B8%8E%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/" title="Spark-基础-累加器和Broadcast"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-累加器和Broadcast</div></div></a></div><div><a href="/2024/03/19/Spark-%E5%9F%BA%E7%A1%80-%E9%80%9A%E8%AE%AF%E6%9C%BA%E5%88%B6/" title="Spark-基础-通信机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">Spark-基础-通信机制</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CHLi</div><div class="author-info__description">Welcome to Mr.Li's blog</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">77</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/MLiLay"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome! This is MLiLay's Blog.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#persist"><span class="toc-number">1.</span> <span class="toc-text">persist</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81"><span class="toc-number">1.2.</span> <span class="toc-text">源码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Checkpoint"><span class="toc-number">2.</span> <span class="toc-text">Checkpoint</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-1"><span class="toc-number">2.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81-1"><span class="toc-number">2.2.</span> <span class="toc-text">源码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.2.1.</span> <span class="toc-text">初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5"><span class="toc-number">2.2.2.</span> <span class="toc-text">数据写入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96"><span class="toc-number">2.2.3.</span> <span class="toc-text">数据读取</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/29/Spark-%E6%BA%90%E7%A0%81-SparkContext/" title="Spark-源码-SparkContext">Spark-源码-SparkContext</a><time datetime="2024-07-29T06:27:54.293Z" title="发表于 2024-07-29 14:27:54">2024-07-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/22/Spark-%E6%BA%90%E7%A0%81-persist%E4%B8%8Echeckpoint/" title="Spark-源码-persist与checkpoint">Spark-源码-persist与checkpoint</a><time datetime="2024-07-22T09:02:52.238Z" title="发表于 2024-07-22 17:02:52">2024-07-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/11/Celeborn/" title="Celeborn">Celeborn</a><time datetime="2024-07-11T12:46:48.425Z" title="发表于 2024-07-11 20:46:48">2024-07-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/11/Spark-%E6%BA%90%E7%A0%81-Broadcast/" title="Spark-源码-Broadcast">Spark-源码-Broadcast</a><time datetime="2024-07-11T12:29:06.531Z" title="发表于 2024-07-11 20:29:06">2024-07-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/01/Spark-%E6%BA%90%E7%A0%81-%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/" title="Spark-源码-任务提交流程（Yarn）">Spark-源码-任务提交流程（Yarn）</a><time datetime="2024-07-01T13:56:32.213Z" title="发表于 2024-07-01 21:56:32">2024-07-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By CHLi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>